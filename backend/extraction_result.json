{
  "timestamp": "2025-08-25T22:43:44.344268",
  "url": "https://drive.google.com/drive/folders/1h6GptTW3DPCdhu7q5tY-83CXrpV8TmY_?usp=drive_link",
  "extraction_results": [
    {
      "success": true,
      "text": "BY NICOLAS BOUCHER\nBASICS TO KNOW\nBY NICOLAS BOUCHER\nACCOUNTING\n10\nSWIPE\n1\nACCOUNTING BASICS TO KNOW #1\nBY NICOLAS BOUCHER\nACCOUNTING\nEQUATION:\nTHE\nAssets = Liabilities + Owners' Equity\n2\nBALANCE SHEET\nIt’s a financial statement that\nshows a company's assets,\nliabilities, and owners' equity at a\nspecific point in time.\nBY NICOLAS BOUCHER\nTHE\nACCOUNTING BASICS TO KNOW #2\n3\nBY NICOLAS BOUCHER\nIt shows a company's revenues,\nexpenses, and net income over a\nperiod of time .\nINCOME\nSTATEMENT\nTHE\nACCOUNTING BASICS TO KNOW #3\n4\nIt shows how a company's cash\nhas changed over a period of time.\nSTATEMENT OF\nCASH FLOWS\nBY NICOLAS BOUCHER\nCASH INFLOWS - REVENUES\nCASH OUTFLOWS - EXPENSES\nACCOUNTING BASICS TO KNOW #4\nFoundation of double-entry\naccounting, \nThe practice of recording\ntransactions in two accounts in \norder to maintain the integrity \nof the accounting equation.\n5\nBY NICOLAS BOUCHER\nDEBITS AND\nCREDITS\nACCOUNTING BASICS TO KNOW #5\nCHART OF\nACCOUNTS\nIt’s a list of all the accounts used by a\ncompany in its general ledger, with\neach account being assigned a\nunique number for identification\npurposes.\nACCOUNTING BASICS TO KNOW #6\nBY NICOLAS BOUCHER\n6\n7\nBY NICOLAS BOUCHER\nTHE GENERAL\nLEDGER\nThe GL is a record of all a company's\nfinancial transactions, with each\ntransaction being recorded in the\nappropriate account.\nACCOUNTING BASICS TO KNOW #7\nACCRUAL BASIS\nOF ACCOUNTING\nIt’s the practice of recognizing\nrevenues and expenses when they\nare earned or incurred, rather than\nwhen they are paid or received.\nACCOUNTING BASICS TO KNOW #8\nBY NICOLAS BOUCHER\n8\nIt’s the practice of matching revenues\nand expenses in the same accounting\nperiod, in order to accurately reflect a\ncompany's financial performance\nduring that period.\nBY NICOLAS BOUCHER\nTHE MATCHING\nPRINCIPLE\nACCOUNTING BASICS TO KNOW #9\n9\nACCOUNTING BASICS TO KNOW #10\nIt means being conservative in\nestimating the value of assets and\nliabilities, and of only recording\ntransactions that are material\n(significant) enough to affect a\ncompany's financial statements.\nCONSERVATISM\n& MATERIALITY\nBY NICOLAS BOUCHER\n10\nTHE ACCOUNTING EQUATION\nTHE BALANCE SHEET\nINCOME STATEMENT\nSTATEMENT OF CASH FLOW\nDEBITS CREDITS \nCHART OF ACCOUNTS\nGENERAL LEDGER\nACCRUAL BASIS OF ACCOUNTING\nTHE MATCHING PRINCIPLE\nCONSERVATISM AND\nMATERIALITY\n1.\n2.\n3.\n4.\n5.\n6.\n7.\n8.\n9.\n10.\nSUMMARY\nBY NICOLAS BOUCHER\nWANT TO\nSUCCEED IN YOUR\nNEXT INTERVIEW ?\nCHECK MY LINKEDIN PROFILE\nFollow me if\nyou don’t want\nto miss my\nnext posts!\nEveryday I am\nposting about\nfinance.\nNICOLAS BOUCHER",
      "page_count": 14,
      "pages": [
        {
          "page": 1,
          "text": "BY NICOLAS BOUCHER\nBASICS TO KNOW\nBY NICOLAS BOUCHER\nACCOUNTING\n10\nSWIPE",
          "char_count": 73,
          "ocr_used": false
        },
        {
          "page": 2,
          "text": "1\nACCOUNTING BASICS TO KNOW #1\nBY NICOLAS BOUCHER\nACCOUNTING\nEQUATION:\nTHE\nAssets = Liabilities + Owners' Equity",
          "char_count": 113,
          "ocr_used": false
        },
        {
          "page": 3,
          "text": "2\nBALANCE SHEET\nIt’s a financial statement that\nshows a company's assets,\nliabilities, and owners' equity at a\nspecific point in time.\nBY NICOLAS BOUCHER\nTHE\nACCOUNTING BASICS TO KNOW #2",
          "char_count": 187,
          "ocr_used": false
        },
        {
          "page": 4,
          "text": "3\nBY NICOLAS BOUCHER\nIt shows a company's revenues,\nexpenses, and net income over a\nperiod of time .\nINCOME\nSTATEMENT\nTHE\nACCOUNTING BASICS TO KNOW #3",
          "char_count": 151,
          "ocr_used": false
        },
        {
          "page": 5,
          "text": "4\nIt shows how a company's cash\nhas changed over a period of time.\nSTATEMENT OF\nCASH FLOWS\nBY NICOLAS BOUCHER\nCASH INFLOWS - REVENUES\nCASH OUTFLOWS - EXPENSES\nACCOUNTING BASICS TO KNOW #4",
          "char_count": 188,
          "ocr_used": false
        },
        {
          "page": 6,
          "text": "Foundation of double-entry\naccounting, \nThe practice of recording\ntransactions in two accounts in \norder to maintain the integrity \nof the accounting equation.\n5\nBY NICOLAS BOUCHER\nDEBITS AND\nCREDITS\nACCOUNTING BASICS TO KNOW #5",
          "char_count": 229,
          "ocr_used": false
        },
        {
          "page": 7,
          "text": "CHART OF\nACCOUNTS\nIt’s a list of all the accounts used by a\ncompany in its general ledger, with\neach account being assigned a\nunique number for identification\npurposes.\nACCOUNTING BASICS TO KNOW #6\nBY NICOLAS BOUCHER\n6",
          "char_count": 219,
          "ocr_used": false
        },
        {
          "page": 8,
          "text": "7\nBY NICOLAS BOUCHER\nTHE GENERAL\nLEDGER\nThe GL is a record of all a company's\nfinancial transactions, with each\ntransaction being recorded in the\nappropriate account.\nACCOUNTING BASICS TO KNOW #7",
          "char_count": 196,
          "ocr_used": false
        },
        {
          "page": 9,
          "text": "ACCRUAL BASIS\nOF ACCOUNTING\nIt’s the practice of recognizing\nrevenues and expenses when they\nare earned or incurred, rather than\nwhen they are paid or received.\nACCOUNTING BASICS TO KNOW #8\nBY NICOLAS BOUCHER\n8",
          "char_count": 211,
          "ocr_used": false
        },
        {
          "page": 10,
          "text": "It’s the practice of matching revenues\nand expenses in the same accounting\nperiod, in order to accurately reflect a\ncompany's financial performance\nduring that period.\nBY NICOLAS BOUCHER\nTHE MATCHING\nPRINCIPLE\nACCOUNTING BASICS TO KNOW #9\n9",
          "char_count": 241,
          "ocr_used": false
        },
        {
          "page": 11,
          "text": "ACCOUNTING BASICS TO KNOW #10\nIt means being conservative in\nestimating the value of assets and\nliabilities, and of only recording\ntransactions that are material\n(significant) enough to affect a\ncompany's financial statements.\nCONSERVATISM\n& MATERIALITY\nBY NICOLAS BOUCHER\n10",
          "char_count": 276,
          "ocr_used": false
        },
        {
          "page": 12,
          "text": "THE ACCOUNTING EQUATION\nTHE BALANCE SHEET\nINCOME STATEMENT\nSTATEMENT OF CASH FLOW\nDEBITS CREDITS \nCHART OF ACCOUNTS\nGENERAL LEDGER\nACCRUAL BASIS OF ACCOUNTING\nTHE MATCHING PRINCIPLE\nCONSERVATISM AND\nMATERIALITY\n1.\n2.\n3.\n4.\n5.\n6.\n7.\n8.\n9.\n10.\nSUMMARY\nBY NICOLAS BOUCHER",
          "char_count": 269,
          "ocr_used": false
        },
        {
          "page": 13,
          "text": "WANT TO\nSUCCEED IN YOUR\nNEXT INTERVIEW ?\nCHECK MY LINKEDIN PROFILE",
          "char_count": 67,
          "ocr_used": false
        },
        {
          "page": 14,
          "text": "Follow me if\nyou don’t want\nto miss my\nnext posts!\nEveryday I am\nposting about\nfinance.\nNICOLAS BOUCHER",
          "char_count": 104,
          "ocr_used": false
        }
      ],
      "metadata": {
        "format": "PDF 1.4",
        "title": "",
        "author": "",
        "subject": "",
        "keywords": "",
        "creator": "",
        "producer": "",
        "creationDate": "",
        "modDate": "",
        "trapped": "",
        "encryption": null
      },
      "char_count": 2524,
      "word_count": 412,
      "ocr_pages_count": 0,
      "error": null,
      "file_id": "1xKpBFi9B9lDkrbi_6ypHGM5q0lSIt29j",
      "filename": "Accounting Basics - Nicolas Boucher.pdf",
      "filepath": "downloads/Accounting Basics - Nicolas Boucher.pdf",
      "download_link": "https://drive.google.com/uc?export=download&id=1xKpBFi9B9lDkrbi_6ypHGM5q0lSIt29j"
    },
    {
      "success": true,
      "text": "Adopting Rust to \nAchieve Business Goals\nOpportunities, challenges, and recommendations\nIncluding extensive sources and further reading\nHugo van de Pol - Tweede golf - Nov 2023\n10 min read\n●\nRust is a modern, multi-purpose programming language that can be used across your stack, \nand therefore across your organisation [1].\n●\nRust is fast, safe, and secure. Rust combines C-like performance with strict type, thread and \nmemory safety all checked at compile time, eliminating entire - typically hard to debug - \nclasses of bugs and vulnerabilities.\n●\nFrom its ﬁrst stable version 8 years ago, Rust has seen increased use by big tech companies like \nGoogle [2] and Microsoft [3]. High-tech companies like Inﬁneon [4] and OEMs like Renault [5] are \nalso starting to use Rust as of 2022.\n●\nThe language is open source, developed by the Rust Project, and governance lies with the Rust \nFoundation [6]. Commercially, there are multiple qualiﬁed Rust compilers available nowadays, \naimed at safety critical use cases [7].\n●\nUsing Rust both improves business outcomes like predictability as well as developer \nsatisfaction. Rust has been Stack Overflow’s most loved language for 8 years in a row [8].\nA short overview of Rust\n[∗] See slide 19 - 21 - Sources and further reading\n2\nRust is for building correct software. From cloud applications to embedded and systems software. \nIt has the opportunity to be used across your stack and across your organization.\nRust is on NIST’s list of safer languages [9].\n3\n●\nHigher quality products in the hands of your customers because of much more reliable \nand stable software [10 and slide 5].\n●\nBetter predictability for both your customers as well as your software engineering teams \nand their management, by eliminating entire classes of bugs and vulnerabilities that are \ntypically hard to debug [3 and 10].\n●\nLower software development costs driven by increased productivity due to catching and \nﬁxing bugs when they are cheapest [11], much easier code reviews [2], and increased \ndeveloper satisfaction [8].\n●\nStaying ahead of customers and industry peers that are adopting Rust as well [slide 7].\n●\nSubstantially higher attraction of new hires. Many talented engineers want to work with \nRust [slide 8 and 8]. And anecdotally, at Tweede golf, we have a constant stream of open \napplications purely because we do a lot of Rust.\nOpportunities of adopting Rust\n4\n“Memory and threading related crashes belong to the \npast now we use Rust. On multiple platforms, both on \nembedded and on PC-platforms, we so far see perfect \navailability of our Rust applications - unmatched by \nany C or Java applications we ever developed.”\nJoeri Klekamp\nCOO - Technolution - The Netherlands\n5\nThe platinum members of the Rust Foundation\nSource: Membership\n6\n●\nRust can be mastered Google found that more than 2/3 of respondents are conﬁdent in \ncontributing to a Rust codebase within two months or less when learning Rust [2], as opposed \nto what you usually hear about the perceived steep learning curve.\n●\nHundreds of vulnerabilities prevented For Android 13, Google Found that “it’s likely that \nusing Rust [1.5M lines of code in total] has already prevented hundreds of vulnerabilities from \nreaching production” [12].\n●\n40% faster software delivery and 2 times less costs according to OxidOS that is building a \nRust-based secure ecosystem for safety critical automotive ECUs [13].\n●\nMuch greater stability and availability of the services they develop, says Technolution’s COO \nJoeri Klekamp [slide 5].\nExperiences and results at other organizations\n7\n“Kotlin and Rust are the two fastest-growing \nlanguage communities, having more than doubled in \nsize in the past two years.”\nState of the Developer Nation - 23th Edition\nThe latest trends from our Q3 2022 survey of 26000+ developers\n8\n●\nFor any project you would write in C, for example because of the performance required.\n●\nFor projects in the domain of bare-metal embedded systems programming, systems \nprogramming or cloud backend services. \n●\nFor products where reliability is crucial, for example because they can cause real harm \nwhen malfunctioning (like a medical device) or they are hard or virtually impossible to \nupdate once deployed.\n●\nFor high throughput systems where you want memory and thread safety to avoid \ncomplicated bugs, but can’t have a garbage collector that may cause unreliable behavior.\n●\nFor products for which security is absolutely critical, for example if there is risk of \nsabotage or espionage.\nType of projects to use Rust for\n9\nA quick comparison of Rust’s rising popularity (on GitHub), which is comparable with Go.\n10\nAdopting a new programming language in a large organization involves change. Change comes with uncertainty, \nobstacles and investments that need to be made. \nSome of the challenges you will face:\n●\nFiguring out what your preferred tools will be for integrating new Rust code into projects that have largely \nbeen written in C, C++, Python (etc.).\n●\nSetting up some internal infrastructure, for example regarding which Rust crates (libraries) can be used.\n●\nConvincing developers that may be reluctant to learn a new language.\n●\nTraining developers (already in progress with us) on Rust and onboarding new hires to Rust.\n●\nCreating guidelines on when to use Rust, comparable to existing guidelines on when to do model-based \ndesign, when to do formal veriﬁcation, (etc.).\n●\nDeveloping a roadmap for Rust’s adoption, deﬁning clear goals, milestones and steps to get there.\nBTW we don’t see hiring experienced Rust engineers as a challenge at this point, as we advise to train the \nengineers you already have instead [18].\nChallenges of adopting Rust\n11\nRust has been engineers’ most loved language for 8 years in a row, according to Stack Overflow [8].\n12\n●\nStart small and reach a ﬁrst real result quickly for example by letting a single team write one or two \nsmall new components of existing production projects in Rust, before taking any broader organizational \nsteps.\n●\nPut a small group of engineers and architects and a senior executive in the lead. \n●\nSet up the infrastructure and choose the tools required for other teams that want to try Rust. \n●\nProvide ongoing support to teams picking up Rust. This includes training, pointing them to the \ninfrastructure and tools to use and answering any questions they may have.\n●\nProvide absolute clarity on the why of adopting Rust, the when and where of using it and on the goals \nyou pursue by choosing Rust for certain projects.\n●\nConsult with industry peers and leaders that adopted Rust, learning ﬁrst hand what worked for them.\n●\nBecome an active member of the Rust community and possibly a member of the Rust foundation.\n●\nWork with us to get guidance along the way, get people trained, and to work with your engineers on their \nﬁrst Rust projects, transferring knowledge and signiﬁcantly lowering ﬁrst project risk.\nRecommendations\n13\nThree examples of Rust projects at Tweede golf\nRust implementation of PTP\nWith funding provided by the NLnet Foundation, we created an open source Rust \nimplementation of the Precision Time Protocol called Statime.\nSatellite Guidance, Navigation and Control\nGama has deployed a solar sail satellite in orbit around earth. Their GNC ﬁrmware in Rust is in \ncharge of controlling the satellite systems and we help their team speed up the development \nand to steer them towards higher quality code.\nRust implementations of sudo and su\nFor ISRG we’re working together with German company Ferrous Systems on Rust \nimplementations of the famous sudo and su commands that are shipped with almost every unix \nOS. Building these implementations is part of ISRG’s Prossimo initiative.\n14\nEven with the best developers and tooling, after 12 years of struggling, the percentage of CVEs related \nto using memory unsafe languages is still around 70% at Microsoft. We should do better than that.\n15\nAbout Tweede golf\nTweede golf (2009) is a software engineering company located in Nijmegen, The Netherlands. We’re \nspecialized in building safe and secure software with Rust, with notable projects like sudo-rs and \nntpd-rs in the digital infrastructure domain.\nAs a silver member of the Rust Foundation, we are on a mission to aid Rust's adoption at innovative \ntech companies in the Netherlands. We do so because we ﬁrmly believe that Rust is the future of \nprogramming. \nSome of our clients\nLearn more\nVisit tweedegolf.nl to learn more about our engineering and training services.\n16\nGetting in touch\nContact me directly or contact Tweede golf via \nhttps://tweedegolf.nl or LinkedIn\nHugo van de Pol\nDirector\nhugo@tweedegolf.com\n+31 6 45 39 49 89\n17\n1.\nThe ofﬁcial Rust website with links to quick starts, docs and community forums.\n2.\nA recent write-up published on Google’s Open Source Blog about their Rust journey in 2022, \ndistinguishing facts from ﬁction and sharing interesting survey results.\n3.\nDavid Weston’s keynote presentation at Tectonics about Microsoft’s strategy to improve \nmemory safety: both hardening existing C and C++ code and investing heavily in Rust. \n4.\nInﬁneon is enabling safe and secure mission-critical software to be developed in Rust, \noffering three different families of microcontrollers together with a Rust compiler built by \nHighTec. \n5.\nRenault announced at escar Europe 2023 they ship their brand-new cars with Rust. Look for \n“renault” in escar Europe's program.\n6.\nThe Rust Foundation is an independent non-proﬁt organization dedicated to stewarding the \nRust programming language, nurturing the Rust ecosystem, and supporting the set of \nmaintainers governing and developing the project.\nSources and further reading 1 of 3\n18\n7.\nCommercial offers for qualiﬁed Rust compilers are currently available from Ferrous Systems \n(Ferrocene), HighTec (HighTec Rust Compiler) and AdaCore (GNAT Pro for Rust).\n8.\nRust has been engineers’ most loved language for 8 years in a row now, according to Stack \nOverflow’s 2022 annual Developer Survey.\n9.\nNIST’s list of safer languages stating that “Safety or quality cannot be \"tested into\" programs. \nIt must be designed in from the start.”\n10.\nLook in particular at the sections “Reliability and stability” and “Predictable runtime behavior”  \nfrom this excellent write-up about using Rust in production from Matthias Endler.\n11.\nAccording to Table 5-1 Relative Cost to Repair Defects When Found at Different Stages of \nSoftware Development from this 2002 NIST report on The Economic Impacts of Inadequate \nInfrastructure for Software Testing a post release bug is estimated to be on average 6 times \nmore expensive to ﬁx then during coding.\n12.\nDetailed blog post by Google on moving to memory safe languages in Android 13 (2022), one \nof them being Rust.\nSources and further reading 2 of 3\n19\n13.\nOxidOS is building a Rust-based secure ecosystem for safety critical automotive ECUs\n14.\nA brief explanation of the fact that around 70 percent of all the vulnerabilities in Microsoft \nproducts (addressed through a security update each year) are memory safety issues.\n15.\nProgramming Language Trends by Languish on GitHub shows Rust’s popularity accelerating.\n16.\nWhy Rust is a Great Fit for Embedded Software, an article on Tweede golf’s Rust blog on using \nRust for bare-metal embedded programming.\n17.\nDevelopment of the Rust language by what is called the Rust Project.\n18.\nA LinkedIn post by Tweede golf’s director Hugo van de Pol about how to look at hiring great \nRust developers.\n19.\nA great community resource about embedded Rust: The Embedded Rustacean\nSources and further reading 3 of 3\n20",
      "page_count": 20,
      "pages": [
        {
          "page": 1,
          "text": "Adopting Rust to \nAchieve Business Goals\nOpportunities, challenges, and recommendations\nIncluding extensive sources and further reading\nHugo van de Pol - Tweede golf - Nov 2023\n10 min read",
          "char_count": 190,
          "ocr_used": false
        },
        {
          "page": 2,
          "text": "●\nRust is a modern, multi-purpose programming language that can be used across your stack, \nand therefore across your organisation [1].\n●\nRust is fast, safe, and secure. Rust combines C-like performance with strict type, thread and \nmemory safety all checked at compile time, eliminating entire - typically hard to debug - \nclasses of bugs and vulnerabilities.\n●\nFrom its ﬁrst stable version 8 years ago, Rust has seen increased use by big tech companies like \nGoogle [2] and Microsoft [3]. High-tech companies like Inﬁneon [4] and OEMs like Renault [5] are \nalso starting to use Rust as of 2022.\n●\nThe language is open source, developed by the Rust Project, and governance lies with the Rust \nFoundation [6]. Commercially, there are multiple qualiﬁed Rust compilers available nowadays, \naimed at safety critical use cases [7].\n●\nUsing Rust both improves business outcomes like predictability as well as developer \nsatisfaction. Rust has been Stack Overflow’s most loved language for 8 years in a row [8].\nA short overview of Rust\n[∗] See slide 19 - 21 - Sources and further reading\n2",
          "char_count": 1085,
          "ocr_used": false
        },
        {
          "page": 3,
          "text": "Rust is for building correct software. From cloud applications to embedded and systems software. \nIt has the opportunity to be used across your stack and across your organization.\nRust is on NIST’s list of safer languages [9].\n3",
          "char_count": 229,
          "ocr_used": false
        },
        {
          "page": 4,
          "text": "●\nHigher quality products in the hands of your customers because of much more reliable \nand stable software [10 and slide 5].\n●\nBetter predictability for both your customers as well as your software engineering teams \nand their management, by eliminating entire classes of bugs and vulnerabilities that are \ntypically hard to debug [3 and 10].\n●\nLower software development costs driven by increased productivity due to catching and \nﬁxing bugs when they are cheapest [11], much easier code reviews [2], and increased \ndeveloper satisfaction [8].\n●\nStaying ahead of customers and industry peers that are adopting Rust as well [slide 7].\n●\nSubstantially higher attraction of new hires. Many talented engineers want to work with \nRust [slide 8 and 8]. And anecdotally, at Tweede golf, we have a constant stream of open \napplications purely because we do a lot of Rust.\nOpportunities of adopting Rust\n4",
          "char_count": 899,
          "ocr_used": false
        },
        {
          "page": 5,
          "text": "“Memory and threading related crashes belong to the \npast now we use Rust. On multiple platforms, both on \nembedded and on PC-platforms, we so far see perfect \navailability of our Rust applications - unmatched by \nany C or Java applications we ever developed.”\nJoeri Klekamp\nCOO - Technolution - The Netherlands\n5",
          "char_count": 314,
          "ocr_used": false
        },
        {
          "page": 6,
          "text": "The platinum members of the Rust Foundation\nSource: Membership\n6",
          "char_count": 65,
          "ocr_used": false
        },
        {
          "page": 7,
          "text": "●\nRust can be mastered Google found that more than 2/3 of respondents are conﬁdent in \ncontributing to a Rust codebase within two months or less when learning Rust [2], as opposed \nto what you usually hear about the perceived steep learning curve.\n●\nHundreds of vulnerabilities prevented For Android 13, Google Found that “it’s likely that \nusing Rust [1.5M lines of code in total] has already prevented hundreds of vulnerabilities from \nreaching production” [12].\n●\n40% faster software delivery and 2 times less costs according to OxidOS that is building a \nRust-based secure ecosystem for safety critical automotive ECUs [13].\n●\nMuch greater stability and availability of the services they develop, says Technolution’s COO \nJoeri Klekamp [slide 5].\nExperiences and results at other organizations\n7",
          "char_count": 800,
          "ocr_used": false
        },
        {
          "page": 8,
          "text": "“Kotlin and Rust are the two fastest-growing \nlanguage communities, having more than doubled in \nsize in the past two years.”\nState of the Developer Nation - 23th Edition\nThe latest trends from our Q3 2022 survey of 26000+ developers\n8",
          "char_count": 236,
          "ocr_used": false
        },
        {
          "page": 9,
          "text": "●\nFor any project you would write in C, for example because of the performance required.\n●\nFor projects in the domain of bare-metal embedded systems programming, systems \nprogramming or cloud backend services. \n●\nFor products where reliability is crucial, for example because they can cause real harm \nwhen malfunctioning (like a medical device) or they are hard or virtually impossible to \nupdate once deployed.\n●\nFor high throughput systems where you want memory and thread safety to avoid \ncomplicated bugs, but can’t have a garbage collector that may cause unreliable behavior.\n●\nFor products for which security is absolutely critical, for example if there is risk of \nsabotage or espionage.\nType of projects to use Rust for\n9",
          "char_count": 731,
          "ocr_used": false
        },
        {
          "page": 10,
          "text": "A quick comparison of Rust’s rising popularity (on GitHub), which is comparable with Go.\n10",
          "char_count": 92,
          "ocr_used": false
        },
        {
          "page": 11,
          "text": "Adopting a new programming language in a large organization involves change. Change comes with uncertainty, \nobstacles and investments that need to be made. \nSome of the challenges you will face:\n●\nFiguring out what your preferred tools will be for integrating new Rust code into projects that have largely \nbeen written in C, C++, Python (etc.).\n●\nSetting up some internal infrastructure, for example regarding which Rust crates (libraries) can be used.\n●\nConvincing developers that may be reluctant to learn a new language.\n●\nTraining developers (already in progress with us) on Rust and onboarding new hires to Rust.\n●\nCreating guidelines on when to use Rust, comparable to existing guidelines on when to do model-based \ndesign, when to do formal veriﬁcation, (etc.).\n●\nDeveloping a roadmap for Rust’s adoption, deﬁning clear goals, milestones and steps to get there.\nBTW we don’t see hiring experienced Rust engineers as a challenge at this point, as we advise to train the \nengineers you already have instead [18].\nChallenges of adopting Rust\n11",
          "char_count": 1051,
          "ocr_used": false
        },
        {
          "page": 12,
          "text": "Rust has been engineers’ most loved language for 8 years in a row, according to Stack Overflow [8].\n12",
          "char_count": 103,
          "ocr_used": false
        },
        {
          "page": 13,
          "text": "●\nStart small and reach a ﬁrst real result quickly for example by letting a single team write one or two \nsmall new components of existing production projects in Rust, before taking any broader organizational \nsteps.\n●\nPut a small group of engineers and architects and a senior executive in the lead. \n●\nSet up the infrastructure and choose the tools required for other teams that want to try Rust. \n●\nProvide ongoing support to teams picking up Rust. This includes training, pointing them to the \ninfrastructure and tools to use and answering any questions they may have.\n●\nProvide absolute clarity on the why of adopting Rust, the when and where of using it and on the goals \nyou pursue by choosing Rust for certain projects.\n●\nConsult with industry peers and leaders that adopted Rust, learning ﬁrst hand what worked for them.\n●\nBecome an active member of the Rust community and possibly a member of the Rust foundation.\n●\nWork with us to get guidance along the way, get people trained, and to work with your engineers on their \nﬁrst Rust projects, transferring knowledge and signiﬁcantly lowering ﬁrst project risk.\nRecommendations\n13",
          "char_count": 1139,
          "ocr_used": false
        },
        {
          "page": 14,
          "text": "Three examples of Rust projects at Tweede golf\nRust implementation of PTP\nWith funding provided by the NLnet Foundation, we created an open source Rust \nimplementation of the Precision Time Protocol called Statime.\nSatellite Guidance, Navigation and Control\nGama has deployed a solar sail satellite in orbit around earth. Their GNC ﬁrmware in Rust is in \ncharge of controlling the satellite systems and we help their team speed up the development \nand to steer them towards higher quality code.\nRust implementations of sudo and su\nFor ISRG we’re working together with German company Ferrous Systems on Rust \nimplementations of the famous sudo and su commands that are shipped with almost every unix \nOS. Building these implementations is part of ISRG’s Prossimo initiative.\n14",
          "char_count": 777,
          "ocr_used": false
        },
        {
          "page": 15,
          "text": "Even with the best developers and tooling, after 12 years of struggling, the percentage of CVEs related \nto using memory unsafe languages is still around 70% at Microsoft. We should do better than that.\n15",
          "char_count": 206,
          "ocr_used": false
        },
        {
          "page": 16,
          "text": "About Tweede golf\nTweede golf (2009) is a software engineering company located in Nijmegen, The Netherlands. We’re \nspecialized in building safe and secure software with Rust, with notable projects like sudo-rs and \nntpd-rs in the digital infrastructure domain.\nAs a silver member of the Rust Foundation, we are on a mission to aid Rust's adoption at innovative \ntech companies in the Netherlands. We do so because we ﬁrmly believe that Rust is the future of \nprogramming. \nSome of our clients\nLearn more\nVisit tweedegolf.nl to learn more about our engineering and training services.\n16",
          "char_count": 587,
          "ocr_used": false
        },
        {
          "page": 17,
          "text": "Getting in touch\nContact me directly or contact Tweede golf via \nhttps://tweedegolf.nl or LinkedIn\nHugo van de Pol\nDirector\nhugo@tweedegolf.com\n+31 6 45 39 49 89\n17",
          "char_count": 165,
          "ocr_used": false
        },
        {
          "page": 18,
          "text": "1.\nThe ofﬁcial Rust website with links to quick starts, docs and community forums.\n2.\nA recent write-up published on Google’s Open Source Blog about their Rust journey in 2022, \ndistinguishing facts from ﬁction and sharing interesting survey results.\n3.\nDavid Weston’s keynote presentation at Tectonics about Microsoft’s strategy to improve \nmemory safety: both hardening existing C and C++ code and investing heavily in Rust. \n4.\nInﬁneon is enabling safe and secure mission-critical software to be developed in Rust, \noffering three different families of microcontrollers together with a Rust compiler built by \nHighTec. \n5.\nRenault announced at escar Europe 2023 they ship their brand-new cars with Rust. Look for \n“renault” in escar Europe's program.\n6.\nThe Rust Foundation is an independent non-proﬁt organization dedicated to stewarding the \nRust programming language, nurturing the Rust ecosystem, and supporting the set of \nmaintainers governing and developing the project.\nSources and further reading 1 of 3\n18",
          "char_count": 1019,
          "ocr_used": false
        },
        {
          "page": 19,
          "text": "7.\nCommercial offers for qualiﬁed Rust compilers are currently available from Ferrous Systems \n(Ferrocene), HighTec (HighTec Rust Compiler) and AdaCore (GNAT Pro for Rust).\n8.\nRust has been engineers’ most loved language for 8 years in a row now, according to Stack \nOverflow’s 2022 annual Developer Survey.\n9.\nNIST’s list of safer languages stating that “Safety or quality cannot be \"tested into\" programs. \nIt must be designed in from the start.”\n10.\nLook in particular at the sections “Reliability and stability” and “Predictable runtime behavior”  \nfrom this excellent write-up about using Rust in production from Matthias Endler.\n11.\nAccording to Table 5-1 Relative Cost to Repair Defects When Found at Different Stages of \nSoftware Development from this 2002 NIST report on The Economic Impacts of Inadequate \nInfrastructure for Software Testing a post release bug is estimated to be on average 6 times \nmore expensive to ﬁx then during coding.\n12.\nDetailed blog post by Google on moving to memory safe languages in Android 13 (2022), one \nof them being Rust.\nSources and further reading 2 of 3\n19",
          "char_count": 1104,
          "ocr_used": false
        },
        {
          "page": 20,
          "text": "13.\nOxidOS is building a Rust-based secure ecosystem for safety critical automotive ECUs\n14.\nA brief explanation of the fact that around 70 percent of all the vulnerabilities in Microsoft \nproducts (addressed through a security update each year) are memory safety issues.\n15.\nProgramming Language Trends by Languish on GitHub shows Rust’s popularity accelerating.\n16.\nWhy Rust is a Great Fit for Embedded Software, an article on Tweede golf’s Rust blog on using \nRust for bare-metal embedded programming.\n17.\nDevelopment of the Rust language by what is called the Rust Project.\n18.\nA LinkedIn post by Tweede golf’s director Hugo van de Pol about how to look at hiring great \nRust developers.\n19.\nA great community resource about embedded Rust: The Embedded Rustacean\nSources and further reading 3 of 3\n20",
          "char_count": 805,
          "ocr_used": false
        }
      ],
      "metadata": {
        "format": "PDF 1.4",
        "title": "",
        "author": "",
        "subject": "",
        "keywords": "",
        "creator": "",
        "producer": "",
        "creationDate": "",
        "modDate": "",
        "trapped": "",
        "encryption": null
      },
      "char_count": 11596,
      "word_count": 1883,
      "ocr_pages_count": 0,
      "error": null,
      "file_id": "1r6a1syjOMQtfNnSNtHY-BwDPdn65u0Gl",
      "filename": "Adopting Rust to Achieve Business Goals.pdf",
      "filepath": "downloads/Adopting Rust to Achieve Business Goals.pdf",
      "download_link": "https://drive.google.com/uc?export=download&id=1r6a1syjOMQtfNnSNtHY-BwDPdn65u0Gl"
    },
    {
      "success": true,
      "text": "Algorithms\n14\nEvery Programmer Should Know\nBasic to Advance\n1. Searching Algorithms\nSearch each and every element of the array till you find the \nrequired element\nTime Complexity: O (n)\nLinear Search:\nSearches for the element by comparing it with the middle item \nof the sorted array. If a match occurs, index is returned, else \nthe searching area is reduced appropriately to either the \nupper half or lower half of the array\nTime Complexity: O (log2n)\nBinary Search:\n1\n2. Sorting Algorithms\nWorks by swapping adjacent elements in repeated passes, if \nthey are not in correct order.High time complexity and not \nsuitable for large datasets\nTime Complexity: O (n2)\nBubble Sort:\nThe array is split into sorted and unsorted parts. Unsorted \nelements are picked and placed at their correct position in \nthe sorted part\nTime Complexity: O (n2)\nInsertion Sort:\nThe smallest value among the unsorted elements of the array \nis selected in every pass and inserted to its appropriate \nposition into the array\nTime Complexity: O (n2)\nSelection Sort:\n2\nUses the property of max and min heaps having largest and \nsmallest elements at the root level It is an inplace sorting\nalgorithm\nTime Complexity: O (nlogn)\nHeap Sort:\nRepeatedly divide the array into half, sort the halves and then \ncombine them. It is a divide and conquer algorithm\nTime Complexity: O (nlog(n))\nMerge Sort:\nA pivot element is picked and the partitions made around it \nare again recursively partitioned and sorted. It is a divide and \nconquer algorithm.\nTime Complexity: O (nlog(n))\nWorst Case Time Complexity: O(n2)\nQuick Sort:\n3\n3. Basic Math Algorithms\nWorks by recursively dividing the bigger number with smaller \nnumber until the remainder is zero to get the greatest \ncommon divisor\nEuclid's Algorithm for GCD:\nUsed for finding all prime numbers up to a given number by \niteratively marking and removing the multiples of composite \nnumbers\nSieve of Eratosthenes:\nPerform operations at the bit-level or to manipulate bits in \ndifferent ways by using bitwise operators AND, OR, NOT, XOR\nBit Manipulations:\n4\n4. Graph Algorithms\n\u001b Breadth First Search is implemented using a queue and \nstarts at one given vertex and all its adjacent vertices are \nvisited first before going to the next verte\u0014\n\u001b Depth First Search is implemented using a stack and starts \nat one given vertex and continues its search through \nadjacent vertices until there are none left\nTime complexity for both is O(V + E)\nBreadth First Search and\r\nDepth First Search:\nUsed to find the shortest path between two vertices in a \ngraph. It is a greedy algorithm\nDijkstra's Algorithm:\n5\n5. Algorithms Tree\nTraverse the left subtree, visit the root node and then the right \nsubtree\nInorder Traversal:\nVisit the root node, traverse the left subtree and then the right \nsubtree\nPreorder Traversal:\nTraverse the left subtree, then the right subtree and then visit \nthe root node\nTime complexity: O(n)\nPostorder Traversal: \n6\nUsed for finding the minimum spanning tree, by sorting the \nedges in descending order and adding the smallest edge not\r\nadded yet to form a tree with all the nodes\nKruskal's Algorithm:\n7\n6. Dynamic Programming\nDynamic Programming works by storing the result of \nsubproblems to access when needed without \nrecalculation.\r\n\nIt uses memoization which is a top down approach and \ntabulation which is a bottom up approach\n\n is an algorithm for finding \nthe shortest path between all the pairs of vertices in a \nweighted graph. This algorithm is dynamic \nprogramming based\nFloyd-Warshall Algorithm\n8\n7. Backtracking Algorithms\nSolving problems by trying to build a solution one piece \nat a time, removing those solutions that fail to satisfy \nthe constraints of the problem.\n\nStandard questions for backtracking include. The N-\nqueens problem, Sum of Subsets problem, Graph \nColouring and Hamiltonian cycles.\n9\n8. Coding Huffman \nCompression Algorithm\nIt is a technique of compressing data to reduce its size \nwithout losing any of the details. Generally useful to \ncompress data with frequently occurring characters\n6 Building a Huffman treC\n6 Traversing the tree and assigning codes to characters \nbased on their frequency of occurrence\nInvolves two major parts:\n10",
      "page_count": 11,
      "pages": [
        {
          "page": 1,
          "text": "Algorithms\n14\nEvery Programmer Should Know\nBasic to Advance",
          "char_count": 61,
          "ocr_used": false
        },
        {
          "page": 2,
          "text": "1. Searching Algorithms\nSearch each and every element of the array till you find the \nrequired element\nTime Complexity: O (n)\nLinear Search:\nSearches for the element by comparing it with the middle item \nof the sorted array. If a match occurs, index is returned, else \nthe searching area is reduced appropriately to either the \nupper half or lower half of the array\nTime Complexity: O (log2n)\nBinary Search:\n1",
          "char_count": 410,
          "ocr_used": false
        },
        {
          "page": 3,
          "text": "2. Sorting Algorithms\nWorks by swapping adjacent elements in repeated passes, if \nthey are not in correct order.High time complexity and not \nsuitable for large datasets\nTime Complexity: O (n2)\nBubble Sort:\nThe array is split into sorted and unsorted parts. Unsorted \nelements are picked and placed at their correct position in \nthe sorted part\nTime Complexity: O (n2)\nInsertion Sort:\nThe smallest value among the unsorted elements of the array \nis selected in every pass and inserted to its appropriate \nposition into the array\nTime Complexity: O (n2)\nSelection Sort:\n2",
          "char_count": 571,
          "ocr_used": false
        },
        {
          "page": 4,
          "text": "Uses the property of max and min heaps having largest and \nsmallest elements at the root level It is an inplace sorting\nalgorithm\nTime Complexity: O (nlogn)\nHeap Sort:\nRepeatedly divide the array into half, sort the halves and then \ncombine them. It is a divide and conquer algorithm\nTime Complexity: O (nlog(n))\nMerge Sort:\nA pivot element is picked and the partitions made around it \nare again recursively partitioned and sorted. It is a divide and \nconquer algorithm.\nTime Complexity: O (nlog(n))\nWorst Case Time Complexity: O(n2)\nQuick Sort:\n3",
          "char_count": 548,
          "ocr_used": false
        },
        {
          "page": 5,
          "text": "3. Basic Math Algorithms\nWorks by recursively dividing the bigger number with smaller \nnumber until the remainder is zero to get the greatest \ncommon divisor\nEuclid's Algorithm for GCD:\nUsed for finding all prime numbers up to a given number by \niteratively marking and removing the multiples of composite \nnumbers\nSieve of Eratosthenes:\nPerform operations at the bit-level or to manipulate bits in \ndifferent ways by using bitwise operators AND, OR, NOT, XOR\nBit Manipulations:\n4",
          "char_count": 481,
          "ocr_used": false
        },
        {
          "page": 6,
          "text": "4. Graph Algorithms\n\u001b Breadth First Search is implemented using a queue and \nstarts at one given vertex and all its adjacent vertices are \nvisited first before going to the next verte\u0014\n\u001b Depth First Search is implemented using a stack and starts \nat one given vertex and continues its search through \nadjacent vertices until there are none left\nTime complexity for both is O(V + E)\nBreadth First Search and\r\nDepth First Search:\nUsed to find the shortest path between two vertices in a \ngraph. It is a greedy algorithm\nDijkstra's Algorithm:\n5",
          "char_count": 542,
          "ocr_used": false
        },
        {
          "page": 7,
          "text": "5. Algorithms Tree\nTraverse the left subtree, visit the root node and then the right \nsubtree\nInorder Traversal:\nVisit the root node, traverse the left subtree and then the right \nsubtree\nPreorder Traversal:\nTraverse the left subtree, then the right subtree and then visit \nthe root node\nTime complexity: O(n)\nPostorder Traversal: \n6",
          "char_count": 334,
          "ocr_used": false
        },
        {
          "page": 8,
          "text": "Used for finding the minimum spanning tree, by sorting the \nedges in descending order and adding the smallest edge not\r\nadded yet to form a tree with all the nodes\nKruskal's Algorithm:\n7",
          "char_count": 187,
          "ocr_used": false
        },
        {
          "page": 9,
          "text": "6. Dynamic Programming\nDynamic Programming works by storing the result of \nsubproblems to access when needed without \nrecalculation.\r\n\nIt uses memoization which is a top down approach and \ntabulation which is a bottom up approach\n\n is an algorithm for finding \nthe shortest path between all the pairs of vertices in a \nweighted graph. This algorithm is dynamic \nprogramming based\nFloyd-Warshall Algorithm\n8",
          "char_count": 407,
          "ocr_used": false
        },
        {
          "page": 10,
          "text": "7. Backtracking Algorithms\nSolving problems by trying to build a solution one piece \nat a time, removing those solutions that fail to satisfy \nthe constraints of the problem.\n\nStandard questions for backtracking include. The N-\nqueens problem, Sum of Subsets problem, Graph \nColouring and Hamiltonian cycles.\n9",
          "char_count": 311,
          "ocr_used": false
        },
        {
          "page": 11,
          "text": "8. Coding Huffman \nCompression Algorithm\nIt is a technique of compressing data to reduce its size \nwithout losing any of the details. Generally useful to \ncompress data with frequently occurring characters\n6 Building a Huffman treC\n6 Traversing the tree and assigning codes to characters \nbased on their frequency of occurrence\nInvolves two major parts:\n10",
          "char_count": 357,
          "ocr_used": false
        }
      ],
      "metadata": {
        "format": "PDF 1.7",
        "title": "",
        "author": "",
        "subject": "",
        "keywords": "",
        "creator": "",
        "producer": "",
        "creationDate": "",
        "modDate": "",
        "trapped": "",
        "encryption": null
      },
      "char_count": 4208,
      "word_count": 683,
      "ocr_pages_count": 0,
      "error": null,
      "file_id": "1RDNxioM5ERb8k4btY9GBLDVidjP435h2",
      "filename": "Algorithms .pdf",
      "filepath": "downloads/Algorithms .pdf",
      "download_link": "https://drive.google.com/uc?export=download&id=1RDNxioM5ERb8k4btY9GBLDVidjP435h2"
    },
    {
      "success": true,
      "text": "eat MICROSOFT AZURE - AZ 900 04\n# QvICK INTLO DUCTION. s 9%\nen ee he A ll Sa ee\n* | MichosoFT A2URE, commonly veferred as Azure. (60+ bake Corser )\n|\n* DEFINATION :— AZURE Is a cloud Com puting Services Created by\nmicveosoft = for buildiwg,- Testthg , Deploying and manging application\nand servicen through. microsoft - managed datn center.\n* Developers - Micncoft\n* Inttral Retease - 2% OCTOBER, 2008\n* Operating System — LINoZ, wiv dows, 10S, Andvoid -\nes License - cloged Source, oper source, SDKS.\n# GLOBAL (DATA CentER )— 1,65,000 miles Heer + I40 County.\nKI CONTAINERS — Azuve Used (Resouvee G +o vost re:\nTYPE OF MI GRsoFT AZURE SEevices-\n® | SaaS ~ (sorreare ac a Service) — 3°4 Party Sothoare over iternet\n* | PaaS — (Platform as a service)— Tools over mternet\n* | Laas ~ (infrastructure as a sevvice)- cloud based seovices (Storage)\nW\\sto@y Timecine-\nQ 3:\nJove ,2012- VIRTUAL MACHINE APR, 20 1y—(W DOLD Azvee)—T0-(MA)\nto\n1 DEC, 20n- HPC SCHEDULER JULY ,2oty— MSM +Com > OUTAGE\n4 .\nOcT,20\\0- PLATFORM ENHAceEMENTY SeP,r2015- SWITCH SONTLC\ns MS\nJUNE, 2010\" «NET, OS VERSIONING Mate ;2ol6— SEevice FABLIC\n¢ ‘ = ey\nFEG,2010- COMMERCIALLY AVAUAELEY SEP, 2017- NEW LOGO\n2 \\\nNov,2009- PHP, JAVA, CDN CTO oct, 2016- LINLY OLIENTER G pour\n‘ = \\s\nMARCH, 2009- SOL Datadbase APRLL»20B8- AZURE FLONT Dook SEevee\n\\e .\n' hoctower, 2008> Announced Azure: Manchrezo- 44 Millio. Daily Ache Use\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\nLinke htps/ aiken com insur sngh-51445522/\nContent\n\n1 Introduction\n2 Azure Services | |\n3 Virtual Machine ||\n4 Virtual Network ||\n5 Storage Services ||\n6 Core Azure Solution | |\n7 Azure Security | |\n8 Identity, Privacy & Compliance ||\n® Service SLA and Pricing [|\n= MICROSOFT AZURE - AZ 900 OJ\nMooule ¥ OL\n| wuaris croun Comeutina? isa amar | oF Compuhng Services\nover tne Internet. , enabling Faster Innovation , Hexible resources\nand economies of Seate-\nCl) |] ComPoTte- Previde me Compute Power. (memory and processor)\ndy NETWORKING Conhection of Compuly togetmer oF VM.\nI) STORAGE- Store oF data | Information\n) ANALYTICS RR IMPORTANT & KIND OF Load, hors much\nmemory required) ete -\nis SEevic& Provipedr CmiceosorT w2UE) HOLTING AS *A SEEVICR\nPhovipeg -% (CALL SERVICE PROVIDE By IN TENET #)\nA PUBLIC cCLOUDmK | PUVATE CLOLO™ — HYBRID GLOUD\nk General public car * organizahon Creake | ® organizahoy naa:\nhost thety service. Q Ahoud envivonement Adophing today hgon4\nty tneir ata Center Cloud- wwteh Come\n* Puvlic cloud owned Combmahion of Public\nby CUoud Seevicen | & organation i 2 trvvate botr-\nOr hosted provder Tesponsible for\nOperahna tne Service| > Connecting !xICHhKg\n® Provide Yesouvceg they provide datacenter to Ayre\nand service +o doka Centtr vig Internet\nmoth pie organization | we oes not pronde loth help of VPN Tunnel.\nAnd uper. acess to vger\noubside of ne (Example> App Ucahon host\n*\n* Recess Via— lntemelr organi zahon. on Ayure. ano cate Center\nWarnes. .\nQR and the Dalyrase in\n(teyamee> Azw ec) (# Dismovarane= Cost $) | client dam Center.)\nUnkedin - https://www.linkedin.com/in/suraj-singh-61445522/\ni MICROSOFT AZURE - Az 900 02\nTYPE Of CLoUD\n* See ery\nPUBLIC CLOVO PRIVATE Ctoud Hy sBRID ClouSe\nPublic. cloud Sotuton ave| Are declicated to one Hybnd soll ave a blend\nYea dy availiable From organizahon and often OF public & private\nAzure, P2ure prude have. much more Specific] Cloud.\nInPrastructuve @ Services Secunty ton) hen\nto me public public ctoud.\n* AZURE USER ACCOUNT ? open Pree AZuve account from\nInk (WWW. azure. microsoft: com /en-vg/ tree) that give you (2 monty\nFree populay seavices + 200 £ credit fer 30 days + 25 Senviceo\nWK | AZoRE CERTIFICATION !—\nAZ-900 CAZURE FURDAMEN TALS)\n1 q 4\nf AZLOY Ih2 204 | 2-ZB0R AZ-500\nAdministrator} |) peveloper i : Seeunty Cogmeer.\n| wssouaa |! 1 Associate MssoCicbed\nVAZ-3B6Y\n\\\ni |\nyT Ax 400 \\\n1 |\nDEVOPS ERGINEER | || AZURE\n' expert | So-oTtion\n| ACEH Irectuee\nexeeet\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\nae MICROSOFT AZURE - AZ 900 04\ncLovuD BENEFITS\n* | HLOM AVAILIBILITY!— Application hosted ito one dala center\nand ory enching Mtoe anoher patacenter\n> Replicate of Dake. into hwo dittevenr mochice (Data Center)\n® | SCALABILITY!- Too type oF nme: i (Verh cat 0 Honaym tet)\nVEST ICAL HORIZONTAL\n¥ SCALELP/ DOWN RIF ONE VM CuteTUAL MACHINE)\nF INCREAS ING / DECK ERE ING — HIGLy LOTILIZE, THEN Te Afe(c\n— ComeuTe BeTity (Ram 4 Snieteo To pwoqtHee VM evo\nPLoceccot) menraae by (LORD © MLAncet )\n* | ELaAStTIUTy:- Only addtng ne Sealing \\s Catled cae? 3\nFL AQILITY!-  (On-clemand Services) » fast to deploy vm - Conho|\npannel / Porte} easy to Create Virtual machine denne tre\nprvopernts -\n* | DIASTER RECOvVERY'— Replicate the date mo anotner to prevent\ndunng diaster fhme- fasy fo Vecovery . Tt & part ot Prvautibityey-\n* | COVSOMPTION- BASEN MOOEL!— Pay as per te use.\n* Cost OP T\\MIZAT LON CAPE & orey OPT MI2ATION.\n% | GLOBAL REacy- Aral tiorlity oF data Centr across globe.\n% | SECURITY: Secure to access and use the application -\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\na9 MICROSOFT AZURE - AZ 900 oo\n% | croun— capex 2 Of ER-\nCAP EX - Capita Expenaiture OPEX- Opeanonap Fp pend iturt\n® The up-front Spending oF money on | # spends on products and services\nPhysical (nfrastructure- as needed , pay as youre\n¥ Costs From Carex hove a value get willed wnmediatety:\ntat reduces overtined -\nee\n3 | Consumetion ~ Basep— MoveL:—  Aguve or any Cloud Service prouder\noperate on Gaombsarpixan— based model. which mean end user only\npay for the vegourcesS that they use:\n> Beer Cost prediction .\n7 Price for Individual resources and Services are povided.\n=? Billing ls based ON = actual usage.\nER G@ew Cloud Seevices:— (oBEeCTIVE Domain).\n@) | LaaS ~( Infrastructure -as-a- Service)\n@ | Paas — ( Platform- ar— a- sevice)\n@ | SaaS — ( Softwave-as- a -sewiie).\n@) Identity Qa Services type based on Use-—Cane-\n(S) | Deseribe tne shoved reaponsiint ity mode.\n(©) | beseritce Serveriess computing .\nLinkedin -http//wrwwelinkedin.com/in/sura-singh-61445522/\nae MICROSOFT AZURE - AZ 900 66\n(| INF@asteucTuee - As- 4-seRvice (TaaS)- fn tnis cloud service\nInfrastructuve (build pay-as~ yo-go) by venting Servers, virtusd—\n-machine , Storage ,netroor& and operas Systm From cloud\nproviders:\n(a) (2) @)\nSERVER NETWORKING Data CENTER\nJ FLEW scl S/ POWER Wen Agement\nSTo RAGE CeCURIT MAINTEN ACE\n(2) | PUAtfoem — AS- A - SEtvices (PaaS) Provides environement for\nbuilding , Testing and deploying Softeare application , vo tnout\nfocusing on man aging MIA ng ln Fra structure.\n*\\\nSERVERS & cTORAgE Oreeating system *!\n#2\nNETWOREIN , ERELIALLS/ LEcue ty + |pevevopment Toous* >\n¥2 DATABASE MANAGEMENT\nDATACENTER / POSER mAnAgemeNny Rusimets AVAL TICS\n(laa S$) C Paas)\n(2 SOFTWARE -AS- A- SeRvice (SaaS) - User connets to and\nLge = Cloud-lbased cepplicahor (app) over Ane miernels\nExample > othce 36S, emails, Teams Softoare .\nf\n(Laas) + (Paas’) +) HOSTED APPLICATIONS / Ppp's:\n(saaS)\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\n= MICROSOFT AZURE - AZ 900 oy\nRK) SHARED RESPONSIBILITY MoveL:-\nDATA & ACCESS XK RE me RA\nAPPLICATION ee Sad RK AZvReE\nRON TIME ; ee we AZvRE Arve\nOPERATING SYSTEM Ae Fe AZO RE AZORE\nVIRTUAL MACH INE ee RK AZvVLE AZURE\nComPute YOR AZvee ARVRE AZURE\nNETLORE ING He AZvRE ABLE RZORE\nSTORAGE ** AZURE AZURE RZOeE\ne*\nKE —F cheent ave manage tne responcibilit4:\n2)\n\nK|* REsSeevVE-LEss ComPOTING Ke CIMPoetANT) - Ty general clent +\nShaved vequivement of Infrastructure ( ex> CApaety of RAM 4\nbyocessor) , bur In Cate No-load on your Sevver but Shi|\nwe have to pay becauge we vesevved tne Caparsty: tm\nhost applicahon fh Datw- Center\n*-> Modem ae | to woerlG oy Mit CUpproade ts Change.\ntnere ( no reserved server fox work=load, When requ est\nCome It _ Wil] ages allocated +ne vesource en-need\nbasts nd yelease the veXouvce once NO-UZaGe. In Rrure\nhelesto beloco too sia’ are Important —\n\nAZORE FUNCTIONS'- I$ Q Code renning your services and nok\nne underlying platforms or infrastructure: T+ Creates (nfrashucture\nbased on an events:\nAZvee Loatc: |Is a Cloud Service tnat help you ab automate\nand Ovchastate tasks, business process and usoreHocs\nwhen you needed to wm beqrate appltcahon » bata, Systems\nand Services -\n\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\nLinkedin tps wun inkein.com/in/sursngh-61445522/\n23 Content\n\n1 Introduction [|\n2 Azure eee\n3 Virtual Machine | |\n4 Virtual Network | |\n5 Storage Services [|\n6 Core Azure Solution | |\n7 Azure Security [|\n8 Identity, Privacy & Compliance [|\n| Service SLA and Pricing [|\na MICROSOFT AZURE - AZ 900 O38\n| Mobute #:62\nCLOUD AZURE SeevIcEs!— Aduve Sevvices aQre> Ainded into\ntoo parts: A2v te\nCLoOup SEevices\na) wt 2)\nALORE Az LE\nARCHITECTURAL. Core-resovece'’s.\nCow PONENTS\nRegione and Availibitity Zones Compute\nNetioore\nSubseriplion and Resource Grou . Storage\n: batacace.\n\nCQ) AZURE— ARCHITECTURAL COMPONENTS: — Azure Is an Operating\nSystem prontch manage a dat. ceynier cof microsoft. To manage\nbelow = ave ae archittctrual Component —\n\nRegions g Region’ Pairs\nhvatlrbility Zone\nAzure fKesources\n\nAZORE ARCH TECTULAL\nResource Qmous - ComeoNEnTs\nAgure Resource Managers.\nSubsertp hows\nAyre Management grouy'c-\n\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\nPoe MICROSOFT AZURE - Az 900 og\n(t) _REGIONS:!-  Azuve offevs more gqrowal Yegions that any other\nCloud, provider Lotty 60% Regions vepresenting over | 40+ Counties -\nay meee See\nA Regions ave madéwup of One or more ola center\n+ provide Etomaty and scate to reduce customer Latency.\nP Preserve clata residency with) a Comprehenave Compliance offer.\n4 REGION > MIGuTtT BE RAR COLLECTION OF ZONES\nRegion Pae’s:- For Arvailibillty » Iw same Loorst Case scenario.\na complete region might be outage - Cmtgnt be due to Nature\nprovlem or some daiasitr happer ,Floed, power outage-\nTo avoid Wt problem Statement a reator ~pairs & done\nLohere nicrosott Create a paw-zone to prevent hate dune\nsuch dlaster.\nRr Chon Region Should be in same Geographic location )\ni > Cat ond 300 miles of Separnhon betseen Yegion pals)\n5 | #> (Avtomatic veplicate for some services-\nO| k> ( Reels priorihzed vegion recovery inte event outage)\noa .\n(1* > Chair ame  detred low mroaobt (maeott) .\nPAIRING ARE RIXED, DEFINED By > micRoSoryt OnLy\nREGIONS REGION L - PAIR\n— Nor Cental us — Louty Cena us\n~ East us ~ West Us\n—~ West US2 ~ west Central Ut\n~ Us Eager ~- Cenwar U-+\n~ Pndia Loutw - Lada -lentyal —\n~ Canta cent ~ Canada taar -\nJapan tart = Japan wet\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\n= MICROSOFT AZURE - Az 900 jo\n\nFe) AVALLABLITY OPTIONS:- Avr ailibility is a Factor volwcly deude\nhow much time & our applicahon—upe and vunning.\n\nEx> loot. mean ho down-tme , ALuoa y saxtctnngs\ntorailibilits ts defined oy -> SLA ( Service Louel agave ment )\nEx> 99°94. E> 99:99Y. DIASTEL RECOvELy\n\nZovey Lowe-2 Regron-J Region>\nee |\nREGION-L\nBR SINGLE Vu Host | ¥ Each Zone have independar' Reatonap Protectan\nowry. Power Backup , phy cicak lot. Daten Recidenaye-\n& Dependent on\nSuge vm *\n\nFF | AVAL BIL ZONE — One Zone treated as A One Datu Centr.\nMultple zone lead for egos , Each data Center ts <aquspped\nwoth independent Power, cooling and ne hoorkingy. each\nZone ave IMierconnecttd via Fiber-opticg nekdories\n\nPorailibr tity Zone\n\nLinkedIn - https://www.linkedin.com/in/suraj-singh-61445522/\noo MICROSOFT AZURE - AZ 900 \\]\nfe | AzURE Ceevices:— There are various services offeved 4\nk2uve Service ) fer pomt are Capture le2lovs.\na GENERAL Storage] verworcine |\n~ Management Groups —Nirtual machine |[— Storage account |— virtual Nehoorg\n— Resource Groups, ~ Kubernetes ~ Recoveng ~  Eyppress Route\n~ Maveet place. = OS (mage — Datu lake — Pubuc-\\e\n~ Subsert phon ~ Ve Seale sete | ~ Storage Expicrer | Netoork Inte taco\n— Templates — Cloud seavice, | - Data Sox ~ CON Profile -\n~ Tag ~ Hosts — bata Shaves | Route facies\n— Resource Exoleve r z — Hec caches |  N€hoore Secun 6\n3\nte) MAN AGEMENT & Kove !-\n* Management groups Can include multiple Arve Subsea phon\na4 group P ay P\n* 10/000 management syvoug Can cupparttd in a Single dweetory.\nMAW AGEMENT Love Q Management Grove\nAzv hE Az CE (2) Azure Su bsurip how\nSUSCERIPTION-b Susser Ption-2-\nREsovece Gove REsouace Resovece | (B) Resource Group\nQaovel G@ous\nResource Resouece Resource Resource (4) Resources\n1 2 ! =\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\nec MICROSOFT AZURE - Az 900 12\nAZURE RESOLKE!-\n® AKHev Selechng —7 Region and Zone of Date Center.\n* Below aQre the Importent ferveces as part oF Azure—\n| | Virtual Machine.\n2. Storage Account\n3 | Virtual Nebusone COLE SeevICcec\n* | App Services i\nS | SQL DvDakabases :\n® | funchons\nRESOURCE GCOvLFP'— Tr fc very Important +0 group the\nyesource to a Single onit- So Every resource need to\nbe part of vegource qrour:\n> APTER AZULE RCCOUNT 7 WE NEED TO CREME A AWE Geode\nsefiae * Every Recource Group have a Locamor\n-* Resource ean exist in only One resource group.\n+ + (Resource) > Can exist th the aliHtevent Region\n=e Dt is posciete (Resource @ keqron—1) & ( Resource Qrou~p@ ey)\nKR A Resource iH Resource- Group Con be moved From One\n. ft yesource Yroup to ander vesour ce —4roux loot one\ntive one ascocated In }- tesource OpPur\nREsOUVECE Gtovr\nres) (ver)\nha\nHK IF DELETE , RESOURCE Geour Tr Wie mertom aTiCaLLy)\nDer_eTre ALL RESOUL CEC Linkedin = https://www.linkedin.com/in/suraj-singh-61445522/\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445S22/\n= Content\n\n1 Introduction | |\n2 Azure Services ||\n3 Virtual Machine\n4 Virtual Network [|\n5 Storage Services : ||\n6 Core Azure Solution | |\n7 Azure Security [|\n8 Identity, Privacy & Compliance [|\n9 Service SLA and Pricing [|\nWy)\nZs MICROSOFT AZURE - AZ 900 os\nMoDULE #02\nFe | RRO 1- = AZURE VIRTUAL MACHINE\nAzure Virtual Machice (AVM) (s one of Several ty pes of\non-demand , Scalavle Compu tng resources tat Azures Ofters -\nAzure Virtual Machine (AVM) give you ine Fie pie of\nVirtuclizatHon — tarmout having to buy and maintain tne P Aust Gal\nhardwwave than runig.\nTHINK ABOUT BEFORE CREATING a VM (VIRTUAL MACH INE)\nt Application Resource Name s Operating System Ten on VM\n* | Locahen where resowce Store |6 | Confri guratim after Vm start\n3 Size of VM uid VM Need Vvesource .\n= Maxim ot VM\nFL LOCATIONS - Theve ave multiple locabhoyw iy many geeqraphicaf\nvegions aveund tue woorld. These ave the veqon, (2 called\nLecehens wlhien Specifién tne Locaton °F virtual Made.\nThe way +o get tre availlable loco ave —\nop) ® (63) @..\nAguve porte? | Azuve Powersvel| | REST nes | AbvRe cre\nKe] AVALEBILITY I A2uve Announced an Industy leading single\nInstance Virtual machine. Service level aegg agrement of 99-37.\nprovide you Ae ploy te VM tory premivm Stovage OF al)\nAisks . (BELOW SLA)\n-~Love\n*| 99-97 —- Two or more Instances deployed acress hoo or mare vaitionty —\n*) 99-95% — 2 or more inctance deployed in same pvailibrty Set:\n#| 99°9y% = Singte Instance Vm Using prem wom Ssp/ulPa Dist Pall Ose\na | 99-S¥. = Singte Instance VM wit) standard sso Managed Disks\n¥ | 3Sy. ~ Singie Instance VM witw Standard HdD Magnetic Disks.\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\nas MICROSOFT AZURE - AZ 900 0%)\nHe | AZURE RESOURCE MAUAGER + Mec The Azure resources\nManager CAkM) provides a management Layer that enables\nyou to Create , Update and delete vesources mM your azure\nSule cculphon\nAe we Know, Aruvre & @ an Opevating System. whole\nmanage claka Center OF microsoft: , Lahtch manage various\nservice présent In data centty. CARM> help to intact wort it)\nARM Ik Seger Deployment model tohich Can be yg ,\nFor epptecteel Creatng o> Web opplicaten /.virtual machine /or\ndata Storage. (Bod 4\n7 AZURE PORTAL ‘so VEED TO Communicate FARM\n. (Creation. Option FoR Aam )\nein Gf naune eoweesnec|\nRESOURCE é\nmanag be\n(Ison)\nSeevice Monagmene Gem rempiate) pact CUTER T\nUse to Created\n(Given for creahon) oon pene)”\n(Billug + Access Cour Roundanés)\nqe | AZURE SUBSORIPTION!= To start Leorting with aQ2ure, Wweneed\nQ SUubsompton of Asure. Mauve Subseyp hon provides you\nlot autnentcaton & autumized access to Ayure account -\nBILLING Boon DaRy:- Generate sepavak bling Yepos aud\nWvoice for €ach- Cubsuiphon and Conbol “resources.\nSe oe Wan es\nA2vRE ACCOUNTS\nDevelopment Teast Produchm Subso'(p hon\nSubs crrphory Cubsuiiphen Linkedin = hnttps://www.linkedin.com/in/suraj-singh-61445522/\nae MICROSOFT AZURE - AZ 900 Is\nPABA te Prd\nAZURE COMPUTE SERUICES\n® Compute = MEemoey + PROCESSOR ¥ :\n* Compere csleve appucanm are kun, Computuhon proces,\nInstruction ave epecutect,\nCompute SERMCE LiKke!—\nVirtual Mache\nApplicahon Cervices\nContainer Instances\nKaure Kubernel&r Services CALS)\nWomdow vittiat Desttop .\n\nVIRTUAL MACHINE! Virtuol Machines are Softoare emulations\nOF physical Computer: which Imcuded he— CVvirtuet Procescar,\nMemory, Storage, and pctmaias \\P,\n\n*[Unzom. weenie 21005 [f\n\nCREATE VieTURL MaeyINEr— (@AzZv0kE\n\nlL go> All Services Hed During Creat Virtwet ma ckine\n\n2, CUCKR> Virtual Machine. Vi\" [Basies\n\n3. CH) Add -Virtat Mache = | ® | Disa\n\ny choose—> Subsentphom ! 3. | NETWORKING\n\nSs:  Resoure Group > Name. , & | Manaqement\n\n6 Virtual Machin Nome ®>VM LS: | tage\n\n1 Remon — Setectk Regen ' 6. KEeviIEW + CREATE\n\no \\mage — OS Name (wWrdow F |\n\na Size —(Procaser+ aam)- cout | eres\n\n; (12,000 way HR)\n\nlo. Upername H\n\nNW Passusere . { es\n\na Iw Bou no > ((Roe Jwrie(es)), (at last you will yo prices)\n\n'S: (Open por for publicte)  !\n\nte Dentincrd Ot> imave ters y Linkedin - https://www.linkedin.com/in/sura}-singh-61445522/\naS MICROSOFT AZURE - AZ 900 {6\n+e | APTER VI@TUAL MACHINE CREATIONI- Once virtual machine\nMS Created. (once we clich create button tre page cout\nscvot| to \"ARM Templates (Aruee Resovece MAvAGemeNt)\now\nARM iS Q JSON Document (ovenew> inputy cutput> Template)\n(2sony JAVA ScRIPT OBECTLON NO71ATION)\nonce Virtual machine Created It call] Suow Ale are—\nExAmeLle> (bEMOvm)\nAW AME % Resovece grovel x Locenion | uucerrenod\nREcsov KCE)\n” Demovm 4 | ictua Mach {ho pemo-gtr Not Europe | emo As-[\nF( | Demovm - if | Public 1 address | Demo. URP Warnarepe || one act\n& demovm_nsq Relooyr Secunty Yous Demo -aee | Nota Eurepe- Demo A-{\n|\n2 bemovm 334 Vetiosvic Wterfece | Demo Gee wot Emope || deme rt-|\nJ | | Demovm_os-pis| bisa Demo -Uey wort, Eurey> || de me As\n8 bemo- arp. net Virtnad Nero, Demo- GKe Nom €urop— |] pemo As-4\nes] Netto ovgquatcher || Network Warner Olea - Uae Nutrn Guo | Oe mo R41\n@ Tor\nSs \\F we preys\nPs Stop Wean\nire milling sro?\nQ) PRess—> ConwECT Button 7 open py (RPP > SS H)\n“ vv\n+> [ ® PubUec |P= ws HR RR\n> \\F Poenumber= Eo\nbovontload ROP File\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\nee MICROSOFT AZURE - AZ 900 {z\ntH) AZURE CONTAINER SERVICES\nCONTAINER 18 HELPING us IN CREATING lsoLATeo WORK\nLoack ON THE SAME PHYSICAL MACHINE, BOT AT SAME TIME\nTNEse WORKLOAD DO NOT NEED OfbARATING SysTEM\n(%& In general 1F We run 2 VME both need Bun OF to\nYon the resource, to avoid Such requirement neeq> contatners )\nCiwmaye File Stove)\nRE ‘\n(pevevoren ) ae (usec)  Yeneare\nCape Stove) Commtrain\n—7 ¥lontainey are created to ron tre applicahin at UAer bende\n—7%* Container ic light weight, ( doee not have Operahne sgtim) De\nwill alway vuped +0 underline on cohich it & Created.\n7 ® Contamer oct nor Yeau ved Operahng Lystem\nda | AZURE CONTAINER INetances a PaaS obering tnat yong @&\nContamer im Mauve, cottnout tne needs to Manage & virtual maah na\ntr [AZURE KUBRRNETS SERVICES !- Qn orchesbatont Services for Container\ncortw duiibuted Architetture and Large\n* KUBERNETS > Is open source CSofhsare, Lohich & also\nCalled a  orcheshahon Services. hetp to achve and (nteqrated\nmultiple — Contamer colth tearr amount Cf -elfurts.\nte (Ph Azuve—> It lu eovnny an ARS.) , Need to creak.\nmv lye VM and top of VM need to create muthple onmines-\nku@eewets Seevice > FREE\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\na3 MICROSOFT AZURE - AZ 900 ig\nVIRTUAL MACHINE SERIES t= CxS)\n¥ Gee-21)\nseers |VM SIZE EaMiLy USE Cases PRocessok PREIEMSIart\nA Entry level vn Test Cerver, Server Pod 893\nD General Purpase Compute | Appl(canon , Dakibace 24 99\nDv Next Gen- General Purpose] By tenerse applrerahon\n& Compute Ophmize-VM | Batch Processiig , webs QD\n4 Memory 4 storage Opting ECP, SAF, SOL 22,0¢8\ny High performance-VM | High performace seeped Gl, Fey\nLC Storage Ophmize-VM | Mongo Dé , DS Laluve 32,28\nS GPU enable — VM raps co , Video — Q?, 233\nM Memory Ophmired- VY Required Massive $0,209\nParetiet Compile -\n% | AVALIBILTY ZONEI- HELP TO MAISTAIN SCA 99-Gs51. IN Care\nOF Pi Sasteryw RE Covtey-\n| PRICING CatevtmtoOh!- HELP To GET possi bce Costs Pot\nHosting Hest Resource IN A2vRE.\nHL AZURE MARKETPLACE!— FOR DEPLOYING AaVvKE LUBSCIPTION.\nwe | ATURE VIRTUAL NETWoOREI- Dr hosts tue Wirral mechine td auc.\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\nA 14)\n= MICROSOFT AZURE - AZ 900 no.\nVIRTUAL\nNEqWOKe\n| oe Mines 4\nbiste Med INE < 7 DD Keg\nNETwoKK | _s /VieTU AL\nSEcukiT NETWw otic\nON\nSSve sector 7 for Billig purpose 4\nNR Eee Oem\n| 4VIRTUAL MACHINE! ComPUTE MACHINE ON AZURE PLATFORM -\n> Los dIsK= VM having Auk associated oth, vm ,(stormg \\nformation)\natached other dulk fo hesr Pate Duk\n3 4 VIRTUAL HETWHOLKIC— Ig an woloked netoserk on cloud, Virtuef\nNetoork help to Create a networtigs parts «(Who Interfoca ord)\n4 JBETWOKE secDaTy— tached to Seaure |n Lout bound date\nS [VIRTUAL NETWORK™ Attached on VM Machina [Lb a nehoort.\n\\urerface Card\nG [POBLTE-\\P AODEESS = Atlous fo Comput karoge \\wheenot.\n7 T REgouece Govl— All ave part oF Logical Yrove called Vesouree\nGrout\n@ + Sue SCAIPTION — For alloy purpose .\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\n°\nas MICROSOFT AZURE - AZ 900 20 #3\na 4 STEP TO CREATE- VIRTUAL MaeH INE:—\n(4) BASTCS (3) PASE AREAL NMETOORKS\n(| Subsoriphon — For billy \\ | Wriuay Netome—\n2| Resource qeroup— Lopycat qour 2 | Subnak—\na| VM Nowe- VM Wane > | Puble 10 —\nu | Regon— Ex—Euvope / vb & Yipic Secunt4—\n6 | Poraitrta liky T) Inbound Ror —\n& | Image Operaheg System\n7) Size— VM Size Cveher— talore) (A) mans EM EN T— Configur Mmenttorntirg\n© | Unername— ReRX & management option for VM.\n9| Passomt— wm x\nto | Inbound Poss ROK (23 OA) NE. ([S)movAceo Aeldttonah agent, Sonpla\nOT TOT TS TT or apetreation via VM\n(2) visels:— (e) TAGS Conscu dotted bills by apely\nl}os pick- SS D/HNoD Same tng te multiple resource rou,\n2 | ese energtion— (2) Row buy + CLEATE\n3 | Dada Dick\nHou uy Estimuattoss = (0-123 Usb /He)\n¥* Mer Created we have belos vesource He In Virtwal machine:\nExamplar—- CDEMOVM)\nName Type | Resource Groxp . ene Subseriphny\nee ee EC\ndemo- 4rp.vnet Virtual We hoortc ! clemo -arp WermEurore | prc-L\ndemovm ‘Virtwal Machina | Aemo-gre nw 6\nAemovm ~ ie | Pulte IP address f Aemo-4rP tro ft 4\ndemovm - néqr | Vetroorte Secunky Gro; demo -gre ' uoe ”\nAemovm 33 + | Nehoot. ktenfa ce | demo-ger yor ot,\ndemovm-os Dis h-l- ' disk, , DEmMo—-Ger Ha Loe\nNetwork watehed- | Nehsoria watcher ( velcore Wskeay * ” | ”\nKAVALIBILITY ZONE —> HELP, TOMANTAIN SLA 9995). IN Case\nDisact ee REcovey yo j\nLinkedIn - https://www.linkedin.com/in/suraj-singh-61445522/\nLinke n= tps inked com in/sur-singh-61445522/\n=s Content\n\na Introduction | |\n2 Azure Services ||\n3 Virtual Machine [|\n4 Virtual Network\n5 Storage Services | |\n6 Core Azure Solution [|\n7 Azure Security [|\n8 Identity, Privacy & Compliance [|\n9 Service SLA and Pricing [|\nas MICROSOFT AZURE - AZ 900 By\nMoDULE #0\nte | VIRTUAL NETWORK When We Created Q virial machine\nW azure netoorkK tr wolll allocate a > Virtual netoorn Interface\nVIETUAL NETWORK INTERFACE: Mange all traffic movine\n\\n -and-Ouk of the vittial machine (Cvia— virtne, nehnser K-\n-ktyface). so me datr How via virtnad nebo Interface\n1. |> lp Apokess— help to locate a machmne\n2- |> pueitc te Apoeess— help to locate machme on internet\n3. |> PRIVATE 19 ADDeess- help to Locate machine on Locat- nehoore\n* |? Luanets— [+ help to separate one or more subnets:\nGn “ayecqeR. pease fee SLL ape\n# A Collechdy oF (VM) | Connection vita Dedicated lease routs\nComputer 1 each otner| Mrernet aud Sending | onty for Connechon\ne Internet > Public If WHFHE wottr Cupportof| betoeen (Ryure Daln\n© Intranet Private It Enoayphin | Decryphoy Center @ user )\nRwhen we created a WTtual Mache, frufemeahaally Ur will\nCreate a Virtuad Nehoork:\nOK) CREATE VIRTUAL NETWORK NETwoRre \\\nSEcvRi |\n!\nOSs VieTu AL VIRTUAL PuBtc\nDISK MACH INE VeqDOR l? |\nINTE Rept\n— Private 1¢ = 10°0+ 1° Oy Bia es \\\n— Puarcles (3-79. ¢¢-72 \\\n|\n|\nSUBKETE 10°0°0:0/2y\nVIRTUAL NETIOOLK = (l0-0:0°0/14)\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\nae MICROSOFT AZURE - AZ 900 a2\nCREATED VIETOAL NETWOKK: Creston of virtual nehoork i diuded\nmito & Steps— ©\n(0) BasDcs le AopRess Stevert TAG £ REVIEW + LEATE\nSuesaiphon > \\ev4addvess |» Host Tavgare | (Kenew £ create)\n+ R-ey1 on Grr - \\tvd addrey | Dodus Protect | Nane Walue\n7 stance -Nane |» Subnetec > Areva fel thar\n+ Instance begin Crates you\nte Conese\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\n=. MICROSOFT AZURE - AZ 900 12\n| TYPE OF NETMOKK ConmECtion!-\n1. | VIRTUAL Neto C PEERING) > More secuee”\neee\n2. | polyT—h- ate\n3. | Siqe-b-site\n|) | VIRTUAL NETWORe (PeERING):- Thic basically help 10 Interconnected\n+90 ali flevent vitnal machine ‘ooved on Private lPaddrers\nVin =) Ved\nPrvate [Pp addrew |g ip) Privat |Paddnn\nRb Sythe ote aan ——_—oOoO OO\n10-0-0-0/16 200-06\nPomp -to-si@ VPN Connechn\n2. PANT — TOW SITE } 3 SiTe-To- SITE\noe\n'\n! '\nt Vm-} ban ne binned\n= a 1 10 N\nOver ] ' Putace if M b\nLoney) || Gatetogg | JV |\n3 ; ! locat\nPubic ! Privat lf | Nepo\n\\¢ pie\nGenta\n'\n' Prvece | VM\nVPN Sela Support (025),\n. mee '\nPom tt StL VON Compectys | Ro chug 1 Hho Cuco vOurer er machine\n| Welp To voute hayic On internet”\nSince In formahin Haro bow Inlonel |\nSo we are wii VPN Aaland | LOCal Nfs Gateway’ — Informeahan\nts aaalus Thiers, Cocure, L police If op local nehowk boly\ni\nt\nVPN Gateway > Atta ched From La)\n( tev tre haypic\n|\n{\n. Linkedin - https://www.linkedin.com/in/suraj-singh-61445522/\nLinkedin - tps ora Ainkedin.com/in/sura singh 6144S522/\n= Content\n\n1 Introduction |\n2 Azure Services |]\n3 Virtual Machine | |\na Virtual Network | |\n5 Storage Services\n6 Core Azure Solution | |\n7 Azure Security [|\n8 Identity, Privacy & Compliance [|\n9 Service SLA and Pricing ||\nas MICROSOFT AZURE - AZ 900 24\nMoDULE #\nAZURE STOKAGE SERVICES — belore are tue tage oF sherage\nSevvies, tnic ave te parts of Azure Storage Servies\n@® eross @ tases @ aveve HLE @)\n- \\mage Table dare = Sending Meseage\n— Video - feei vies mesage\nHOW To Apo}\n0) ———\n(es) CLIC KA) CREATE BUTTON |\n©) SELECT ofTion \"storage Account” |\n9\n0) BASTCE — Account name | Regon / Performance\nie) ADVANCED — for seaure the accounts\n® VETW OREING — Public / Private 10s\ncy DATA PROTECTION - bete hen or Medificahorn\n© TAGS — Caregoriza ton\ni) REVIED + CLEATE — final cveahon oa\nACCOUNT\nLinkedin = http://www Sinkedin.com/in/sura-singh-61445522/\n+2 MICROSOFT AZURE - AZ 900 2\nAZURE STORAGE Seevices\n\nYM CONTAINER STORAGE (BLOB) — te optimized for Storing\nQmount of unstructured date , such ag text or bmary date\n\nQ dish STORAGE provider ducks for virtual machines »\n\nAppliCahoyn , and otner Services to access And use-\n\n@ AZOLE ALES'—  cete up higaly availiable nehoerks file\nShaved tnat Can be accessed by Using: tne standard\nServér message block ($8) Protocols.\n\nAZURE STORAGE ACCESS TIERS\n\nHot Cool AK CHIVE\n\n‘ ophmized for store * Opnmized tor storing: ophmzed For stoning\ndata thal ta Aaka thar ia data thar & raveta—\naccessed infrequently accessed | accessed 0 Stuved,\nrequenttyy mat least 30 days wat least 180 days\n\nREAO £ WRITE\n\nAOE VERY fee)? (ess ee) (Read Plone Weppeunos)\nMost Cost Read 4 conte Cost Thu cu\n\n4) (' Rae y) (7 Cay to stove )\nDM wirl| ake Hue-\nOrn\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\noat MICROSOFT AZURE - AZ 900 2e\n\nExXeLoRe AZURE MARKETPLACE:- tb similar like Play Store,\n\nAzure marketplace allows customer to find, try , purchase and\nprocess applicaton and services Prom hundrede of bead ing\nservice providéy ,Lohich are all certted to von on Aoure-\n\n(7 Open source contaner plattorms\n\n27 vittuel machine and Oatabase Wm ages\n\n37 Appucahon bulld and Deplogment Soft ave\n\n#7 Develovev's tools\n\nAZURE DATARACE SERVICES\n\n* AZURE Cosmos bATtAGASE'> te a globally= Aishibuted clatebase\n‘nat elasheally and Inde pendently Seale -\n\n* AZURE SQL DATABASE? Is arelatonal dambase ac a Services\n(Daas) based on tne latest version of micvosottsar\nServer dotavare eugre:\n\nba AZURE DATAGACE FoR MY SOL & a Fitltge = managed My So\nAotabase Services Far appUcahem Aeveloper<-\n\nB [azuge Dataease for Poser SQL> & a relahonal\ndatabase services based on tne Open-Source Poshavee\ndatabase enganre-\n\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\nLinkedin htps:/ war nkedin.com/in/sursngh-61445522/\n= Content\n\n1 Introduction [|\n2 Azure Services |]\n3 Virtual Machine | |\n4 Virtual Network [|\n5 Storage Services [|\n6 Core Azure Solution\n7 Azure Security | |\n8 Identity, Privacy & Compliance |]\n9 Service SLA and Pricing [|\nss MICROSOFT AZURE - AZ 900 24\nMopULE#\n\n* CORE SOLUTION!-\n\nCORE AZURE SOLUTION AZVRE MANAGEMENT Toots.\n\nt}loT to Azure Cpheve ‘Portal, Powersnet|, CLL\n\n\\ | Synapse fraly ties to batabricks | Advisor\n\nW] Arhated Mat etlipence We Monttor\n\n“| Machine learning WW Seyvice Health.\nve\nVo\n\nee\n\nRK | AZVRE- INTERNET OF AHIWGS— oT describe phy sical Objec ti\nthat ave embedded vstth sensors J PYOCESSING- alility ,.s oftware\nand otner technologies and tnat Connect and exchange date\nLetty otter device and System over tne Internet.\n\n¥ AZURE 10T CENTRAL!- tc Fully managed global loT Saat\n\nSoluhon that makes ft easy to Connect , monitor and manage\nloT assets at seale-\n\nKX | Azvokee lot hves:- a managed Services hosted m the cloud\nthat act Ie Central message hob bi-direchonal Communicahern\nbetween l0T Appucahon ang the device tt Man age.\n\n® lOT CENTER % ( DASHBOARD) > TOP @ (let Mos)\n* loT (Build top of hub, It has U2 to manaye multiple\nCENTRAL dence) _ _ -\nK lot evice Cond by lor Mb)\nre be y *)\nLinkedIn - https://www.linkedin.com/in/suraj-singh-61445522/\n3 MICROSOFT AZURE - AZ 900 QR\nFF BIG DATA & Rwy TICs 7? over a period oF hme, mors all\nOrgantzahorn Is Cap turing a huge amount of data. (by > Develoe\neCommerce website)» Hove fam many people vetted ebe-\nHere big- bata help to analxe te compete data: heve we\ndump all date and Usieg ETL Too| and process tre date\naud create a datwn Whovehoye- Below ave we BQ -dater\n“Tools —\n—7| R AZVLE SYNAPSE ANALYTICS -— A Cloud -based enterprise\n| bata  Warehouge- Solution , (Epa mpe > Hadoop )\n7X AZORE HO INSIQGHTI=  & Fully — managed 7 Opeh sourced analy cs\nServices for enttrerites .\n—7\\|R AZURE bATABAICKS!- Apache spark based analytes Servic eg.\nWese three ave sae i tommen ped {0 analyze huge Data\nFE) ARTIFICAL IWTALLIQeu CE d) MACHINE. LEARNING’ (MUis when\nUnderstand te pattern ant leavn ne output wir help of @2)\nAZURE MACHINE LEARNING!— Cloud-based to develop, and\ndeploy machine learnings mode}\nCOGwITIVEe Seevice:— avicklLy enable app to see, hear, Speal,\nunderstand and imterpret a upev’s needs.\nA2URE BOT SERVICES! Develop Intetlyenr Jn tea Prue grade boa\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\nas MICROSOFT AZURE - AZ 900 2g\nHr) AZURE MAwaRUEmMeENT Toors!— Below are tre part oF AMT\nCAwWkE Man aceEMENntT Tools) —\nARVLE PORTAL ARVRE POWEY gyecr\n[axvee PORTAL |\nAZvVLE MOBILE APP COmmMAVP LINE interace Cet) |\nAZURE Rest APL [ AZURE CLoOvo ar |\nAZURE REsovece Managetc (aem\nH | POWERSHELL” Install power Shell locally, Cveate a resource qrove ana\nvirtuad machines acces® and vped the Cloud sell 4 renew\nazure Advisor reComme nolttio\ntH [AZURE CLE!— Install tne A2uve CLE Locally »Create a yesource qroup\nand virtusl machine , Uge ne Cloud shell and revu ayure recommend\nWHEY ALL MODULE INSTALLED, WE NEED AOVUOR( AZURE ADVIS Oo)\n> Probably we are not vse / or Ueing VM _ upto tre capa\n> Only one VM k 1s try LAE ,anoter dhe in not Lpe mudr-\n> MSo,venen deploy loovm™) [+ & Mot posible to Analge all vm-\n# $o tne sol 14 Azvee Aovisoe) *\nLeecer\nAZURE ADVISORI— Pmaly ze depo yed Azuve Yyesource and\nmakes retommendiaklorn on best practiced toophme Azyre deployment\neltaler lity\nSecunky\nPerforman ce\nCost &\nOperahon Excetlen ce\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\neS MICROSOFT AZURE - AZ 900 20\n(FREE)(Resovee)\nAZHRE Avdvisot!— Pratyzee aAeployed Azure resource and\nmake yvecommendiatery on vest prahces to Opnmize Deployemen €\nI Reliability 4 Teese\nis Secunty ST Opernbownl Epctllence-\n°F Performance\nAZVRE Monviok!— Azure monitor maximum tne availibility and\nperformance oF applicahoyn and services boy collechny , a i\nana aching On telemetry from cloucl and on-premises envirenmenG\nApputahey merger AVALIBILITY 2 PERFORM ACE\nLog Pala rics : > How Much Raw vse?\nSmarr merte > How mom CApacity PRocescot Use ?\nPutomahon Achont > If WORD InceeaceD 2\n. OOOO TL\nCustomized Dash board ls a Mert gre chergdt wee Cr0 UhI7 Hy\nGo> Resource —y Powershel| Ra —» vm\nGreve\nChee Wein se per te weed.)\nXH\nMETRICS AE LIKE!— score CSwetevm)\n| CRU CREDIT Remaining METLUc NAwE\nw | bata bist Bonduer dhe CVICUae Meee 1054)\nWe | data DUE lOFF Consumyhon METRES\n“ Dise write byte ,\nVN) Nenoork iy THRO AGG REG RAT 10\nVite z\nvn, | REERMERYErELA Percentage - CPU. ‘a |) -Wve rage)\nin.\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\nae MICROSOFT AZURE - AZ 900 2/\n% (_A2VRE seevick Hem qTy:- Evaluate the impact of Aruve Sevuices\n[Ssue@s woth — percowalized guidance and Support, notheahon and\nissue vegoluhon updates -\nDn case any eure ceric K down, tren [toil]\nnerHect in (service healt) -? once sve Vesloved (tr atl\nbau acne oan on ene\nTe Heck he update on tne Sam-\n* (Uv PLAw eo ooTAge / PLANVED OviAge) AR\nHK | PZLRE KEsoukcl manauce (ACM) TEMPLATES —\n& Btu aq Isow Hic )—> Tt & declerahix\nAzure Vregource Manger C AEM) templates are Javaseryt\nObeck . 4nic Can be Vaed +0 Create and Aeploy Aruve\nMPras tructuve. Lo the ut haves to wont program commands.\nbeclarahve Sym tay (l) Wo need to wnte program\nRepeatable Cesutt .\nOrcnesrrahon (2) No nced +o wnle C eputncrep\nModular Piles powershel| etc -\nBuilt-in - validation\nPape bxparialla. Coole\n— a,\n% | Beauty of Atmi—\nW| te Hou Implement Adm ang t\nYun particth go meae we\nTe—eveeule iFeoil| clo he new\nChanges toienr Tee to\nQ ALM in Cond) by Pava meter\nFile (Same templake UNed\nmv th ple envimnemeut)- Linkedin = https://www.linkedin.com/in/suraj-singh-61445522/\nLinkeln= tps: sinkein com/in/sur-sngh-51445522/\n=3 Content\n\na Introduction ||\n2 Azure Services [|\n3 Virtual Machine [|\n4 Virtual Network [|\n5 Storage Services [|\n6 Core Azure Solution [|\n7 Azure Security\n8 Identity, Privacy & Compliance | |\n9 Service SLA and Pricing [|\n& MICROSOFT AZURE - AZ 900 Ze\nMopule +\n+#|  SEeueity:- The Coming loc s coll Cover tre below Sechan_\n| Aves secoarty tearvees | Azuee Wetaeee Secoecy |\n~ Security Caste * DEFEWSE Ww obeTH\n+ RESDO RCE Hy diene “ NETWORK CecuRTy akour$\n~ Key VACLT “Mew aces\n* peoreatao Hosts “  Doos PROTECTION .\nO11; AZURE CEecoRiTty Chute + In azure we are Creating &\nresources. (vm Machine, Storage) And all these vesource ured\nto be secuved. Me seeurity the resources are from—\n(0) Protect Rem malware -\n12) Protect From by autronred Access,\n(e) Protect From potential attacks / hackevs -\nWhew Possial€? IF hell is some prodem in our in Prastructure-\nAZURE SEcueity CewTeR IS mbuilt I Micrsoft Azure. and\nIt lg monttoriry Ine servvides like (Virtual machine, App Servic)\nDatarase, Storage). BM proude beet protechm to bol,\nAZuve and on-premiue Pala Center. (In Case deployed iy dhe\nVirtus, machine):\nProtect ” Cane CE VICTUM MetniMe , NOw-aAz2URe VIRTUAL MACHINE\na\nAA Bis Pneente Wee ARR EE\n2 ThinG Here %Y Céeveity!> pert clide\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\nae MICROSOFT AZURE - AZ 900 23\nMICROSOFT S£eu City cen ted, (Azvte) Air ill hete you:\n\nWw | Provides Secunty Yecommendahon— Lt will hele to assy nment ang\nQivVe YA YC Comméencdahon\n\n& | Detect and Block, Malwave:— Tt will hep to analze tne malroare\nattack, immediatly vend alert. and potect vA from such king oF\nmalioare attack.\n\nBt ill detect 4 blocic— Malwere anweul,\n\n@ | avay2ze Ano 1 een Ti POTENTIAL AT Tac KSi— De will help Gnalze\nprotehtial attuel & help to dente tue Same.\n\nWW LYUST- Iw- TIME A ecess Conttor For poet:-\n\nSuppose we have Q virtuel machine , kweo novo we Need\nto oto Pp RDP (Remote Desktop) to that Virtual maching So for\ndome, the RoPon vyrtuak machine. Wwe need to open tne Lomdsw\npot no (#3329) and We are pry SSH login iy Lunyy ( Pert Bay\nee ne eas\n\nInslead open these botn port favamentalerr. we cay lontgur\nmM such a way trot on-demand. When we heed bb do ROP wv\nson System to that wituel mache, Mat hme only Port vail! be\nSper. And these  partnf uolll| Cloned aurtomancailly— a Fre Pus\ntime - Frame -\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\n= MICROSOFT AZURE - AZ 900 2¢\n62. | AZUKE SEcuRITY CEeNnTER- CAPABLLITEG:- ( RESoURCE HY Genes)\nUOPoLicy ComPLiance)— Lith help of policy we install Some Agent Spe)\nbohicl basically help Analze the virtual machine. Qnd Submit the\nweparr i) (Leg amalyst's) +\nAs Seon we Created @ Azure mMachwe, (re agent Softioere\nls metalled wh the Same and thu WW done under poicy, Compluanced .\n(WY) QOvnTmvovs ALECMENTS!— Assess New teployement wesourte fo\nensure that thes Ore Confiqure property. AS Soon neD VM\nCreated watt help oF cent Sis) MStal| and helpus In averment-\n(ly, OT AU KED RecommenpATiON!— Recommendations hated on exishig\nwerktoad —vottn Inghuchon, on hoo to \\nplement trem.\n(om) HREAT PROTECTION!” Amaley ze atttmptee|  thveats thou 4 alevG\nand mpbcted resource report:\nFt] Eyawele © Poetals\nControls May cuvvent Potenay Steve Unhreot Hoy Reource Athen\nScores Stores lnomase Reroute a a\nO7 |Enaie mca lo ° +18% ( lwpomts) | Lord =\n@ 7 | Sere & ° ttyl. ( $ pols) Sofs Wee\nManagemen\nPorts\n® | Ewereyr dota} 4 3.1 42°71, (0-8 I Poury | Lola ——\n9 travel\nUnkedin - https://www.linkedin.com/in/suraj-singh-61445522/\nee MICROSOFT AZURE - AZ 900 BE\n- AZURE Sutiwec!- Azuve senhnel cia secunty Infermahon.\nmanagement (SIEM) and Secunt4 automated nes ponse (Soak) Solution\nthat provides secunty analyhis and treat Intelligence acvast an\nenterprise -\n° ottce 36r\na _——<\nIN TE GeatiONs Azuve Advanced Thvear Propechon\nMiwwsolt cloud Appucarin Security:\nRO)\nCOLLECT!=-| Collect data of all user, device, application & Infrasucture\n®\nDETECT S~ under Post-Proce sates, deteckinp te tmeats (Gased on At/ mL)\nINVESTIGATE] nveshgahin oF thrects:\n®\nRespmded by Invokity Some Kind oF Yo too\n03.| AZURE KEY VaULT!- peuve key vault Store applicahon Secreats 14\nQ Cenbalized cloud locahon In order to Seeuvely Conbo| access\npermissions And access logging -\nWY storing secrets Baeneo by HARDWARE SECLEITY MopoLes CHS Mm)\nOQ | secrets Maw aeemeny\n@) key Manraqemeny\n(4) ;\nCELTIOATE Man ng emeN\n° Linkedin - https://www.linkedin.com/in/suraj-singh-61445522/\n<2 MICROSOFT AZURE - AZ 900 B¢\nRw LAZURE DdDenicateo HOTS:- Azuve dedtcatecl hott provides\nphysical Servers Ynat Wost one oy meve Azure virtual machnes\nthat is dedicated to a single organizahon load\n%* This will help allocated dedicored hardedve\nyt? Specitte — Subsoiphon>Cvm, stotaqe veil| allocated dedicated).\nWw  BeENeeits:-\nRY Hardwave isolation at tne server lerels\n* Cenhol over mamttnance event Hinaiy »\n* Ati gned. wit Apure hyd use benfau-\nSEcURE NETWORK ConnectiMmty:-\n(How ‘tne virtual mache 1s secured —?) ¢y—\nOl. | DEFEWSE Ib DEPTH — A layer opproack. to secure computer\nSystem - or Virtual nehoork.\n|| Provides multiple levels of Protechiong -\n“7 Attacks against one Layer are isolaton From subsequent layer\n) © © 3) ie) 2) (2)\nad * Hacker is mayer ly \\ntererted To. (password, Cratitenal deta\\)\nSo the protechey required ot eo Lovet -L\n> HWadceey need to hack Cac layer Staring From Physics Seounty\nto tre Darn Poi. -\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\na MICROSOFT AZURE - Az 900 BP\nSuinkeo Security MoOPEL‘- Aguve pronde Shared Secon model:\nWy thic Certain tatnay ie ake by Micresott and (enaiy ting is take\nCare 4) cu lromer.\n(ty- Below are he detril Around tre tye ct model )\nDate Govevanance and — Cosye me Cobromer bose mw Co Stomer™\n~ Rigne Management\nWuhowt tonna mimoaetr |\nei, yet rt\nmucrosstr\nPageret tne ier | cattemer [mmr\n(# C= Micwesott/ Customer)\nNEtwokic SEcvekt Geovr — NsSqQ Filter netoovk Waffic to and\nForm , Azuve resource on Azure vival nehoovk:\n*k Set Mbound & obkbound yules oko Filter boy Source and\naeshnahon |? addvess ,port® and proincol:\n¥ Add moltple ‘rules 7 a8 needed just Subsophon limite\n¥ dverrjdes Aefaulr rules wlhe nero h wher pranty «\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\nfe MICROSOFT AZURE - Az 900 3x\nAZLRE fFrebwacc!— Firevoal| as a Sevvices ( Faas) dnat grants /denies\nServer access based On originatixp If address , m order to\nprotect hehoovt resource:\ni Applies [Inbound and outbound ‘rattle Filtering rules.\nWoy Burlt-m high availasitiry\n\"oT unrestricted cloud Sealatcility\n“ User Ayure monitor losang\nAzure — Applicate aliens ay 7 Ms0 provides @ Hreoal| , web —\n—Oppuicahory fyréwal| (wat). WAt provides Centralized ,\nInbounded = protechon from web applications:\nAzv Re DDoS ( Diste1 eo Teo DEWIAL of Sedvices) PRETECTION\nIF a Sewice Veg uested From molti ple Locahew acrois yrote fe\na Specie Sever, coher lead Slo or un Vesponsive » So Aaure\n1s destgned fo handle Such Situahon and block such kind of Yesuest.\n[rue asic veesion oF dos a ceee|\n* Tr wlll ciutomaheally detected tnat hartle u not coming From a\nQennsian Needy uper. but From a(Bolt) and It will blocked:\nAzure backbone Is pre—proqam to handle  Sech cikvattery:\niS Ensuring For fevver avattibor ty th acer cur “Yea west\nW] Basie Service her & automean cally Rnable\nWo] Foy standard servuwe ter adac mihi gahor Capabilities (2 nll\nWL hetp te ger more detal| Rom where attack lapper,\nATTACKER Inenea Renee =e Lo] Azure ppdos I Virco Ate\nBAackGove Cera PROTECTION 2 NETV OAKS\nNey poss blue ©)| shep rea)\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\nLinkedin - tps: tinkedin.com/in/surj singh 6144S522/\n= Content\n\na Introduction [|\nz Azure Services [|\n3 Virtual Machine [|\n4 Virtual Network ||\n5 Storage Services [|\n6 Core Azure Solution | |\n7 Azure Security [|\n8 Identity, Privacy & Compliance\n9 Service SLA and Pricing ||\nee)\ncans MICROSOFT AZURE - AZ 900\nMopnuctE #\nAZ-900, IDENTITY, GovERnance PRivacy & ComPulAr ce\nNey\nAzvute IDERTITY Leevicec:—\n's | kuthenHeateon vevus Autuorgzation:\n2. | Azuve AD, MFA, S80, Qnd Condihdsn Access.\nAzvte Goveenarce Peaqwee? —\nVe Rea\n2. | Resource locke and tags\n3. Polley 7, blue prnt’ and CAF-\nAZv@e PRivacy & ComPuUAVce!—\ni * Privacy statement and onlme Seevicen Terms .\n2. | Trushe center and Compliance dowmenthonc\n3. Azure sovereign VegaQht.\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\ncs MICROSOFT AZURE - AZ 900 46\ntl. | Azuee IDES TITY SERVICES - OBJECTIVE PomAiNi—\n7 Explam tne difference beteeern auhtnentcation £ autuenzenns\n> petne Azuve Achve Divectoyy\n> Describe the Funenann k vsuage of Powe ache dict\n= pesotbe tne aah and Upuage OF Condinmal Atcem,\n- wv Up le - Fackyr (MFA) ana Single — Shay (S80) -\nFR | Autwerh cation Autusn yarn -\n—— 2.\n~ lAentties tne person or - Determmer an authenticated pensions\nfeaices Seeking actew tea ~ or seevncek Level acum\nYuource>.\n— Regret  ledthinyat access |-  betine toluene dotn Hticbay Can\nCrede-pab . ACCES , and What they Canude\nwort ‘*.\n- Berc tr Creaking Se Cure_\nldeutty & access Conrms) ( Bered on sdemipiing » prenvde the\nPruciples aceem to ne Seeviter uw cated\nAeA) .\n(The process oF identi Fa licg\ntre perso tobe (a \\amed) u\nCalled Auntnenhcar m0\n® | AZOLE Active DIRECTOeY CARD “AS aR Micrscoft Azure\nCloud— based tdenhty and acess management Setvices\n' Auntnenticahon (employee sign-in to acces vesourres)\n2 Sing te- sing-en (S$o)\n; Appltcaton Maragement\n* Business — to-busnegs: (82 8)\ns Bvemess—to— Customer (82C)\nLG Dence Manag emt nk Linkedin - https://www. linkedin.com/in/suraj-singh-61445522/\na1\n<3 MICROSOFT AZURE - AZ 900 bea\nR |ComDITION ACceSss— ts Used by Azure Reh Diveetory te\nbring signals togetwer ,to make dlecicions ,and enforce organizahae\npelt cies -\nUser or Group Membership\n(e Lo Cahier\nDevites\nApp Ucabiory\nRisk berechen\nRist Detecton> tty help of ML/AD Azure detect the patter\n6F logm like Caty / Country ) If foo Login attempt dene for)\nTandom Locahm ~It will auto derected rusk Detechay |\nASSIGN ROLE Access vicad THE petivity\nhoc e loy's 4 Remwet *\nROLE FRSIG NM\n© ®\nUZURE POCTAL =P ACCESS Comroe Claw)\nCheck access = (Add vole ase nment) >\nPOA AR AS Sacra prores sWbsay\n(Z\nG) sign Rleo>\nOwner\n@ubhi betor\nReader\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\nZa MICROSOFT AZURE - AZ 900 Bc\nAZURE GOoveknArc£s METHOD LOGIES\nRBAC (ROLE -Bacen access cont eer)\n7 fine-grained access management\n+ Seq veqrate dues vith he team and grant or the\nAmeunt of acesc +o vsber tat they need 4° peoforn,\nAneir joes :\nEnable aceess +o tre Asuve partap anol cit acces\nto resources:\n* Giving vole +0 parheular secunty prnciple pe-aceu\ntne resource -\nREsouRCE Loce’s | pyotect your Azure yesource from\nacardentaf deletion ov moditicatory , Manages locks at\nSubscrphon , resource qroer, er individual Yesource lee}\nvoity  Anuve pate.\n_ ST\nCan Not Delete Yes Yes abi\nRead onty Yes wo wo\n* Mosty sexvicet are not aste to ve-calt.\n® Vesouvce lock help +o prevent from ace dente detehom .\nsive AZUCE PORTAL 7 SLSScRIPTION > Sirecky SERAEH Winperd F REcouece Lock,\nyack ee\n¢ eae 2 (peci-e) Lorkige> {aedifeetery > on) ee\n® Read al\n(O) Linkedin - https://www.linkedin.com/in/suraj-singh-61445522/\na3 MICROSOFT AZURE - AZ 900 49\nTAG S1—\n* Neen useful Fer rollma vp bel iv Mfor manny\n& pefine CKEY4VALLE) To de tre 12g for Avy esourcte br\ngery te malTee Informatng .\nx = Provides = Mefn dain fer a Pauve Yesuvees .\n* Logicty Oo qAmared yesource intdD ao tnyanony-\nAZUVGE PO@TALS— Enanyrlt :—\nio ) e 6.\nAZURE PORTAL DY LEFT SIDE EyProee SECTION > TAGS.\nREESE IPOe i Azuve policy help to enfore organteahona|\nStandard and to aaew Com pliancet at-Scale, Provides\ngovernance and resource tonsistene wo iy vequtatory\nCompliance , eae wst and manafemen\n> Evaluates and identities Azuve resource t+nat donot\ncomay roth Your po Udes -\n—-) Provide bu lt -i+) policy and Inthahve — defmatons., under\nwean auch ag storage ’ netoorking compute ,\n6ecu center and Momttorirg . ©\nAZURE PoetTAL EXAMPLE: — ries Dehrahn\nSEU SE PO ETA ERA EE\n® >) ® fessiaenmen Gh\nAZUCE poeta =F SEARCH PD POLICY 7 | Merman\nLinkedin - https://www.inkedin.com/in/sura}-singh-61445522/\n= MICROSOFT AZURE - AZ 900 5O\nAZURE BLU EPRINTS = makes iH possible For developement\nteam = to rapidly butld and stand up nero envivonement -\nDevelopement team can ae ice build +ust throws,\nOrgauizahonal compliance wrt arset of built-ty com ponent\n(Such as nekoorking ) Mm order to Speed vp alevelopement en i\n\n’ Rote Assignments *\n\n2 Katt ey ASS Van Ment\n\nB Azuve Resource Managers Temp tales\n\nie Resourre Syvoups.\nCLOUD ADOPTION FRAME worice.—\neTemeqy- Define busmecs jushticahon 2 Cypecdced outComes\nPuan = ALIQn achenable adophery) plan to boimecs outcomes.\nReroy— Prepare tne cloud environement fer tne planned charges.\nMIG@aTe — Migrate 4 Modemize EMshing lentloads.\nINNevate~ Develoes Nero Cloud-native or highrtd 6 ution.\nGoveen — Govern tne envivonement 4 uscrrloads\nMaAnene ~ eperatove? Magements fr clord & ligonid sa.\n\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\n— MICROSOFT AZURE - AZ 900 5\nok | AZURE Paiwacy!— Micrsoft Comittied ensuring the.\nprivacy OF organ zahons tnrougle ee microsoft Gonhatual\nagreements y»and by provding uv ser Control & trans parenisyy :\nCOMPRLLAN CES — — ume Yiatperat Mircrsoft respect lao and\nregulations and provide comprehesive coverage of “mpliance\naftering’s-\nMicrsoft provides tne mart Complerehencve set-of\nCompliance oF fering, Cinetu dmg Cert fication and atteckahens)\noF any coud 6ervice provider. some compliance edfernep\nIncludimg —\nCore CCRiminat J vetice Iwroemation sysnem)\nHIPAA CHEACTH IntsverAwce\nCSA CTAR CERTIFICATION\n1sO JIEC 27018\nEU ModBU CLAvscEe¥\nNIST (Nattonat Inchtute of Standards 4+ Techndlogy )\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\nLinketn- http://w Linkedin. com/in/surasingh-61445522/\n= Content\n\n1 Introduction | |\n2 Azure Services [|\n3 Virtual Machine [|\n4 Virtual Network [|\n5 Storage Services [|\n6 Core Azure Solution zz\n7 Azure Security [|\n8 Identity, Privacy & Compliance [|\n9 Service SLA and Pricing\nay MICROSOFT AZURE - Az 900 46\nMopuLle f#\nAZURE PRICING , Ceevice LeveL AqREEmMEDy &£ LiFecyeres\na  ——————————SSS__ eee\nAZURE suBSckletiona— @ rremwivg &MARGING CLT\n* Sublan\\ptimn Option # Bvailiable product & Seavices\nR PrlanGP £TCO Caltulstod.\nSERVICE LEVEL be teEemerts Qaseevnce: LIFE tyre\nKH fae * Premed 2 fevornt tatlively keabars\n* Siena tein your Cloud uphiiu-\nAZULE sSuesagiption s:— @®\n® ® @\nme: . ADD subseprir,\nOpen Azure loan th 4o +0 a z=\nYortet ze aw tay? subsuuptin -\nSr —— SS SS — eee\nchoose Lub cen p ten —\nts Free Trial\n2. Pas—As-You-go\n3. Azuve studenti -\n(rcheose your Subseripren a4 per your toate vequivement\nAnd achveates|\n(12 mont) + i200 ered) “+ feesngrtaer|\n(25+ Sevvies)\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\n- MICROSOFT AZURE 4Q\nAZURE PuecHaAt WG!— Three malin dcustome types oN cohich_\ntne avaliable purchased ophoy fer Azuve products and\nCeentes ave Contingen ts ave —\n\n“> Enterprises\n\n—? web dlreck\n\n—F cloud Sduhim Pronder ( Cses)\nFactoR  ARFECTIONG Couts:—\nOe€esovece type @ seevices @ vecaqions\n|\n\nM@ — virtual machine\n\n| Ape services\nOf (@ased on sevvices)\n= Were Of Customer\n|\n®|- Infra chructure Locahan-\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| Linkedin - https://www.linkedin.com/in/suraj-singh-61445522/\nsae MICROSOFT AZURE +8\n| AZURE PUCCHASING !— Thee main co stomer type of ahith\n-\n| tue avialiable Purchased option fer Ayure productr 4\n| Serv ices ave configured ave—\n|\\-> Ente, prises\ni web- Bite or\n(> Cloud Soluhon Pronder (Css)\n|\n|\n|FactoR Mreeet Coste:—\nA Resovece Tyre-  (Vivhis| wacites CPU/ 2M | storey.)\nVirtwe} Yerource atlocated ihe virtnaft Mmadhire:\nLR SERVICES— — Dapond on type OF Seevice Uka (Enterprice, Web\n| —divec, Wound Gdlubon Partaod.\n|\nLK LocaqcoN— Cost vary between Locahons tnabh oHer Arre\n|\n| Product, Getviced and Yetources.\n|\nIt Banduidh— Deka memirg \\N- and- oad OF Ayuve dotaduter.\n|\n| Some tmbound acum hanster ave Pree , Such ao clota ay\n(WW Ayuve eka Guter . For outbound dake trancfer— soade\nae dora wemy out of Pouve PEDO Sy Bw\n| based Mm zoned.\n| (aze QE > PRICING CALaviatoe ) > CHocse seavice )\nowe sence Seleored (along with Yegion) the price woil|\n| be allocated mbdtHom.\n|\n:\n| (TF Pricting Change an lOcahen Aelge.\n=) Linkedin = https: /7 Jinkedin.com/in/suraj-singh-61445522/\nay MICROSOFT AZURE 42a-\nExXpLORE “Teco ( TOTAL cost ot ow NEeLHIP) > —\nA tool estimate cost Saving You can\n| Yeolize by a | to Azure.\n| A report compares tne Cos of on-premises Mfrastucture\n| tery ne Cost of Ugmg Azuve product and sewice in\n| Cloud - ;\nNhe\nyy] ot Compote oy. eh\n4 2. DATA CEOTER 93y. oy.\nOo\n}\ni %. METWoRKing, 2}: 42y.\nXx\naq)\ni 4. STORAGE 4y. Soy.\n\\ .\n/\nExample tost = (Paerros, 499) (592/612)\nMINIMIZING Coste: (Pm-uU- CEA)\n® 1) PERFORM — Perform Lost analyses. Use ( Pricing | TEO Calewloter)\n:\naR iz) MowttOoR — Monttor usuage and Azure Advisor -\n#* Dvse — vse ma Free trat customer C spending rm tle )\nHw se — vpe Azure Reservation 4 Azure hybrd Bentil (Hob)\n+ 9 CHooLe-— Cheote Low-Coit locaton: and Peps .\nwe 4) KEEP — Keep vretodate with laert Lubscripnan . ober,\n& 17) Apeey Apply tag to densify (os\\- 00 nev\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/\n(se)\nMICROSOFT AZURE Oe\nEXPLORE support — option !—\n| Every Azure subscrtphon Meludeqd Free access fe\nbilling and — subseriptter, support; Azure porta) products and\n| Services dowmentahnm, onlmne cett-hele docomentate,\n“uhrle paper and comm on ity Sup pert -\n“7 Also reach 40 me , In care any Supper vequired\nGrownd tne ee %\n| -Scort TECH NCH Sov—RR\nBaste railiable. + all oe\nMicrsoft A2zuve. accoury\nDEVELOPER Tat & Mon-produchs, Business hows acces to\n| Envivonement Lop pens regret vig\n| STawo AeD Producten) vomrttoog| suyy accen to Lusget\n| Envi ovement Exgueen ma pane] ene\n|\n| PROFESSIONAL Lopiness- (niheay \"\n|\nDeperndecor.\n|\n|\n| Linkedin - https://wwwlinkedin.com/in/suraj-singh-61445522/\n\\\n\n= MICROSOFT AZURE Fy\n\n| Performance targetc ave €xpressed as uphme and\n\n| Connectwity) gurantees\n\n| ®  Performace -terget From 39-987. (3-5) + 99-997.(4-4)\n\n|\n\n* IF a services fale to meet the quam tees ,oa\n\npercentage of ea Servue fees cam be Credited .\n\n|\n\n| | 952073 43.2 mune tes @76 heurs\n\n| 99> asy. U6 minvtes 4:49 nWours ”\n\n|\n\n99-99%. 4:22 mmubo S2.56 m motes,\n\n|\n\n|\n\n|\n\n|\n\n|\n\n| Linkedin « https://www.linkedin.com/in/suraj-singh-61445522/\nTHaANk You |\n\ni,\nmites\n\nWOKE",
      "page_count": 61,
      "pages": [
        {
          "page": 1,
          "text": "eat MICROSOFT AZURE - AZ 900 04\n# QvICK INTLO DUCTION. s 9%\nen ee he A ll Sa ee\n* | MichosoFT A2URE, commonly veferred as Azure. (60+ bake Corser )\n|\n* DEFINATION :— AZURE Is a cloud Com puting Services Created by\nmicveosoft = for buildiwg,- Testthg , Deploying and manging application\nand servicen through. microsoft - managed datn center.\n* Developers - Micncoft\n* Inttral Retease - 2% OCTOBER, 2008\n* Operating System — LINoZ, wiv dows, 10S, Andvoid -\nes License - cloged Source, oper source, SDKS.\n# GLOBAL (DATA CentER )— 1,65,000 miles Heer + I40 County.\nKI CONTAINERS — Azuve Used (Resouvee G +o vost re:\nTYPE OF MI GRsoFT AZURE SEevices-\n® | SaaS ~ (sorreare ac a Service) — 3°4 Party Sothoare over iternet\n* | PaaS — (Platform as a service)— Tools over mternet\n* | Laas ~ (infrastructure as a sevvice)- cloud based seovices (Storage)\nW\\sto@y Timecine-\nQ 3:\nJove ,2012- VIRTUAL MACHINE APR, 20 1y—(W DOLD Azvee)—T0-(MA)\nto\n1 DEC, 20n- HPC SCHEDULER JULY ,2oty— MSM +Com > OUTAGE\n4 .\nOcT,20\\0- PLATFORM ENHAceEMENTY SeP,r2015- SWITCH SONTLC\ns MS\nJUNE, 2010\" «NET, OS VERSIONING Mate ;2ol6— SEevice FABLIC\n¢ ‘ = ey\nFEG,2010- COMMERCIALLY AVAUAELEY SEP, 2017- NEW LOGO\n2 \\\nNov,2009- PHP, JAVA, CDN CTO oct, 2016- LINLY OLIENTER G pour\n‘ = \\s\nMARCH, 2009- SOL Datadbase APRLL»20B8- AZURE FLONT Dook SEevee\n\\e .\n' hoctower, 2008> Announced Azure: Manchrezo- 44 Millio. Daily Ache Use\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 1448,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 2,
          "text": "Linke htps/ aiken com insur sngh-51445522/\nContent\n\n1 Introduction\n2 Azure Services | |\n3 Virtual Machine ||\n4 Virtual Network ||\n5 Storage Services ||\n6 Core Azure Solution | |\n7 Azure Security | |\n8 Identity, Privacy & Compliance ||\n® Service SLA and Pricing [|",
          "char_count": 264,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 3,
          "text": "= MICROSOFT AZURE - AZ 900 OJ\nMooule ¥ OL\n| wuaris croun Comeutina? isa amar | oF Compuhng Services\nover tne Internet. , enabling Faster Innovation , Hexible resources\nand economies of Seate-\nCl) |] ComPoTte- Previde me Compute Power. (memory and processor)\ndy NETWORKING Conhection of Compuly togetmer oF VM.\nI) STORAGE- Store oF data | Information\n) ANALYTICS RR IMPORTANT & KIND OF Load, hors much\nmemory required) ete -\nis SEevic& Provipedr CmiceosorT w2UE) HOLTING AS *A SEEVICR\nPhovipeg -% (CALL SERVICE PROVIDE By IN TENET #)\nA PUBLIC cCLOUDmK | PUVATE CLOLO™ — HYBRID GLOUD\nk General public car * organizahon Creake | ® organizahoy naa:\nhost thety service. Q Ahoud envivonement Adophing today hgon4\nty tneir ata Center Cloud- wwteh Come\n* Puvlic cloud owned Combmahion of Public\nby CUoud Seevicen | & organation i 2 trvvate botr-\nOr hosted provder Tesponsible for\nOperahna tne Service| > Connecting !xICHhKg\n® Provide Yesouvceg they provide datacenter to Ayre\nand service +o doka Centtr vig Internet\nmoth pie organization | we oes not pronde loth help of VPN Tunnel.\nAnd uper. acess to vger\noubside of ne (Example> App Ucahon host\n*\n* Recess Via— lntemelr organi zahon. on Ayure. ano cate Center\nWarnes. .\nQR and the Dalyrase in\n(teyamee> Azw ec) (# Dismovarane= Cost $) | client dam Center.)\nUnkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 1361,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 4,
          "text": "i MICROSOFT AZURE - Az 900 02\nTYPE Of CLoUD\n* See ery\nPUBLIC CLOVO PRIVATE Ctoud Hy sBRID ClouSe\nPublic. cloud Sotuton ave| Are declicated to one Hybnd soll ave a blend\nYea dy availiable From organizahon and often OF public & private\nAzure, P2ure prude have. much more Specific] Cloud.\nInPrastructuve @ Services Secunty ton) hen\nto me public public ctoud.\n* AZURE USER ACCOUNT ? open Pree AZuve account from\nInk (WWW. azure. microsoft: com /en-vg/ tree) that give you (2 monty\nFree populay seavices + 200 £ credit fer 30 days + 25 Senviceo\nWK | AZoRE CERTIFICATION !—\nAZ-900 CAZURE FURDAMEN TALS)\n1 q 4\nf AZLOY Ih2 204 | 2-ZB0R AZ-500\nAdministrator} |) peveloper i : Seeunty Cogmeer.\n| wssouaa |! 1 Associate MssoCicbed\nVAZ-3B6Y\n\\\ni |\nyT Ax 400 \\\n1 |\nDEVOPS ERGINEER | || AZURE\n' expert | So-oTtion\n| ACEH Irectuee\nexeeet\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 883,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 5,
          "text": "ae MICROSOFT AZURE - AZ 900 04\ncLovuD BENEFITS\n* | HLOM AVAILIBILITY!— Application hosted ito one dala center\nand ory enching Mtoe anoher patacenter\n> Replicate of Dake. into hwo dittevenr mochice (Data Center)\n® | SCALABILITY!- Too type oF nme: i (Verh cat 0 Honaym tet)\nVEST ICAL HORIZONTAL\n¥ SCALELP/ DOWN RIF ONE VM CuteTUAL MACHINE)\nF INCREAS ING / DECK ERE ING — HIGLy LOTILIZE, THEN Te Afe(c\n— ComeuTe BeTity (Ram 4 Snieteo To pwoqtHee VM evo\nPLoceccot) menraae by (LORD © MLAncet )\n* | ELaAStTIUTy:- Only addtng ne Sealing \\s Catled cae? 3\nFL AQILITY!-  (On-clemand Services) » fast to deploy vm - Conho|\npannel / Porte} easy to Create Virtual machine denne tre\nprvopernts -\n* | DIASTER RECOvVERY'— Replicate the date mo anotner to prevent\ndunng diaster fhme- fasy fo Vecovery . Tt & part ot Prvautibityey-\n* | COVSOMPTION- BASEN MOOEL!— Pay as per te use.\n* Cost OP T\\MIZAT LON CAPE & orey OPT MI2ATION.\n% | GLOBAL REacy- Aral tiorlity oF data Centr across globe.\n% | SECURITY: Secure to access and use the application -\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 1091,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 6,
          "text": "a9 MICROSOFT AZURE - AZ 900 oo\n% | croun— capex 2 Of ER-\nCAP EX - Capita Expenaiture OPEX- Opeanonap Fp pend iturt\n® The up-front Spending oF money on | # spends on products and services\nPhysical (nfrastructure- as needed , pay as youre\n¥ Costs From Carex hove a value get willed wnmediatety:\ntat reduces overtined -\nee\n3 | Consumetion ~ Basep— MoveL:—  Aguve or any Cloud Service prouder\noperate on Gaombsarpixan— based model. which mean end user only\npay for the vegourcesS that they use:\n> Beer Cost prediction .\n7 Price for Individual resources and Services are povided.\n=? Billing ls based ON = actual usage.\nER G@ew Cloud Seevices:— (oBEeCTIVE Domain).\n@) | LaaS ~( Infrastructure -as-a- Service)\n@ | Paas — ( Platform- ar— a- sevice)\n@ | SaaS — ( Softwave-as- a -sewiie).\n@) Identity Qa Services type based on Use-—Cane-\n(S) | Deseribe tne shoved reaponsiint ity mode.\n(©) | beseritce Serveriess computing .\nLinkedin -http//wrwwelinkedin.com/in/sura-singh-61445522/",
          "char_count": 973,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 7,
          "text": "ae MICROSOFT AZURE - AZ 900 66\n(| INF@asteucTuee - As- 4-seRvice (TaaS)- fn tnis cloud service\nInfrastructuve (build pay-as~ yo-go) by venting Servers, virtusd—\n-machine , Storage ,netroor& and operas Systm From cloud\nproviders:\n(a) (2) @)\nSERVER NETWORKING Data CENTER\nJ FLEW scl S/ POWER Wen Agement\nSTo RAGE CeCURIT MAINTEN ACE\n(2) | PUAtfoem — AS- A - SEtvices (PaaS) Provides environement for\nbuilding , Testing and deploying Softeare application , vo tnout\nfocusing on man aging MIA ng ln Fra structure.\n*\\\nSERVERS & cTORAgE Oreeating system *!\n#2\nNETWOREIN , ERELIALLS/ LEcue ty + |pevevopment Toous* >\n¥2 DATABASE MANAGEMENT\nDATACENTER / POSER mAnAgemeNny Rusimets AVAL TICS\n(laa S$) C Paas)\n(2 SOFTWARE -AS- A- SeRvice (SaaS) - User connets to and\nLge = Cloud-lbased cepplicahor (app) over Ane miernels\nExample > othce 36S, emails, Teams Softoare .\nf\n(Laas) + (Paas’) +) HOSTED APPLICATIONS / Ppp's:\n(saaS)\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 977,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 8,
          "text": "= MICROSOFT AZURE - AZ 900 oy\nRK) SHARED RESPONSIBILITY MoveL:-\nDATA & ACCESS XK RE me RA\nAPPLICATION ee Sad RK AZvReE\nRON TIME ; ee we AZvRE Arve\nOPERATING SYSTEM Ae Fe AZO RE AZORE\nVIRTUAL MACH INE ee RK AZvVLE AZURE\nComPute YOR AZvee ARVRE AZURE\nNETLORE ING He AZvRE ABLE RZORE\nSTORAGE ** AZURE AZURE RZOeE\ne*\nKE —F cheent ave manage tne responcibilit4:\n2)\n\nK|* REsSeevVE-LEss ComPOTING Ke CIMPoetANT) - Ty general clent +\nShaved vequivement of Infrastructure ( ex> CApaety of RAM 4\nbyocessor) , bur In Cate No-load on your Sevver but Shi|\nwe have to pay becauge we vesevved tne Caparsty: tm\nhost applicahon fh Datw- Center\n*-> Modem ae | to woerlG oy Mit CUpproade ts Change.\ntnere ( no reserved server fox work=load, When requ est\nCome It _ Wil] ages allocated +ne vesource en-need\nbasts nd yelease the veXouvce once NO-UZaGe. In Rrure\nhelesto beloco too sia’ are Important —\n\nAZORE FUNCTIONS'- I$ Q Code renning your services and nok\nne underlying platforms or infrastructure: T+ Creates (nfrashucture\nbased on an events:\nAZvee Loatc: |Is a Cloud Service tnat help you ab automate\nand Ovchastate tasks, business process and usoreHocs\nwhen you needed to wm beqrate appltcahon » bata, Systems\nand Services -\n\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 1274,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 9,
          "text": "Linkedin tps wun inkein.com/in/sursngh-61445522/\n23 Content\n\n1 Introduction [|\n2 Azure eee\n3 Virtual Machine | |\n4 Virtual Network | |\n5 Storage Services [|\n6 Core Azure Solution | |\n7 Azure Security [|\n8 Identity, Privacy & Compliance [|\n| Service SLA and Pricing [|",
          "char_count": 268,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 10,
          "text": "a MICROSOFT AZURE - AZ 900 O38\n| Mobute #:62\nCLOUD AZURE SeevIcEs!— Aduve Sevvices aQre> Ainded into\ntoo parts: A2v te\nCLoOup SEevices\na) wt 2)\nALORE Az LE\nARCHITECTURAL. Core-resovece'’s.\nCow PONENTS\nRegione and Availibitity Zones Compute\nNetioore\nSubseriplion and Resource Grou . Storage\n: batacace.\n\nCQ) AZURE— ARCHITECTURAL COMPONENTS: — Azure Is an Operating\nSystem prontch manage a dat. ceynier cof microsoft. To manage\nbelow = ave ae archittctrual Component —\n\nRegions g Region’ Pairs\nhvatlrbility Zone\nAzure fKesources\n\nAZORE ARCH TECTULAL\nResource Qmous - ComeoNEnTs\nAgure Resource Managers.\nSubsertp hows\nAyre Management grouy'c-\n\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 702,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 11,
          "text": "Poe MICROSOFT AZURE - Az 900 og\n(t) _REGIONS:!-  Azuve offevs more gqrowal Yegions that any other\nCloud, provider Lotty 60% Regions vepresenting over | 40+ Counties -\nay meee See\nA Regions ave madéwup of One or more ola center\n+ provide Etomaty and scate to reduce customer Latency.\nP Preserve clata residency with) a Comprehenave Compliance offer.\n4 REGION > MIGuTtT BE RAR COLLECTION OF ZONES\nRegion Pae’s:- For Arvailibillty » Iw same Loorst Case scenario.\na complete region might be outage - Cmtgnt be due to Nature\nprovlem or some daiasitr happer ,Floed, power outage-\nTo avoid Wt problem Statement a reator ~pairs & done\nLohere nicrosott Create a paw-zone to prevent hate dune\nsuch dlaster.\nRr Chon Region Should be in same Geographic location )\ni > Cat ond 300 miles of Separnhon betseen Yegion pals)\n5 | #> (Avtomatic veplicate for some services-\nO| k> ( Reels priorihzed vegion recovery inte event outage)\noa .\n(1* > Chair ame  detred low mroaobt (maeott) .\nPAIRING ARE RIXED, DEFINED By > micRoSoryt OnLy\nREGIONS REGION L - PAIR\n— Nor Cental us — Louty Cena us\n~ East us ~ West Us\n—~ West US2 ~ west Central Ut\n~ Us Eager ~- Cenwar U-+\n~ Pndia Loutw - Lada -lentyal —\n~ Canta cent ~ Canada taar -\nJapan tart = Japan wet\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 1291,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 12,
          "text": "= MICROSOFT AZURE - Az 900 jo\n\nFe) AVALLABLITY OPTIONS:- Avr ailibility is a Factor volwcly deude\nhow much time & our applicahon—upe and vunning.\n\nEx> loot. mean ho down-tme , ALuoa y saxtctnngs\ntorailibilits ts defined oy -> SLA ( Service Louel agave ment )\nEx> 99°94. E> 99:99Y. DIASTEL RECOvELy\n\nZovey Lowe-2 Regron-J Region>\nee |\nREGION-L\nBR SINGLE Vu Host | ¥ Each Zone have independar' Reatonap Protectan\nowry. Power Backup , phy cicak lot. Daten Recidenaye-\n& Dependent on\nSuge vm *\n\nFF | AVAL BIL ZONE — One Zone treated as A One Datu Centr.\nMultple zone lead for egos , Each data Center ts <aquspped\nwoth independent Power, cooling and ne hoorkingy. each\nZone ave IMierconnecttd via Fiber-opticg nekdories\n\nPorailibr tity Zone\n\nLinkedIn - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 798,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 13,
          "text": "oo MICROSOFT AZURE - AZ 900 \\]\nfe | AzURE Ceevices:— There are various services offeved 4\nk2uve Service ) fer pomt are Capture le2lovs.\na GENERAL Storage] verworcine |\n~ Management Groups —Nirtual machine |[— Storage account |— virtual Nehoorg\n— Resource Groups, ~ Kubernetes ~ Recoveng ~  Eyppress Route\n~ Maveet place. = OS (mage — Datu lake — Pubuc-\\e\n~ Subsert phon ~ Ve Seale sete | ~ Storage Expicrer | Netoork Inte taco\n— Templates — Cloud seavice, | - Data Sox ~ CON Profile -\n~ Tag ~ Hosts — bata Shaves | Route facies\n— Resource Exoleve r z — Hec caches |  N€hoore Secun 6\n3\nte) MAN AGEMENT & Kove !-\n* Management groups Can include multiple Arve Subsea phon\na4 group P ay P\n* 10/000 management syvoug Can cupparttd in a Single dweetory.\nMAW AGEMENT Love Q Management Grove\nAzv hE Az CE (2) Azure Su bsurip how\nSUSCERIPTION-b Susser Ption-2-\nREsovece Gove REsouace Resovece | (B) Resource Group\nQaovel G@ous\nResource Resouece Resource Resource (4) Resources\n1 2 ! =\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 1037,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 14,
          "text": "ec MICROSOFT AZURE - Az 900 12\nAZURE RESOLKE!-\n® AKHev Selechng —7 Region and Zone of Date Center.\n* Below aQre the Importent ferveces as part oF Azure—\n| | Virtual Machine.\n2. Storage Account\n3 | Virtual Nebusone COLE SeevICcec\n* | App Services i\nS | SQL DvDakabases :\n® | funchons\nRESOURCE GCOvLFP'— Tr fc very Important +0 group the\nyesource to a Single onit- So Every resource need to\nbe part of vegource qrour:\n> APTER AZULE RCCOUNT 7 WE NEED TO CREME A AWE Geode\nsefiae * Every Recource Group have a Locamor\n-* Resource ean exist in only One resource group.\n+ + (Resource) > Can exist th the aliHtevent Region\n=e Dt is posciete (Resource @ keqron—1) & ( Resource Qrou~p@ ey)\nKR A Resource iH Resource- Group Con be moved From One\n. ft yesource Yroup to ander vesour ce —4roux loot one\ntive one ascocated In }- tesource OpPur\nREsOUVECE Gtovr\nres) (ver)\nha\nHK IF DELETE , RESOURCE Geour Tr Wie mertom aTiCaLLy)\nDer_eTre ALL RESOUL CEC Linkedin = https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 1000,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 15,
          "text": "Linkedin - https://www.linkedin.com/in/suraj-singh-61445S22/\n= Content\n\n1 Introduction | |\n2 Azure Services ||\n3 Virtual Machine\n4 Virtual Network [|\n5 Storage Services : ||\n6 Core Azure Solution | |\n7 Azure Security [|\n8 Identity, Privacy & Compliance [|\n9 Service SLA and Pricing [|",
          "char_count": 285,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 16,
          "text": "Wy)\nZs MICROSOFT AZURE - AZ 900 os\nMoDULE #02\nFe | RRO 1- = AZURE VIRTUAL MACHINE\nAzure Virtual Machice (AVM) (s one of Several ty pes of\non-demand , Scalavle Compu tng resources tat Azures Ofters -\nAzure Virtual Machine (AVM) give you ine Fie pie of\nVirtuclizatHon — tarmout having to buy and maintain tne P Aust Gal\nhardwwave than runig.\nTHINK ABOUT BEFORE CREATING a VM (VIRTUAL MACH INE)\nt Application Resource Name s Operating System Ten on VM\n* | Locahen where resowce Store |6 | Confri guratim after Vm start\n3 Size of VM uid VM Need Vvesource .\n= Maxim ot VM\nFL LOCATIONS - Theve ave multiple locabhoyw iy many geeqraphicaf\nvegions aveund tue woorld. These ave the veqon, (2 called\nLecehens wlhien Specifién tne Locaton °F virtual Made.\nThe way +o get tre availlable loco ave —\nop) ® (63) @..\nAguve porte? | Azuve Powersvel| | REST nes | AbvRe cre\nKe] AVALEBILITY I A2uve Announced an Industy leading single\nInstance Virtual machine. Service level aegg agrement of 99-37.\nprovide you Ae ploy te VM tory premivm Stovage OF al)\nAisks . (BELOW SLA)\n-~Love\n*| 99-97 —- Two or more Instances deployed acress hoo or mare vaitionty —\n*) 99-95% — 2 or more inctance deployed in same pvailibrty Set:\n#| 99°9y% = Singte Instance Vm Using prem wom Ssp/ulPa Dist Pall Ose\na | 99-S¥. = Singte Instance VM wit) standard sso Managed Disks\n¥ | 3Sy. ~ Singie Instance VM witw Standard HdD Magnetic Disks.\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 1457,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 17,
          "text": "as MICROSOFT AZURE - AZ 900 0%)\nHe | AZURE RESOURCE MAUAGER + Mec The Azure resources\nManager CAkM) provides a management Layer that enables\nyou to Create , Update and delete vesources mM your azure\nSule cculphon\nAe we Know, Aruvre & @ an Opevating System. whole\nmanage claka Center OF microsoft: , Lahtch manage various\nservice présent In data centty. CARM> help to intact wort it)\nARM Ik Seger Deployment model tohich Can be yg ,\nFor epptecteel Creatng o> Web opplicaten /.virtual machine /or\ndata Storage. (Bod 4\n7 AZURE PORTAL ‘so VEED TO Communicate FARM\n. (Creation. Option FoR Aam )\nein Gf naune eoweesnec|\nRESOURCE é\nmanag be\n(Ison)\nSeevice Monagmene Gem rempiate) pact CUTER T\nUse to Created\n(Given for creahon) oon pene)”\n(Billug + Access Cour Roundanés)\nqe | AZURE SUBSORIPTION!= To start Leorting with aQ2ure, Wweneed\nQ SUubsompton of Asure. Mauve Subseyp hon provides you\nlot autnentcaton & autumized access to Ayure account -\nBILLING Boon DaRy:- Generate sepavak bling Yepos aud\nWvoice for €ach- Cubsuiphon and Conbol “resources.\nSe oe Wan es\nA2vRE ACCOUNTS\nDevelopment Teast Produchm Subso'(p hon\nSubs crrphory Cubsuiiphen Linkedin = hnttps://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 1200,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 18,
          "text": "ae MICROSOFT AZURE - AZ 900 Is\nPABA te Prd\nAZURE COMPUTE SERUICES\n® Compute = MEemoey + PROCESSOR ¥ :\n* Compere csleve appucanm are kun, Computuhon proces,\nInstruction ave epecutect,\nCompute SERMCE LiKke!—\nVirtual Mache\nApplicahon Cervices\nContainer Instances\nKaure Kubernel&r Services CALS)\nWomdow vittiat Desttop .\n\nVIRTUAL MACHINE! Virtuol Machines are Softoare emulations\nOF physical Computer: which Imcuded he— CVvirtuet Procescar,\nMemory, Storage, and pctmaias \\P,\n\n*[Unzom. weenie 21005 [f\n\nCREATE VieTURL MaeyINEr— (@AzZv0kE\n\nlL go> All Services Hed During Creat Virtwet ma ckine\n\n2, CUCKR> Virtual Machine. Vi\" [Basies\n\n3. CH) Add -Virtat Mache = | ® | Disa\n\ny choose—> Subsentphom ! 3. | NETWORKING\n\nSs:  Resoure Group > Name. , & | Manaqement\n\n6 Virtual Machin Nome ®>VM LS: | tage\n\n1 Remon — Setectk Regen ' 6. KEeviIEW + CREATE\n\no \\mage — OS Name (wWrdow F |\n\na Size —(Procaser+ aam)- cout | eres\n\n; (12,000 way HR)\n\nlo. Upername H\n\nNW Passusere . { es\n\na Iw Bou no > ((Roe Jwrie(es)), (at last you will yo prices)\n\n'S: (Open por for publicte)  !\n\nte Dentincrd Ot> imave ters y Linkedin - https://www.linkedin.com/in/sura}-singh-61445522/",
          "char_count": 1152,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 19,
          "text": "aS MICROSOFT AZURE - AZ 900 {6\n+e | APTER VI@TUAL MACHINE CREATIONI- Once virtual machine\nMS Created. (once we clich create button tre page cout\nscvot| to \"ARM Templates (Aruee Resovece MAvAGemeNt)\now\nARM iS Q JSON Document (ovenew> inputy cutput> Template)\n(2sony JAVA ScRIPT OBECTLON NO71ATION)\nonce Virtual machine Created It call] Suow Ale are—\nExAmeLle> (bEMOvm)\nAW AME % Resovece grovel x Locenion | uucerrenod\nREcsov KCE)\n” Demovm 4 | ictua Mach {ho pemo-gtr Not Europe | emo As-[\nF( | Demovm - if | Public 1 address | Demo. URP Warnarepe || one act\n& demovm_nsq Relooyr Secunty Yous Demo -aee | Nota Eurepe- Demo A-{\n|\n2 bemovm 334 Vetiosvic Wterfece | Demo Gee wot Emope || deme rt-|\nJ | | Demovm_os-pis| bisa Demo -Uey wort, Eurey> || de me As\n8 bemo- arp. net Virtnad Nero, Demo- GKe Nom €urop— |] pemo As-4\nes] Netto ovgquatcher || Network Warner Olea - Uae Nutrn Guo | Oe mo R41\n@ Tor\nSs \\F we preys\nPs Stop Wean\nire milling sro?\nQ) PRess—> ConwECT Button 7 open py (RPP > SS H)\n“ vv\n+> [ ® PubUec |P= ws HR RR\n> \\F Poenumber= Eo\nbovontload ROP File\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 1124,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 20,
          "text": "ee MICROSOFT AZURE - AZ 900 {z\ntH) AZURE CONTAINER SERVICES\nCONTAINER 18 HELPING us IN CREATING lsoLATeo WORK\nLoack ON THE SAME PHYSICAL MACHINE, BOT AT SAME TIME\nTNEse WORKLOAD DO NOT NEED OfbARATING SysTEM\n(%& In general 1F We run 2 VME both need Bun OF to\nYon the resource, to avoid Such requirement neeq> contatners )\nCiwmaye File Stove)\nRE ‘\n(pevevoren ) ae (usec)  Yeneare\nCape Stove) Commtrain\n—7 ¥lontainey are created to ron tre applicahin at UAer bende\n—7%* Container ic light weight, ( doee not have Operahne sgtim) De\nwill alway vuped +0 underline on cohich it & Created.\n7 ® Contamer oct nor Yeau ved Operahng Lystem\nda | AZURE CONTAINER INetances a PaaS obering tnat yong @&\nContamer im Mauve, cottnout tne needs to Manage & virtual maah na\ntr [AZURE KUBRRNETS SERVICES !- Qn orchesbatont Services for Container\ncortw duiibuted Architetture and Large\n* KUBERNETS > Is open source CSofhsare, Lohich & also\nCalled a  orcheshahon Services. hetp to achve and (nteqrated\nmultiple — Contamer colth tearr amount Cf -elfurts.\nte (Ph Azuve—> It lu eovnny an ARS.) , Need to creak.\nmv lye VM and top of VM need to create muthple onmines-\nku@eewets Seevice > FREE\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 1228,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 21,
          "text": "a3 MICROSOFT AZURE - AZ 900 ig\nVIRTUAL MACHINE SERIES t= CxS)\n¥ Gee-21)\nseers |VM SIZE EaMiLy USE Cases PRocessok PREIEMSIart\nA Entry level vn Test Cerver, Server Pod 893\nD General Purpase Compute | Appl(canon , Dakibace 24 99\nDv Next Gen- General Purpose] By tenerse applrerahon\n& Compute Ophmize-VM | Batch Processiig , webs QD\n4 Memory 4 storage Opting ECP, SAF, SOL 22,0¢8\ny High performance-VM | High performace seeped Gl, Fey\nLC Storage Ophmize-VM | Mongo Dé , DS Laluve 32,28\nS GPU enable — VM raps co , Video — Q?, 233\nM Memory Ophmired- VY Required Massive $0,209\nParetiet Compile -\n% | AVALIBILTY ZONEI- HELP TO MAISTAIN SCA 99-Gs51. IN Care\nOF Pi Sasteryw RE Covtey-\n| PRICING CatevtmtoOh!- HELP To GET possi bce Costs Pot\nHosting Hest Resource IN A2vRE.\nHL AZURE MARKETPLACE!— FOR DEPLOYING AaVvKE LUBSCIPTION.\nwe | ATURE VIRTUAL NETWoOREI- Dr hosts tue Wirral mechine td auc.\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 950,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 22,
          "text": "A 14)\n= MICROSOFT AZURE - AZ 900 no.\nVIRTUAL\nNEqWOKe\n| oe Mines 4\nbiste Med INE < 7 DD Keg\nNETwoKK | _s /VieTU AL\nSEcukiT NETWw otic\nON\nSSve sector 7 for Billig purpose 4\nNR Eee Oem\n| 4VIRTUAL MACHINE! ComPUTE MACHINE ON AZURE PLATFORM -\n> Los dIsK= VM having Auk associated oth, vm ,(stormg \\nformation)\natached other dulk fo hesr Pate Duk\n3 4 VIRTUAL HETWHOLKIC— Ig an woloked netoserk on cloud, Virtuef\nNetoork help to Create a networtigs parts «(Who Interfoca ord)\n4 JBETWOKE secDaTy— tached to Seaure |n Lout bound date\nS [VIRTUAL NETWORK™ Attached on VM Machina [Lb a nehoort.\n\\urerface Card\nG [POBLTE-\\P AODEESS = Atlous fo Comput karoge \\wheenot.\n7 T REgouece Govl— All ave part oF Logical Yrove called Vesouree\nGrout\n@ + Sue SCAIPTION — For alloy purpose .\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 827,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 23,
          "text": "°\nas MICROSOFT AZURE - AZ 900 20 #3\na 4 STEP TO CREATE- VIRTUAL MaeH INE:—\n(4) BASTCS (3) PASE AREAL NMETOORKS\n(| Subsoriphon — For billy \\ | Wriuay Netome—\n2| Resource qeroup— Lopycat qour 2 | Subnak—\na| VM Nowe- VM Wane > | Puble 10 —\nu | Regon— Ex—Euvope / vb & Yipic Secunt4—\n6 | Poraitrta liky T) Inbound Ror —\n& | Image Operaheg System\n7) Size— VM Size Cveher— talore) (A) mans EM EN T— Configur Mmenttorntirg\n© | Unername— ReRX & management option for VM.\n9| Passomt— wm x\nto | Inbound Poss ROK (23 OA) NE. ([S)movAceo Aeldttonah agent, Sonpla\nOT TOT TS TT or apetreation via VM\n(2) visels:— (e) TAGS Conscu dotted bills by apely\nl}os pick- SS D/HNoD Same tng te multiple resource rou,\n2 | ese energtion— (2) Row buy + CLEATE\n3 | Dada Dick\nHou uy Estimuattoss = (0-123 Usb /He)\n¥* Mer Created we have belos vesource He In Virtwal machine:\nExamplar—- CDEMOVM)\nName Type | Resource Groxp . ene Subseriphny\nee ee EC\ndemo- 4rp.vnet Virtual We hoortc ! clemo -arp WermEurore | prc-L\ndemovm ‘Virtwal Machina | Aemo-gre nw 6\nAemovm ~ ie | Pulte IP address f Aemo-4rP tro ft 4\ndemovm - néqr | Vetroorte Secunky Gro; demo -gre ' uoe ”\nAemovm 33 + | Nehoot. ktenfa ce | demo-ger yor ot,\ndemovm-os Dis h-l- ' disk, , DEmMo—-Ger Ha Loe\nNetwork watehed- | Nehsoria watcher ( velcore Wskeay * ” | ”\nKAVALIBILITY ZONE —> HELP, TOMANTAIN SLA 9995). IN Case\nDisact ee REcovey yo j\nLinkedIn - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 1432,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 24,
          "text": "Linke n= tps inked com in/sur-singh-61445522/\n=s Content\n\na Introduction | |\n2 Azure Services ||\n3 Virtual Machine [|\n4 Virtual Network\n5 Storage Services | |\n6 Core Azure Solution [|\n7 Azure Security [|\n8 Identity, Privacy & Compliance [|\n9 Service SLA and Pricing [|",
          "char_count": 269,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 25,
          "text": "as MICROSOFT AZURE - AZ 900 By\nMoDULE #0\nte | VIRTUAL NETWORK When We Created Q virial machine\nW azure netoorkK tr wolll allocate a > Virtual netoorn Interface\nVIETUAL NETWORK INTERFACE: Mange all traffic movine\n\\n -and-Ouk of the vittial machine (Cvia— virtne, nehnser K-\n-ktyface). so me datr How via virtnad nebo Interface\n1. |> lp Apokess— help to locate a machmne\n2- |> pueitc te Apoeess— help to locate machme on internet\n3. |> PRIVATE 19 ADDeess- help to Locate machine on Locat- nehoore\n* |? Luanets— [+ help to separate one or more subnets:\nGn “ayecqeR. pease fee SLL ape\n# A Collechdy oF (VM) | Connection vita Dedicated lease routs\nComputer 1 each otner| Mrernet aud Sending | onty for Connechon\ne Internet > Public If WHFHE wottr Cupportof| betoeen (Ryure Daln\n© Intranet Private It Enoayphin | Decryphoy Center @ user )\nRwhen we created a WTtual Mache, frufemeahaally Ur will\nCreate a Virtuad Nehoork:\nOK) CREATE VIRTUAL NETWORK NETwoRre \\\nSEcvRi |\n!\nOSs VieTu AL VIRTUAL PuBtc\nDISK MACH INE VeqDOR l? |\nINTE Rept\n— Private 1¢ = 10°0+ 1° Oy Bia es \\\n— Puarcles (3-79. ¢¢-72 \\\n|\n|\nSUBKETE 10°0°0:0/2y\nVIRTUAL NETIOOLK = (l0-0:0°0/14)\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 1207,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 26,
          "text": "ae MICROSOFT AZURE - AZ 900 a2\nCREATED VIETOAL NETWOKK: Creston of virtual nehoork i diuded\nmito & Steps— ©\n(0) BasDcs le AopRess Stevert TAG £ REVIEW + LEATE\nSuesaiphon > \\ev4addvess |» Host Tavgare | (Kenew £ create)\n+ R-ey1 on Grr - \\tvd addrey | Dodus Protect | Nane Walue\n7 stance -Nane |» Subnetec > Areva fel thar\n+ Instance begin Crates you\nte Conese\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 420,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 27,
          "text": "=. MICROSOFT AZURE - AZ 900 12\n| TYPE OF NETMOKK ConmECtion!-\n1. | VIRTUAL Neto C PEERING) > More secuee”\neee\n2. | polyT—h- ate\n3. | Siqe-b-site\n|) | VIRTUAL NETWORe (PeERING):- Thic basically help 10 Interconnected\n+90 ali flevent vitnal machine ‘ooved on Private lPaddrers\nVin =) Ved\nPrvate [Pp addrew |g ip) Privat |Paddnn\nRb Sythe ote aan ——_—oOoO OO\n10-0-0-0/16 200-06\nPomp -to-si@ VPN Connechn\n2. PANT — TOW SITE } 3 SiTe-To- SITE\noe\n'\n! '\nt Vm-} ban ne binned\n= a 1 10 N\nOver ] ' Putace if M b\nLoney) || Gatetogg | JV |\n3 ; ! locat\nPubic ! Privat lf | Nepo\n\\¢ pie\nGenta\n'\n' Prvece | VM\nVPN Sela Support (025),\n. mee '\nPom tt StL VON Compectys | Ro chug 1 Hho Cuco vOurer er machine\n| Welp To voute hayic On internet”\nSince In formahin Haro bow Inlonel |\nSo we are wii VPN Aaland | LOCal Nfs Gateway’ — Informeahan\nts aaalus Thiers, Cocure, L police If op local nehowk boly\ni\nt\nVPN Gateway > Atta ched From La)\n( tev tre haypic\n|\n{\n. Linkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 1001,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 28,
          "text": "Linkedin - tps ora Ainkedin.com/in/sura singh 6144S522/\n= Content\n\n1 Introduction |\n2 Azure Services |]\n3 Virtual Machine | |\na Virtual Network | |\n5 Storage Services\n6 Core Azure Solution | |\n7 Azure Security [|\n8 Identity, Privacy & Compliance [|\n9 Service SLA and Pricing ||",
          "char_count": 278,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 29,
          "text": "as MICROSOFT AZURE - AZ 900 24\nMoDULE #\nAZURE STOKAGE SERVICES — belore are tue tage oF sherage\nSevvies, tnic ave te parts of Azure Storage Servies\n@® eross @ tases @ aveve HLE @)\n- \\mage Table dare = Sending Meseage\n— Video - feei vies mesage\nHOW To Apo}\n0) ———\n(es) CLIC KA) CREATE BUTTON |\n©) SELECT ofTion \"storage Account” |\n9\n0) BASTCE — Account name | Regon / Performance\nie) ADVANCED — for seaure the accounts\n® VETW OREING — Public / Private 10s\ncy DATA PROTECTION - bete hen or Medificahorn\n© TAGS — Caregoriza ton\ni) REVIED + CLEATE — final cveahon oa\nACCOUNT\nLinkedin = http://www Sinkedin.com/in/sura-singh-61445522/",
          "char_count": 630,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 30,
          "text": "+2 MICROSOFT AZURE - AZ 900 2\nAZURE STORAGE Seevices\n\nYM CONTAINER STORAGE (BLOB) — te optimized for Storing\nQmount of unstructured date , such ag text or bmary date\n\nQ dish STORAGE provider ducks for virtual machines »\n\nAppliCahoyn , and otner Services to access And use-\n\n@ AZOLE ALES'—  cete up higaly availiable nehoerks file\nShaved tnat Can be accessed by Using: tne standard\nServér message block ($8) Protocols.\n\nAZURE STORAGE ACCESS TIERS\n\nHot Cool AK CHIVE\n\n‘ ophmized for store * Opnmized tor storing: ophmzed For stoning\ndata thal ta Aaka thar ia data thar & raveta—\naccessed infrequently accessed | accessed 0 Stuved,\nrequenttyy mat least 30 days wat least 180 days\n\nREAO £ WRITE\n\nAOE VERY fee)? (ess ee) (Read Plone Weppeunos)\nMost Cost Read 4 conte Cost Thu cu\n\n4) (' Rae y) (7 Cay to stove )\nDM wirl| ake Hue-\nOrn\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 889,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 31,
          "text": "oat MICROSOFT AZURE - AZ 900 2e\n\nExXeLoRe AZURE MARKETPLACE:- tb similar like Play Store,\n\nAzure marketplace allows customer to find, try , purchase and\nprocess applicaton and services Prom hundrede of bead ing\nservice providéy ,Lohich are all certted to von on Aoure-\n\n(7 Open source contaner plattorms\n\n27 vittuel machine and Oatabase Wm ages\n\n37 Appucahon bulld and Deplogment Soft ave\n\n#7 Develovev's tools\n\nAZURE DATARACE SERVICES\n\n* AZURE Cosmos bATtAGASE'> te a globally= Aishibuted clatebase\n‘nat elasheally and Inde pendently Seale -\n\n* AZURE SQL DATABASE? Is arelatonal dambase ac a Services\n(Daas) based on tne latest version of micvosottsar\nServer dotavare eugre:\n\nba AZURE DATAGACE FoR MY SOL & a Fitltge = managed My So\nAotabase Services Far appUcahem Aeveloper<-\n\nB [azuge Dataease for Poser SQL> & a relahonal\ndatabase services based on tne Open-Source Poshavee\ndatabase enganre-\n\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 958,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 32,
          "text": "Linkedin htps:/ war nkedin.com/in/sursngh-61445522/\n= Content\n\n1 Introduction [|\n2 Azure Services |]\n3 Virtual Machine | |\n4 Virtual Network [|\n5 Storage Services [|\n6 Core Azure Solution\n7 Azure Security | |\n8 Identity, Privacy & Compliance |]\n9 Service SLA and Pricing [|",
          "char_count": 274,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 33,
          "text": "ss MICROSOFT AZURE - AZ 900 24\nMopULE#\n\n* CORE SOLUTION!-\n\nCORE AZURE SOLUTION AZVRE MANAGEMENT Toots.\n\nt}loT to Azure Cpheve ‘Portal, Powersnet|, CLL\n\n\\ | Synapse fraly ties to batabricks | Advisor\n\nW] Arhated Mat etlipence We Monttor\n\n“| Machine learning WW Seyvice Health.\nve\nVo\n\nee\n\nRK | AZVRE- INTERNET OF AHIWGS— oT describe phy sical Objec ti\nthat ave embedded vstth sensors J PYOCESSING- alility ,.s oftware\nand otner technologies and tnat Connect and exchange date\nLetty otter device and System over tne Internet.\n\n¥ AZURE 10T CENTRAL!- tc Fully managed global loT Saat\n\nSoluhon that makes ft easy to Connect , monitor and manage\nloT assets at seale-\n\nKX | Azvokee lot hves:- a managed Services hosted m the cloud\nthat act Ie Central message hob bi-direchonal Communicahern\nbetween l0T Appucahon ang the device tt Man age.\n\n® lOT CENTER % ( DASHBOARD) > TOP @ (let Mos)\n* loT (Build top of hub, It has U2 to manaye multiple\nCENTRAL dence) _ _ -\nK lot evice Cond by lor Mb)\nre be y *)\nLinkedIn - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 1054,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 34,
          "text": "3 MICROSOFT AZURE - AZ 900 QR\nFF BIG DATA & Rwy TICs 7? over a period oF hme, mors all\nOrgantzahorn Is Cap turing a huge amount of data. (by > Develoe\neCommerce website)» Hove fam many people vetted ebe-\nHere big- bata help to analxe te compete data: heve we\ndump all date and Usieg ETL Too| and process tre date\naud create a datwn Whovehoye- Below ave we BQ -dater\n“Tools —\n—7| R AZVLE SYNAPSE ANALYTICS -— A Cloud -based enterprise\n| bata  Warehouge- Solution , (Epa mpe > Hadoop )\n7X AZORE HO INSIQGHTI=  & Fully — managed 7 Opeh sourced analy cs\nServices for enttrerites .\n—7\\|R AZURE bATABAICKS!- Apache spark based analytes Servic eg.\nWese three ave sae i tommen ped {0 analyze huge Data\nFE) ARTIFICAL IWTALLIQeu CE d) MACHINE. LEARNING’ (MUis when\nUnderstand te pattern ant leavn ne output wir help of @2)\nAZURE MACHINE LEARNING!— Cloud-based to develop, and\ndeploy machine learnings mode}\nCOGwITIVEe Seevice:— avicklLy enable app to see, hear, Speal,\nunderstand and imterpret a upev’s needs.\nA2URE BOT SERVICES! Develop Intetlyenr Jn tea Prue grade boa\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 1122,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 35,
          "text": "as MICROSOFT AZURE - AZ 900 2g\nHr) AZURE MAwaRUEmMeENT Toors!— Below are tre part oF AMT\nCAwWkE Man aceEMENntT Tools) —\nARVLE PORTAL ARVRE POWEY gyecr\n[axvee PORTAL |\nAZvVLE MOBILE APP COmmMAVP LINE interace Cet) |\nAZURE Rest APL [ AZURE CLoOvo ar |\nAZURE REsovece Managetc (aem\nH | POWERSHELL” Install power Shell locally, Cveate a resource qrove ana\nvirtuad machines acces® and vped the Cloud sell 4 renew\nazure Advisor reComme nolttio\ntH [AZURE CLE!— Install tne A2uve CLE Locally »Create a yesource qroup\nand virtusl machine , Uge ne Cloud shell and revu ayure recommend\nWHEY ALL MODULE INSTALLED, WE NEED AOVUOR( AZURE ADVIS Oo)\n> Probably we are not vse / or Ueing VM _ upto tre capa\n> Only one VM k 1s try LAE ,anoter dhe in not Lpe mudr-\n> MSo,venen deploy loovm™) [+ & Mot posible to Analge all vm-\n# $o tne sol 14 Azvee Aovisoe) *\nLeecer\nAZURE ADVISORI— Pmaly ze depo yed Azuve Yyesource and\nmakes retommendiaklorn on best practiced toophme Azyre deployment\neltaler lity\nSecunky\nPerforman ce\nCost &\nOperahon Excetlen ce\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 1091,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 36,
          "text": "eS MICROSOFT AZURE - AZ 900 20\n(FREE)(Resovee)\nAZHRE Avdvisot!— Pratyzee aAeployed Azure resource and\nmake yvecommendiatery on vest prahces to Opnmize Deployemen €\nI Reliability 4 Teese\nis Secunty ST Opernbownl Epctllence-\n°F Performance\nAZVRE Monviok!— Azure monitor maximum tne availibility and\nperformance oF applicahoyn and services boy collechny , a i\nana aching On telemetry from cloucl and on-premises envirenmenG\nApputahey merger AVALIBILITY 2 PERFORM ACE\nLog Pala rics : > How Much Raw vse?\nSmarr merte > How mom CApacity PRocescot Use ?\nPutomahon Achont > If WORD InceeaceD 2\n. OOOO TL\nCustomized Dash board ls a Mert gre chergdt wee Cr0 UhI7 Hy\nGo> Resource —y Powershel| Ra —» vm\nGreve\nChee Wein se per te weed.)\nXH\nMETRICS AE LIKE!— score CSwetevm)\n| CRU CREDIT Remaining METLUc NAwE\nw | bata bist Bonduer dhe CVICUae Meee 1054)\nWe | data DUE lOFF Consumyhon METRES\n“ Dise write byte ,\nVN) Nenoork iy THRO AGG REG RAT 10\nVite z\nvn, | REERMERYErELA Percentage - CPU. ‘a |) -Wve rage)\nin.\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 1061,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 37,
          "text": "ae MICROSOFT AZURE - AZ 900 2/\n% (_A2VRE seevick Hem qTy:- Evaluate the impact of Aruve Sevuices\n[Ssue@s woth — percowalized guidance and Support, notheahon and\nissue vegoluhon updates -\nDn case any eure ceric K down, tren [toil]\nnerHect in (service healt) -? once sve Vesloved (tr atl\nbau acne oan on ene\nTe Heck he update on tne Sam-\n* (Uv PLAw eo ooTAge / PLANVED OviAge) AR\nHK | PZLRE KEsoukcl manauce (ACM) TEMPLATES —\n& Btu aq Isow Hic )—> Tt & declerahix\nAzure Vregource Manger C AEM) templates are Javaseryt\nObeck . 4nic Can be Vaed +0 Create and Aeploy Aruve\nMPras tructuve. Lo the ut haves to wont program commands.\nbeclarahve Sym tay (l) Wo need to wnte program\nRepeatable Cesutt .\nOrcnesrrahon (2) No nced +o wnle C eputncrep\nModular Piles powershel| etc -\nBuilt-in - validation\nPape bxparialla. Coole\n— a,\n% | Beauty of Atmi—\nW| te Hou Implement Adm ang t\nYun particth go meae we\nTe—eveeule iFeoil| clo he new\nChanges toienr Tee to\nQ ALM in Cond) by Pava meter\nFile (Same templake UNed\nmv th ple envimnemeut)- Linkedin = https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 1084,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 38,
          "text": "Linkeln= tps: sinkein com/in/sur-sngh-51445522/\n=3 Content\n\na Introduction ||\n2 Azure Services [|\n3 Virtual Machine [|\n4 Virtual Network [|\n5 Storage Services [|\n6 Core Azure Solution [|\n7 Azure Security\n8 Identity, Privacy & Compliance | |\n9 Service SLA and Pricing [|",
          "char_count": 270,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 39,
          "text": "& MICROSOFT AZURE - AZ 900 Ze\nMopule +\n+#|  SEeueity:- The Coming loc s coll Cover tre below Sechan_\n| Aves secoarty tearvees | Azuee Wetaeee Secoecy |\n~ Security Caste * DEFEWSE Ww obeTH\n+ RESDO RCE Hy diene “ NETWORK CecuRTy akour$\n~ Key VACLT “Mew aces\n* peoreatao Hosts “  Doos PROTECTION .\nO11; AZURE CEecoRiTty Chute + In azure we are Creating &\nresources. (vm Machine, Storage) And all these vesource ured\nto be secuved. Me seeurity the resources are from—\n(0) Protect Rem malware -\n12) Protect From by autronred Access,\n(e) Protect From potential attacks / hackevs -\nWhew Possial€? IF hell is some prodem in our in Prastructure-\nAZURE SEcueity CewTeR IS mbuilt I Micrsoft Azure. and\nIt lg monttoriry Ine servvides like (Virtual machine, App Servic)\nDatarase, Storage). BM proude beet protechm to bol,\nAZuve and on-premiue Pala Center. (In Case deployed iy dhe\nVirtus, machine):\nProtect ” Cane CE VICTUM MetniMe , NOw-aAz2URe VIRTUAL MACHINE\na\nAA Bis Pneente Wee ARR EE\n2 ThinG Here %Y Céeveity!> pert clide\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 1076,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 40,
          "text": "ae MICROSOFT AZURE - AZ 900 23\nMICROSOFT S£eu City cen ted, (Azvte) Air ill hete you:\n\nWw | Provides Secunty Yecommendahon— Lt will hele to assy nment ang\nQivVe YA YC Comméencdahon\n\n& | Detect and Block, Malwave:— Tt will hep to analze tne malroare\nattack, immediatly vend alert. and potect vA from such king oF\nmalioare attack.\n\nBt ill detect 4 blocic— Malwere anweul,\n\n@ | avay2ze Ano 1 een Ti POTENTIAL AT Tac KSi— De will help Gnalze\nprotehtial attuel & help to dente tue Same.\n\nWW LYUST- Iw- TIME A ecess Conttor For poet:-\n\nSuppose we have Q virtuel machine , kweo novo we Need\nto oto Pp RDP (Remote Desktop) to that Virtual maching So for\ndome, the RoPon vyrtuak machine. Wwe need to open tne Lomdsw\npot no (#3329) and We are pry SSH login iy Lunyy ( Pert Bay\nee ne eas\n\nInslead open these botn port favamentalerr. we cay lontgur\nmM such a way trot on-demand. When we heed bb do ROP wv\nson System to that wituel mache, Mat hme only Port vail! be\nSper. And these  partnf uolll| Cloned aurtomancailly— a Fre Pus\ntime - Frame -\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 1093,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 41,
          "text": "= MICROSOFT AZURE - AZ 900 2¢\n62. | AZUKE SEcuRITY CEeNnTER- CAPABLLITEG:- ( RESoURCE HY Genes)\nUOPoLicy ComPLiance)— Lith help of policy we install Some Agent Spe)\nbohicl basically help Analze the virtual machine. Qnd Submit the\nweparr i) (Leg amalyst's) +\nAs Seon we Created @ Azure mMachwe, (re agent Softioere\nls metalled wh the Same and thu WW done under poicy, Compluanced .\n(WY) QOvnTmvovs ALECMENTS!— Assess New teployement wesourte fo\nensure that thes Ore Confiqure property. AS Soon neD VM\nCreated watt help oF cent Sis) MStal| and helpus In averment-\n(ly, OT AU KED RecommenpATiON!— Recommendations hated on exishig\nwerktoad —vottn Inghuchon, on hoo to \\nplement trem.\n(om) HREAT PROTECTION!” Amaley ze atttmptee|  thveats thou 4 alevG\nand mpbcted resource report:\nFt] Eyawele © Poetals\nControls May cuvvent Potenay Steve Unhreot Hoy Reource Athen\nScores Stores lnomase Reroute a a\nO7 |Enaie mca lo ° +18% ( lwpomts) | Lord =\n@ 7 | Sere & ° ttyl. ( $ pols) Sofs Wee\nManagemen\nPorts\n® | Ewereyr dota} 4 3.1 42°71, (0-8 I Poury | Lola ——\n9 travel\nUnkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 1116,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 42,
          "text": "ee MICROSOFT AZURE - AZ 900 BE\n- AZURE Sutiwec!- Azuve senhnel cia secunty Infermahon.\nmanagement (SIEM) and Secunt4 automated nes ponse (Soak) Solution\nthat provides secunty analyhis and treat Intelligence acvast an\nenterprise -\n° ottce 36r\na _——<\nIN TE GeatiONs Azuve Advanced Thvear Propechon\nMiwwsolt cloud Appucarin Security:\nRO)\nCOLLECT!=-| Collect data of all user, device, application & Infrasucture\n®\nDETECT S~ under Post-Proce sates, deteckinp te tmeats (Gased on At/ mL)\nINVESTIGATE] nveshgahin oF thrects:\n®\nRespmded by Invokity Some Kind oF Yo too\n03.| AZURE KEY VaULT!- peuve key vault Store applicahon Secreats 14\nQ Cenbalized cloud locahon In order to Seeuvely Conbo| access\npermissions And access logging -\nWY storing secrets Baeneo by HARDWARE SECLEITY MopoLes CHS Mm)\nOQ | secrets Maw aeemeny\n@) key Manraqemeny\n(4) ;\nCELTIOATE Man ng emeN\n° Linkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 922,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 43,
          "text": "<2 MICROSOFT AZURE - AZ 900 B¢\nRw LAZURE DdDenicateo HOTS:- Azuve dedtcatecl hott provides\nphysical Servers Ynat Wost one oy meve Azure virtual machnes\nthat is dedicated to a single organizahon load\n%* This will help allocated dedicored hardedve\nyt? Specitte — Subsoiphon>Cvm, stotaqe veil| allocated dedicated).\nWw  BeENeeits:-\nRY Hardwave isolation at tne server lerels\n* Cenhol over mamttnance event Hinaiy »\n* Ati gned. wit Apure hyd use benfau-\nSEcURE NETWORK ConnectiMmty:-\n(How ‘tne virtual mache 1s secured —?) ¢y—\nOl. | DEFEWSE Ib DEPTH — A layer opproack. to secure computer\nSystem - or Virtual nehoork.\n|| Provides multiple levels of Protechiong -\n“7 Attacks against one Layer are isolaton From subsequent layer\n) © © 3) ie) 2) (2)\nad * Hacker is mayer ly \\ntererted To. (password, Cratitenal deta\\)\nSo the protechey required ot eo Lovet -L\n> HWadceey need to hack Cac layer Staring From Physics Seounty\nto tre Darn Poi. -\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 995,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 44,
          "text": "a MICROSOFT AZURE - Az 900 BP\nSuinkeo Security MoOPEL‘- Aguve pronde Shared Secon model:\nWy thic Certain tatnay ie ake by Micresott and (enaiy ting is take\nCare 4) cu lromer.\n(ty- Below are he detril Around tre tye ct model )\nDate Govevanance and — Cosye me Cobromer bose mw Co Stomer™\n~ Rigne Management\nWuhowt tonna mimoaetr |\nei, yet rt\nmucrosstr\nPageret tne ier | cattemer [mmr\n(# C= Micwesott/ Customer)\nNEtwokic SEcvekt Geovr — NsSqQ Filter netoovk Waffic to and\nForm , Azuve resource on Azure vival nehoovk:\n*k Set Mbound & obkbound yules oko Filter boy Source and\naeshnahon |? addvess ,port® and proincol:\n¥ Add moltple ‘rules 7 a8 needed just Subsophon limite\n¥ dverrjdes Aefaulr rules wlhe nero h wher pranty «\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 782,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 45,
          "text": "fe MICROSOFT AZURE - Az 900 3x\nAZLRE fFrebwacc!— Firevoal| as a Sevvices ( Faas) dnat grants /denies\nServer access based On originatixp If address , m order to\nprotect hehoovt resource:\ni Applies [Inbound and outbound ‘rattle Filtering rules.\nWoy Burlt-m high availasitiry\n\"oT unrestricted cloud Sealatcility\n“ User Ayure monitor losang\nAzure — Applicate aliens ay 7 Ms0 provides @ Hreoal| , web —\n—Oppuicahory fyréwal| (wat). WAt provides Centralized ,\nInbounded = protechon from web applications:\nAzv Re DDoS ( Diste1 eo Teo DEWIAL of Sedvices) PRETECTION\nIF a Sewice Veg uested From molti ple Locahew acrois yrote fe\na Specie Sever, coher lead Slo or un Vesponsive » So Aaure\n1s destgned fo handle Such Situahon and block such kind of Yesuest.\n[rue asic veesion oF dos a ceee|\n* Tr wlll ciutomaheally detected tnat hartle u not coming From a\nQennsian Needy uper. but From a(Bolt) and It will blocked:\nAzure backbone Is pre—proqam to handle  Sech cikvattery:\niS Ensuring For fevver avattibor ty th acer cur “Yea west\nW] Basie Service her & automean cally Rnable\nWo] Foy standard servuwe ter adac mihi gahor Capabilities (2 nll\nWL hetp te ger more detal| Rom where attack lapper,\nATTACKER Inenea Renee =e Lo] Azure ppdos I Virco Ate\nBAackGove Cera PROTECTION 2 NETV OAKS\nNey poss blue ©)| shep rea)\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 1361,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 46,
          "text": "Linkedin - tps: tinkedin.com/in/surj singh 6144S522/\n= Content\n\na Introduction [|\nz Azure Services [|\n3 Virtual Machine [|\n4 Virtual Network ||\n5 Storage Services [|\n6 Core Azure Solution | |\n7 Azure Security [|\n8 Identity, Privacy & Compliance\n9 Service SLA and Pricing ||",
          "char_count": 274,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 47,
          "text": "ee)\ncans MICROSOFT AZURE - AZ 900\nMopnuctE #\nAZ-900, IDENTITY, GovERnance PRivacy & ComPulAr ce\nNey\nAzvute IDERTITY Leevicec:—\n's | kuthenHeateon vevus Autuorgzation:\n2. | Azuve AD, MFA, S80, Qnd Condihdsn Access.\nAzvte Goveenarce Peaqwee? —\nVe Rea\n2. | Resource locke and tags\n3. Polley 7, blue prnt’ and CAF-\nAZv@e PRivacy & ComPuUAVce!—\ni * Privacy statement and onlme Seevicen Terms .\n2. | Trushe center and Compliance dowmenthonc\n3. Azure sovereign VegaQht.\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 524,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 48,
          "text": "cs MICROSOFT AZURE - AZ 900 46\ntl. | Azuee IDES TITY SERVICES - OBJECTIVE PomAiNi—\n7 Explam tne difference beteeern auhtnentcation £ autuenzenns\n> petne Azuve Achve Divectoyy\n> Describe the Funenann k vsuage of Powe ache dict\n= pesotbe tne aah and Upuage OF Condinmal Atcem,\n- wv Up le - Fackyr (MFA) ana Single — Shay (S80) -\nFR | Autwerh cation Autusn yarn -\n—— 2.\n~ lAentties tne person or - Determmer an authenticated pensions\nfeaices Seeking actew tea ~ or seevncek Level acum\nYuource>.\n— Regret  ledthinyat access |-  betine toluene dotn Hticbay Can\nCrede-pab . ACCES , and What they Canude\nwort ‘*.\n- Berc tr Creaking Se Cure_\nldeutty & access Conrms) ( Bered on sdemipiing » prenvde the\nPruciples aceem to ne Seeviter uw cated\nAeA) .\n(The process oF identi Fa licg\ntre perso tobe (a \\amed) u\nCalled Auntnenhcar m0\n® | AZOLE Active DIRECTOeY CARD “AS aR Micrscoft Azure\nCloud— based tdenhty and acess management Setvices\n' Auntnenticahon (employee sign-in to acces vesourres)\n2 Sing te- sing-en (S$o)\n; Appltcaton Maragement\n* Business — to-busnegs: (82 8)\ns Bvemess—to— Customer (82C)\nLG Dence Manag emt nk Linkedin - https://www. linkedin.com/in/suraj-singh-61445522/",
          "char_count": 1177,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 49,
          "text": "a1\n<3 MICROSOFT AZURE - AZ 900 bea\nR |ComDITION ACceSss— ts Used by Azure Reh Diveetory te\nbring signals togetwer ,to make dlecicions ,and enforce organizahae\npelt cies -\nUser or Group Membership\n(e Lo Cahier\nDevites\nApp Ucabiory\nRisk berechen\nRist Detecton> tty help of ML/AD Azure detect the patter\n6F logm like Caty / Country ) If foo Login attempt dene for)\nTandom Locahm ~It will auto derected rusk Detechay |\nASSIGN ROLE Access vicad THE petivity\nhoc e loy's 4 Remwet *\nROLE FRSIG NM\n© ®\nUZURE POCTAL =P ACCESS Comroe Claw)\nCheck access = (Add vole ase nment) >\nPOA AR AS Sacra prores sWbsay\n(Z\nG) sign Rleo>\nOwner\n@ubhi betor\nReader\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 701,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 50,
          "text": "Za MICROSOFT AZURE - AZ 900 Bc\nAZURE GOoveknArc£s METHOD LOGIES\nRBAC (ROLE -Bacen access cont eer)\n7 fine-grained access management\n+ Seq veqrate dues vith he team and grant or the\nAmeunt of acesc +o vsber tat they need 4° peoforn,\nAneir joes :\nEnable aceess +o tre Asuve partap anol cit acces\nto resources:\n* Giving vole +0 parheular secunty prnciple pe-aceu\ntne resource -\nREsouRCE Loce’s | pyotect your Azure yesource from\nacardentaf deletion ov moditicatory , Manages locks at\nSubscrphon , resource qroer, er individual Yesource lee}\nvoity  Anuve pate.\n_ ST\nCan Not Delete Yes Yes abi\nRead onty Yes wo wo\n* Mosty sexvicet are not aste to ve-calt.\n® Vesouvce lock help +o prevent from ace dente detehom .\nsive AZUCE PORTAL 7 SLSScRIPTION > Sirecky SERAEH Winperd F REcouece Lock,\nyack ee\n¢ eae 2 (peci-e) Lorkige> {aedifeetery > on) ee\n® Read al\n(O) Linkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 914,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 51,
          "text": "a3 MICROSOFT AZURE - AZ 900 49\nTAG S1—\n* Neen useful Fer rollma vp bel iv Mfor manny\n& pefine CKEY4VALLE) To de tre 12g for Avy esourcte br\ngery te malTee Informatng .\nx = Provides = Mefn dain fer a Pauve Yesuvees .\n* Logicty Oo qAmared yesource intdD ao tnyanony-\nAZUVGE PO@TALS— Enanyrlt :—\nio ) e 6.\nAZURE PORTAL DY LEFT SIDE EyProee SECTION > TAGS.\nREESE IPOe i Azuve policy help to enfore organteahona|\nStandard and to aaew Com pliancet at-Scale, Provides\ngovernance and resource tonsistene wo iy vequtatory\nCompliance , eae wst and manafemen\n> Evaluates and identities Azuve resource t+nat donot\ncomay roth Your po Udes -\n—-) Provide bu lt -i+) policy and Inthahve — defmatons., under\nwean auch ag storage ’ netoorking compute ,\n6ecu center and Momttorirg . ©\nAZURE PoetTAL EXAMPLE: — ries Dehrahn\nSEU SE PO ETA ERA EE\n® >) ® fessiaenmen Gh\nAZUCE poeta =F SEARCH PD POLICY 7 | Merman\nLinkedin - https://www.inkedin.com/in/sura}-singh-61445522/",
          "char_count": 950,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 52,
          "text": "= MICROSOFT AZURE - AZ 900 5O\nAZURE BLU EPRINTS = makes iH possible For developement\nteam = to rapidly butld and stand up nero envivonement -\nDevelopement team can ae ice build +ust throws,\nOrgauizahonal compliance wrt arset of built-ty com ponent\n(Such as nekoorking ) Mm order to Speed vp alevelopement en i\n\n’ Rote Assignments *\n\n2 Katt ey ASS Van Ment\n\nB Azuve Resource Managers Temp tales\n\nie Resourre Syvoups.\nCLOUD ADOPTION FRAME worice.—\neTemeqy- Define busmecs jushticahon 2 Cypecdced outComes\nPuan = ALIQn achenable adophery) plan to boimecs outcomes.\nReroy— Prepare tne cloud environement fer tne planned charges.\nMIG@aTe — Migrate 4 Modemize EMshing lentloads.\nINNevate~ Develoes Nero Cloud-native or highrtd 6 ution.\nGoveen — Govern tne envivonement 4 uscrrloads\nMaAnene ~ eperatove? Magements fr clord & ligonid sa.\n\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 892,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 53,
          "text": "— MICROSOFT AZURE - AZ 900 5\nok | AZURE Paiwacy!— Micrsoft Comittied ensuring the.\nprivacy OF organ zahons tnrougle ee microsoft Gonhatual\nagreements y»and by provding uv ser Control & trans parenisyy :\nCOMPRLLAN CES — — ume Yiatperat Mircrsoft respect lao and\nregulations and provide comprehesive coverage of “mpliance\naftering’s-\nMicrsoft provides tne mart Complerehencve set-of\nCompliance oF fering, Cinetu dmg Cert fication and atteckahens)\noF any coud 6ervice provider. some compliance edfernep\nIncludimg —\nCore CCRiminat J vetice Iwroemation sysnem)\nHIPAA CHEACTH IntsverAwce\nCSA CTAR CERTIFICATION\n1sO JIEC 27018\nEU ModBU CLAvscEe¥\nNIST (Nattonat Inchtute of Standards 4+ Techndlogy )\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 753,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 54,
          "text": "Linketn- http://w Linkedin. com/in/surasingh-61445522/\n= Content\n\n1 Introduction | |\n2 Azure Services [|\n3 Virtual Machine [|\n4 Virtual Network [|\n5 Storage Services [|\n6 Core Azure Solution zz\n7 Azure Security [|\n8 Identity, Privacy & Compliance [|\n9 Service SLA and Pricing",
          "char_count": 276,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 55,
          "text": "ay MICROSOFT AZURE - Az 900 46\nMopuLle f#\nAZURE PRICING , Ceevice LeveL AqREEmMEDy &£ LiFecyeres\na  ——————————SSS__ eee\nAZURE suBSckletiona— @ rremwivg &MARGING CLT\n* Sublan\\ptimn Option # Bvailiable product & Seavices\nR PrlanGP £TCO Caltulstod.\nSERVICE LEVEL be teEemerts Qaseevnce: LIFE tyre\nKH fae * Premed 2 fevornt tatlively keabars\n* Siena tein your Cloud uphiiu-\nAZULE sSuesagiption s:— @®\n® ® @\nme: . ADD subseprir,\nOpen Azure loan th 4o +0 a z=\nYortet ze aw tay? subsuuptin -\nSr —— SS SS — eee\nchoose Lub cen p ten —\nts Free Trial\n2. Pas—As-You-go\n3. Azuve studenti -\n(rcheose your Subseripren a4 per your toate vequivement\nAnd achveates|\n(12 mont) + i200 ered) “+ feesngrtaer|\n(25+ Sevvies)\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 762,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 56,
          "text": "- MICROSOFT AZURE 4Q\nAZURE PuecHaAt WG!— Three malin dcustome types oN cohich_\ntne avaliable purchased ophoy fer Azuve products and\nCeentes ave Contingen ts ave —\n\n“> Enterprises\n\n—? web dlreck\n\n—F cloud Sduhim Pronder ( Cses)\nFactoR  ARFECTIONG Couts:—\nOe€esovece type @ seevices @ vecaqions\n|\n\nM@ — virtual machine\n\n| Ape services\nOf (@ased on sevvices)\n= Were Of Customer\n|\n®|- Infra chructure Locahan-\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n| Linkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 489,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 57,
          "text": "sae MICROSOFT AZURE +8\n| AZURE PUCCHASING !— Thee main co stomer type of ahith\n-\n| tue avialiable Purchased option fer Ayure productr 4\n| Serv ices ave configured ave—\n|\\-> Ente, prises\ni web- Bite or\n(> Cloud Soluhon Pronder (Css)\n|\n|\n|FactoR Mreeet Coste:—\nA Resovece Tyre-  (Vivhis| wacites CPU/ 2M | storey.)\nVirtwe} Yerource atlocated ihe virtnaft Mmadhire:\nLR SERVICES— — Dapond on type OF Seevice Uka (Enterprice, Web\n| —divec, Wound Gdlubon Partaod.\n|\nLK LocaqcoN— Cost vary between Locahons tnabh oHer Arre\n|\n| Product, Getviced and Yetources.\n|\nIt Banduidh— Deka memirg \\N- and- oad OF Ayuve dotaduter.\n|\n| Some tmbound acum hanster ave Pree , Such ao clota ay\n(WW Ayuve eka Guter . For outbound dake trancfer— soade\nae dora wemy out of Pouve PEDO Sy Bw\n| based Mm zoned.\n| (aze QE > PRICING CALaviatoe ) > CHocse seavice )\nowe sence Seleored (along with Yegion) the price woil|\n| be allocated mbdtHom.\n|\n:\n| (TF Pricting Change an lOcahen Aelge.\n=) Linkedin = https: /7 Jinkedin.com/in/suraj-singh-61445522/",
          "char_count": 1019,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 58,
          "text": "ay MICROSOFT AZURE 42a-\nExXpLORE “Teco ( TOTAL cost ot ow NEeLHIP) > —\nA tool estimate cost Saving You can\n| Yeolize by a | to Azure.\n| A report compares tne Cos of on-premises Mfrastucture\n| tery ne Cost of Ugmg Azuve product and sewice in\n| Cloud - ;\nNhe\nyy] ot Compote oy. eh\n4 2. DATA CEOTER 93y. oy.\nOo\n}\ni %. METWoRKing, 2}: 42y.\nXx\naq)\ni 4. STORAGE 4y. Soy.\n\\ .\n/\nExample tost = (Paerros, 499) (592/612)\nMINIMIZING Coste: (Pm-uU- CEA)\n® 1) PERFORM — Perform Lost analyses. Use ( Pricing | TEO Calewloter)\n:\naR iz) MowttOoR — Monttor usuage and Azure Advisor -\n#* Dvse — vse ma Free trat customer C spending rm tle )\nHw se — vpe Azure Reservation 4 Azure hybrd Bentil (Hob)\n+ 9 CHooLe-— Cheote Low-Coit locaton: and Peps .\nwe 4) KEEP — Keep vretodate with laert Lubscripnan . ober,\n& 17) Apeey Apply tag to densify (os\\- 00 nev\nLinkedin - https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 895,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 59,
          "text": "(se)\nMICROSOFT AZURE Oe\nEXPLORE support — option !—\n| Every Azure subscrtphon Meludeqd Free access fe\nbilling and — subseriptter, support; Azure porta) products and\n| Services dowmentahnm, onlmne cett-hele docomentate,\n“uhrle paper and comm on ity Sup pert -\n“7 Also reach 40 me , In care any Supper vequired\nGrownd tne ee %\n| -Scort TECH NCH Sov—RR\nBaste railiable. + all oe\nMicrsoft A2zuve. accoury\nDEVELOPER Tat & Mon-produchs, Business hows acces to\n| Envivonement Lop pens regret vig\n| STawo AeD Producten) vomrttoog| suyy accen to Lusget\n| Envi ovement Exgueen ma pane] ene\n|\n| PROFESSIONAL Lopiness- (niheay \"\n|\nDeperndecor.\n|\n|\n| Linkedin - https://wwwlinkedin.com/in/suraj-singh-61445522/",
          "char_count": 698,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 60,
          "text": "\\\n\n= MICROSOFT AZURE Fy\n\n| Performance targetc ave €xpressed as uphme and\n\n| Connectwity) gurantees\n\n| ®  Performace -terget From 39-987. (3-5) + 99-997.(4-4)\n\n|\n\n* IF a services fale to meet the quam tees ,oa\n\npercentage of ea Servue fees cam be Credited .\n\n|\n\n| | 952073 43.2 mune tes @76 heurs\n\n| 99> asy. U6 minvtes 4:49 nWours ”\n\n|\n\n99-99%. 4:22 mmubo S2.56 m motes,\n\n|\n\n|\n\n|\n\n|\n\n|\n\n| Linkedin « https://www.linkedin.com/in/suraj-singh-61445522/",
          "char_count": 451,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 61,
          "text": "THaANk You |\n\ni,\nmites\n\nWOKE",
          "char_count": 29,
          "ocr_used": true,
          "original_char_count": 0
        }
      ],
      "metadata": {
        "format": "PDF 1.3",
        "title": "",
        "author": "",
        "subject": "",
        "keywords": "",
        "creator": "",
        "producer": "",
        "creationDate": "",
        "modDate": "",
        "trapped": "",
        "encryption": null
      },
      "char_count": 52779,
      "word_count": 9068,
      "ocr_pages_count": 61,
      "error": null,
      "file_id": "1FZ5eBG5D1zTypUvKHoUGsNQMrsbNGXue",
      "filename": "Azure Notes.pdf",
      "filepath": "downloads/Azure Notes.pdf",
      "download_link": "https://drive.google.com/uc?export=download&id=1FZ5eBG5D1zTypUvKHoUGsNQMrsbNGXue"
    },
    {
      "success": true,
      "text": "Start-ups\nP2P\nLending\nSaaS\nAggregator\nCrowd\nSourcing\nMarketplace\nMarketplace\nExchange\nServices\nSubscription\nB2B\nB2C\nTypes of Startups\n31 Startup Business\nModels You Must Know\n(With Brand Examples)\nNavdeep Yadav\nCredit:\n01\nFreemium = Free + Premium\nThe freemium business model allows users to\nutilize basic features of a software, game, or\nservice for “free”, and then charges for\n“upgrades” to the basic package.\nFreemium Business Model\nExamples:\n02\nTypes of\nSubscription \nSubscription\nBox\nSubscription\nPass\nUmbrella\nSubscription\nEcosystem\nSubscription\nSubscription business models are based on the\nidea of selling a product or service to receive\nmonthly or yearly recurring subscription\nrevenue.\nSubscription Business Model\nExamples:\n03\nA marketplace is a platform (e-commerce site\nor mobile app) where products or services are\nsold to customers by third- party sellers.\nMarketplace Business Model\nExamples:\n04\n In the aggregator business model, the service\nis delivered to customers under their brand\nname.\nAggregator Business Model\nExamples:\n05\nPay-As-You-\nGo Plans\nCredit-Based\nSaaS\nMarketplace\nServices\nConsumption-\nBased\nThis is a pricing strategy where users pay\nbased on how much they consume. It is\nwidely used by cloud computing companies.\nPay-As-You-Go Business Model\nExamples:\n2.9% + 30￠ \nAudiobooks for\ncredits\n06\nFFS’s business model is based on charging\ncustomers a fixed and variable fee for every\nsuccessful payment.\nFee-For-Service (FFS)\nBusiness Model\nExamples:\n07\nEdTech\nBusiness\nModels\nMarketplace\nFree Trial\nSubscription\nFreemium\nSubscription\nAd Revenue\nBusiness\nModel\nIn the EdTech business model, you make\nmoney either by selling content or providing\nteaching as a service to the end user.\nExamples:\nEdTech Business Model\n08\nThe 'lock-in' strategy aims to keep customers\nfrom switching to competitors by increasing\nswitching costs or the effort to transfer, or by\noffering a better brand experience or\nincentives.\nExamples:\nLock-in business model\n09\nAPIs\nDeveloper\nGets Paid\nDeveloper\nPays\nFree\nIndirect\nAn application programming interface, or\nsimply an API, is a tool that allows third- party\napplications to communicate with your\nservice.\nExamples:\nAPI Licensing Business Model\n10\nOpen-source software is software with source\ncode that anyone can inspect, modify, and\nenhance for personal use.\nExamples:\nOpen-Source Business Model\n11\nThe app or algorithm collects data from\ncustomers to improve the system or monetize\nit by adding value to other companies.\nExamples:\nData as a Business Model\nGPT 3\n12\nBlockchain Business\nModel\nBlockchain-Based Software Products\nToken Economy - Utility Token Business Model\nP2P Blockchain Business Model\nBlockchain as a Service Business Model (BaaS)\nToken Economy\nDevelopment Platforms\nNetwork Fee Charge\nBlockchain is a distributed ledger technology that\nallows other companies and business to deploy\ntheir smart contract without any central authority\nlike AWS, Digital Ocean, etc.\nExamples:\nBlockchain Business Model\n13\nIn the freeterprise (free & enterprise) business\nmodel the free professional accounts are\ndriven into the funnel through the free product\nand later convert into a B2B/enterprise\naccount.\nExamples:\nFreeterprise Business Model\n14\nThis model is widely used in hardware items\nwhere one item is sold at a low price or at\nlosses and generates profits from refills or\nadd-ons.\nExamples:\nRazor Blade Business Model\n15\nIn the D2C business model, the brand cut out\nthe middleman and sells its product directly to\nthe end consumer from its website using a\nthird-party logistic partner.\nDirect-To-Consumer (D2C)\nBusiness Model\nExamples:\n16\nA private label/white label product is\nmanufactured by a contract manufacturer or a\nthird-party supplier and sold under the brand\nname.\nPrivate Label vs. White Label\nBusiness Model\nExamples:\n17\nHere, the franchisee (store owner) uses the\nbrand, branding and business model of a\nfranchisor (company).\nExamples:\nFranchise Business Model\n18\nThis business model is used by social media\nand search engine giants who use your search\nengine and interest data to show ads.\nExamples:\nAd-Based Business Model\n19\nIt's a diversification business strategy where\neach business unit like the tentacle of an\noctopus works independently but is\nconnected to the main body.\nExamples:\nOctopus Business Model\n20\nThe revenue is generated by directly selling an\nitem or a service to a customer. Widely used\nby e-commerce sites or any other product you\npurchase online.\nExamples:\nTransactional Business Model\n21\nIn a P2P business model, two individuals interact to\nbuy and sell goods and services directly with each\nother without a third party or using a platform.\nPeer-To-Peer (P2P)\nBusiness Model\nExamples:\n22\nIn the peer-to-peer (P2P) lending business model, a\nprivate individual (\"P2P lender\") lends/invests or\nborrows money from another private individual\n(\"P2P borrower\").\nP2P Lending as a Business\nModel\nExamples:\n23\nBrokerage businesses usually charge a\ncommission or fee to one or both parties in\nexchange for services rendered. \nExamples:\nBrokerage Business Model\n24\nDropshipping is an e-commerce retail model\nthat allows stores to sell products without\nkeeping any physical inventory.\nDropshipping Business Model\n25\nIt is based on the idea of a shared economy that\nprovides millennials with the flexibility of living or\nworking out of shared spaces without the headache\nof ownership or lease.\nSpace as a Service Business\nModel\nExamples:\n26\nIn the 3PL business model, the business\noutsources its products’ distribution,\nwarehousing, and fulfillment to an external\nlogistics company that carries out these\nprocesses on its behalf.\n3rd Party Logistic (3PL)\nBusiness Model\nExamples:\n27\nLast-mile delivery consists of the set of\nactivities in a supply chain that will bring the\nservice and product to the final customer.\nLast Mile Delivery as a\nBusiness Model\nExamples:\n28\nAffiliate marketing is a revenue-earning\nstrategy of promoting other companies'\nproducts and charging commissions for every\nsale.\nExamples:\nAffiliate Business Model\n29\nIn-App Purchases\nProducts\nIn-App Products\nConsumeable\nNon-Consumeable\nSubscriptions\nThis is also known as the in-app purchase\nwhere you are required to make in-app\npurchases for an intangible product.\nThe Virtual Goods Business\nModel\nExamples:\n30\nGhost Kitchens, also known as Dark or Black\nBox Kitchens, are restaurants that exclusively\nsell meals for home delivery, with no dine-in\nservice.\nExamples:\nCloud Kitchen Business Models\n31\nCrowdsourcing = Crowd acts as a source for a\nspecific platform.\n \nIn a crowdsourcing business model, you voluntarily\nget help from people around the world without\nhaving to hire them as regular employees.\nCrowdsourcing Business Models",
      "page_count": 32,
      "pages": [
        {
          "page": 1,
          "text": "Start-ups\nP2P\nLending\nSaaS\nAggregator\nCrowd\nSourcing\nMarketplace\nMarketplace\nExchange\nServices\nSubscription\nB2B\nB2C\nTypes of Startups\n31 Startup Business\nModels You Must Know\n(With Brand Examples)\nNavdeep Yadav\nCredit:",
          "char_count": 219,
          "ocr_used": false
        },
        {
          "page": 2,
          "text": "01\nFreemium = Free + Premium\nThe freemium business model allows users to\nutilize basic features of a software, game, or\nservice for “free”, and then charges for\n“upgrades” to the basic package.\nFreemium Business Model\nExamples:",
          "char_count": 228,
          "ocr_used": false
        },
        {
          "page": 3,
          "text": "02\nTypes of\nSubscription \nSubscription\nBox\nSubscription\nPass\nUmbrella\nSubscription\nEcosystem\nSubscription\nSubscription business models are based on the\nidea of selling a product or service to receive\nmonthly or yearly recurring subscription\nrevenue.\nSubscription Business Model\nExamples:",
          "char_count": 288,
          "ocr_used": false
        },
        {
          "page": 4,
          "text": "03\nA marketplace is a platform (e-commerce site\nor mobile app) where products or services are\nsold to customers by third- party sellers.\nMarketplace Business Model\nExamples:",
          "char_count": 174,
          "ocr_used": false
        },
        {
          "page": 5,
          "text": "04\n In the aggregator business model, the service\nis delivered to customers under their brand\nname.\nAggregator Business Model\nExamples:",
          "char_count": 136,
          "ocr_used": false
        },
        {
          "page": 6,
          "text": "05\nPay-As-You-\nGo Plans\nCredit-Based\nSaaS\nMarketplace\nServices\nConsumption-\nBased\nThis is a pricing strategy where users pay\nbased on how much they consume. It is\nwidely used by cloud computing companies.\nPay-As-You-Go Business Model\nExamples:\n2.9% + 30￠ \nAudiobooks for\ncredits",
          "char_count": 279,
          "ocr_used": false
        },
        {
          "page": 7,
          "text": "06\nFFS’s business model is based on charging\ncustomers a fixed and variable fee for every\nsuccessful payment.\nFee-For-Service (FFS)\nBusiness Model\nExamples:",
          "char_count": 157,
          "ocr_used": false
        },
        {
          "page": 8,
          "text": "07\nEdTech\nBusiness\nModels\nMarketplace\nFree Trial\nSubscription\nFreemium\nSubscription\nAd Revenue\nBusiness\nModel\nIn the EdTech business model, you make\nmoney either by selling content or providing\nteaching as a service to the end user.\nExamples:\nEdTech Business Model",
          "char_count": 265,
          "ocr_used": false
        },
        {
          "page": 9,
          "text": "08\nThe 'lock-in' strategy aims to keep customers\nfrom switching to competitors by increasing\nswitching costs or the effort to transfer, or by\noffering a better brand experience or\nincentives.\nExamples:\nLock-in business model",
          "char_count": 225,
          "ocr_used": false
        },
        {
          "page": 10,
          "text": "09\nAPIs\nDeveloper\nGets Paid\nDeveloper\nPays\nFree\nIndirect\nAn application programming interface, or\nsimply an API, is a tool that allows third- party\napplications to communicate with your\nservice.\nExamples:\nAPI Licensing Business Model",
          "char_count": 234,
          "ocr_used": false
        },
        {
          "page": 11,
          "text": "10\nOpen-source software is software with source\ncode that anyone can inspect, modify, and\nenhance for personal use.\nExamples:\nOpen-Source Business Model",
          "char_count": 153,
          "ocr_used": false
        },
        {
          "page": 12,
          "text": "11\nThe app or algorithm collects data from\ncustomers to improve the system or monetize\nit by adding value to other companies.\nExamples:\nData as a Business Model\nGPT 3",
          "char_count": 167,
          "ocr_used": false
        },
        {
          "page": 13,
          "text": "12\nBlockchain Business\nModel\nBlockchain-Based Software Products\nToken Economy - Utility Token Business Model\nP2P Blockchain Business Model\nBlockchain as a Service Business Model (BaaS)\nToken Economy\nDevelopment Platforms\nNetwork Fee Charge\nBlockchain is a distributed ledger technology that\nallows other companies and business to deploy\ntheir smart contract without any central authority\nlike AWS, Digital Ocean, etc.\nExamples:\nBlockchain Business Model",
          "char_count": 454,
          "ocr_used": false
        },
        {
          "page": 14,
          "text": "13\nIn the freeterprise (free & enterprise) business\nmodel the free professional accounts are\ndriven into the funnel through the free product\nand later convert into a B2B/enterprise\naccount.\nExamples:\nFreeterprise Business Model",
          "char_count": 228,
          "ocr_used": false
        },
        {
          "page": 15,
          "text": "14\nThis model is widely used in hardware items\nwhere one item is sold at a low price or at\nlosses and generates profits from refills or\nadd-ons.\nExamples:\nRazor Blade Business Model",
          "char_count": 182,
          "ocr_used": false
        },
        {
          "page": 16,
          "text": "15\nIn the D2C business model, the brand cut out\nthe middleman and sells its product directly to\nthe end consumer from its website using a\nthird-party logistic partner.\nDirect-To-Consumer (D2C)\nBusiness Model\nExamples:",
          "char_count": 218,
          "ocr_used": false
        },
        {
          "page": 17,
          "text": "16\nA private label/white label product is\nmanufactured by a contract manufacturer or a\nthird-party supplier and sold under the brand\nname.\nPrivate Label vs. White Label\nBusiness Model\nExamples:",
          "char_count": 194,
          "ocr_used": false
        },
        {
          "page": 18,
          "text": "17\nHere, the franchisee (store owner) uses the\nbrand, branding and business model of a\nfranchisor (company).\nExamples:\nFranchise Business Model",
          "char_count": 144,
          "ocr_used": false
        },
        {
          "page": 19,
          "text": "18\nThis business model is used by social media\nand search engine giants who use your search\nengine and interest data to show ads.\nExamples:\nAd-Based Business Model",
          "char_count": 164,
          "ocr_used": false
        },
        {
          "page": 20,
          "text": "19\nIt's a diversification business strategy where\neach business unit like the tentacle of an\noctopus works independently but is\nconnected to the main body.\nExamples:\nOctopus Business Model",
          "char_count": 189,
          "ocr_used": false
        },
        {
          "page": 21,
          "text": "20\nThe revenue is generated by directly selling an\nitem or a service to a customer. Widely used\nby e-commerce sites or any other product you\npurchase online.\nExamples:\nTransactional Business Model",
          "char_count": 197,
          "ocr_used": false
        },
        {
          "page": 22,
          "text": "21\nIn a P2P business model, two individuals interact to\nbuy and sell goods and services directly with each\nother without a third party or using a platform.\nPeer-To-Peer (P2P)\nBusiness Model\nExamples:",
          "char_count": 200,
          "ocr_used": false
        },
        {
          "page": 23,
          "text": "22\nIn the peer-to-peer (P2P) lending business model, a\nprivate individual (\"P2P lender\") lends/invests or\nborrows money from another private individual\n(\"P2P borrower\").\nP2P Lending as a Business\nModel\nExamples:",
          "char_count": 212,
          "ocr_used": false
        },
        {
          "page": 24,
          "text": "23\nBrokerage businesses usually charge a\ncommission or fee to one or both parties in\nexchange for services rendered. \nExamples:\nBrokerage Business Model",
          "char_count": 153,
          "ocr_used": false
        },
        {
          "page": 25,
          "text": "24\nDropshipping is an e-commerce retail model\nthat allows stores to sell products without\nkeeping any physical inventory.\nDropshipping Business Model",
          "char_count": 150,
          "ocr_used": false
        },
        {
          "page": 26,
          "text": "25\nIt is based on the idea of a shared economy that\nprovides millennials with the flexibility of living or\nworking out of shared spaces without the headache\nof ownership or lease.\nSpace as a Service Business\nModel\nExamples:",
          "char_count": 224,
          "ocr_used": false
        },
        {
          "page": 27,
          "text": "26\nIn the 3PL business model, the business\noutsources its products’ distribution,\nwarehousing, and fulfillment to an external\nlogistics company that carries out these\nprocesses on its behalf.\n3rd Party Logistic (3PL)\nBusiness Model\nExamples:",
          "char_count": 242,
          "ocr_used": false
        },
        {
          "page": 28,
          "text": "27\nLast-mile delivery consists of the set of\nactivities in a supply chain that will bring the\nservice and product to the final customer.\nLast Mile Delivery as a\nBusiness Model\nExamples:",
          "char_count": 186,
          "ocr_used": false
        },
        {
          "page": 29,
          "text": "28\nAffiliate marketing is a revenue-earning\nstrategy of promoting other companies'\nproducts and charging commissions for every\nsale.\nExamples:\nAffiliate Business Model",
          "char_count": 168,
          "ocr_used": false
        },
        {
          "page": 30,
          "text": "29\nIn-App Purchases\nProducts\nIn-App Products\nConsumeable\nNon-Consumeable\nSubscriptions\nThis is also known as the in-app purchase\nwhere you are required to make in-app\npurchases for an intangible product.\nThe Virtual Goods Business\nModel\nExamples:",
          "char_count": 247,
          "ocr_used": false
        },
        {
          "page": 31,
          "text": "30\nGhost Kitchens, also known as Dark or Black\nBox Kitchens, are restaurants that exclusively\nsell meals for home delivery, with no dine-in\nservice.\nExamples:\nCloud Kitchen Business Models",
          "char_count": 189,
          "ocr_used": false
        },
        {
          "page": 32,
          "text": "31\nCrowdsourcing = Crowd acts as a source for a\nspecific platform.\n \nIn a crowdsourcing business model, you voluntarily\nget help from people around the world without\nhaving to hire them as regular employees.\nCrowdsourcing Business Models",
          "char_count": 238,
          "ocr_used": false
        }
      ],
      "metadata": {
        "format": "PDF 1.4",
        "title": "",
        "author": "",
        "subject": "",
        "keywords": "",
        "creator": "",
        "producer": "",
        "creationDate": "",
        "modDate": "",
        "trapped": "",
        "encryption": null
      },
      "char_count": 6704,
      "word_count": 1016,
      "ocr_pages_count": 0,
      "error": null,
      "file_id": "1_MK3PURUsXoLN5SR1VmBMWyJw0GlBa6C",
      "filename": "Business Models.pdf",
      "filepath": "downloads/Business Models.pdf",
      "download_link": "https://drive.google.com/uc?export=download&id=1_MK3PURUsXoLN5SR1VmBMWyJw0GlBa6C"
    },
    {
      "success": true,
      "text": "Bhavishya Pandit\nContext\nEngineering \nContext\nEngineering\nBhavishya Pandit\nIs Context Engineering  a Game\nChanger?\nLLMs aren’t “smart” they predict based on what you feed them. Without\nengineered context, you get:\nContext pipelines reduce hallucinations significantly\nMissed info (“lost in the middle” effect)\nAccuracy drop beyond 32K tokens, even in “long-context” models\nBhavishya Pandit\nContext Engineering ≠ Prompt\nEngineering\nThink prompting is all you need to make AI smart? Think again.\nPrompting is just a message. Context engineering is the entire setup that makes\nthat message powerful.\nGood prompting gets you a response. Good context engineering gets you the right\nresponse every time.\nAI doesn’t think in a vacuum. The better the context, the better the intelligence.\nHere’s the difference:\nThe Three Pillars\nContext Engineering isn’t just about words it’s about building the perfect\nenvironment for AI reasoning.\nContext assembly: Dynamically selecting and formatting the right info for the task.\n Context orchestration: Managing flow what comes first, what gets updated, what’s\npersistent memory vs. one-shot input.\n Context optimization: Compressing, summarizing, and restructuring to fit token\nlimits without losing meaning.\nThese pillars turn chaotic data into structured reasoning fuel.\nHere are the 3 pillars that make it work:\nBhavishya Pandit\nContext\nassembly\nContext\norchestration\nContext\noptimization\nWhy do they matter?\nWithout strong pillars, context engineering collapses into… well, just fancy\nprompting.\nHere’s why they matter:\nClarity: The AI knows exactly what’s important and what’s not.\nConsistency: You get reliable, repeatable results instead of guesswork.\nControl: You shape the AI’s reasoning instead of leaving it to chance.\nBottom line: Mastering the 3 pillars turns AI from a “text generator” into a precision\nproblem-solver.\nBhavishya Pandit\nClarity\nConsistency\nControl\nComponents of a Modern\nContext Window\nToday’s AI doesn’t just read your prompt it works inside a layered context window.\nHere’s what makes it tick:\nSystem instructions: High-level directives that define the model's persona, rules,\nand behavior (e.g., “You are an expert legal assistant”).\nConversation history: Record of previous turns in dialogue to maintain coherence.\nRetrieved knowledge: External facts pulled from databases, or APIs to ground the\nmodel's response in reality. This is where techniques like RAG come into play.\nTool definitions and outputs: Descriptions of available tools (like a calculator or a\nweb search API) and the results from recent tool calls.\nUser preferences and Long-Term memory: Information about the user that persists\nacross sessions, such as their communication style or previous support tickets.\nOutput schemas: Instructions, often in formats like JSON, that constrain the\nmodel's output to a specific structure.\nBhavishya Pandit\nComponents of Context Engineering\n4 Context Management\nStrategies\nManaging context isn’t about stuffing more data into AI it’s about shaping its\nmental workspace. These 4 strategies make the difference between noise and\nprecision:\n1.Write: Define the task with crystal-clear instructions and reasoning steps. (If you don’t\ntell AI how to think, it makes up its own rules.)\n2.Select: Pick only the information that matters. (More data ≠ better answers.\nRelevance wins every time.)\nBhavishya Pandit\n3. Compress: Summarize large chunks of data without losing meaning. (Context\nwindows are limited make every token count.)\n4. Isolate: Protect critical facts or instructions so they never get overridden.\nBhavishya Pandit\n4 Context Management\nStrategies\nThe Architecture Stack\nThink of AI like a brain it only works well when the “mental workspace” is designed\nright. That’s where the Context Architecture Stack comes in:\nRaw data layer: Facts, knowledge base, documents, APIs. The building blocks.\nCuration layer: Filters and ranks the most relevant info for the task.\nStructuring layer: Formats context with instructions, examples, memory, and\nconstraints.\nDynamic assembly layer: Real-time orchestration of all the above to match the AI’s\nreasoning step-by-step.\nGreat prompts live here but great context stacks make prompts unstoppable.\nBhavishya Pandit\nRaw data\nlayer\nCuration\nlayer\nStructuring\nlayer\nDynamic\nassembly layer\nStatic vs Dynamic Layers\nNot all context is created equal. AI thinks better when you know what to “lock in”\nand what to “adapt.”\nStatic context: Fixed information that rarely changes. Example: Brand guidelines,\ntone of voice, core rules.\nDynamic context: Flexible, task-specific data that shifts in real time. Example: Live\nuser inputs, recent chat history, fresh search results.\nThe secret sauce? Blend static + dynamic layers to give AI both stability and agility.\nBhavishya Pandit\nFlow of Context Engineering\nFilter\nStructure\nAssemble\nAdapt\nContext Engineering isn’t a one-shot trick it’s a flow that turns raw data into AI\nreasoning power.\nCollect: Gather relevant info, past work, and live inputs.\nFilter: Cut out noise, keep only what truly matters.\n Structure: Organize data into clear instructions, examples, and rules.\nAssemble: Combine static + dynamic layers to build the perfect context window.\nAdapt: Continuously update context as the task or conversation evolves\nBhavishya Pandit\nCollect\nBhavishya Pandit\nJoin our newsletter for:\n Step-by-step guides to mastering complex topics\n Industry trends & innovations delivered straight to your inbox\n Actionable tips to enhance your skills and stay competitive\nInsights on cutting-edge AI & software development\n💡 Whether you're a developer, researcher, or tech enthusiast, this newsletter is your\nshortcut to staying informed and ahead of the curve.\nStay Ahead with Our Tech Newsletter! 🚀\n👉 Join 1.1k+ leaders and professionals to stay ahead in GenAI!\n🔗 https://bhavishyapandit9.substack.com/\nBhavishya Pandit\nREPOST\nLIKE\nCOMMENT\nFollow to stay updated on\nGenerative AI",
      "page_count": 13,
      "pages": [
        {
          "page": 1,
          "text": "Bhavishya Pandit\nContext\nEngineering \nContext\nEngineering",
          "char_count": 59,
          "ocr_used": false
        },
        {
          "page": 2,
          "text": "Bhavishya Pandit\nIs Context Engineering  a Game\nChanger?\nLLMs aren’t “smart” they predict based on what you feed them. Without\nengineered context, you get:\nContext pipelines reduce hallucinations significantly\nMissed info (“lost in the middle” effect)\nAccuracy drop beyond 32K tokens, even in “long-context” models",
          "char_count": 315,
          "ocr_used": false
        },
        {
          "page": 3,
          "text": "Bhavishya Pandit\nContext Engineering ≠ Prompt\nEngineering\nThink prompting is all you need to make AI smart? Think again.\nPrompting is just a message. Context engineering is the entire setup that makes\nthat message powerful.\nGood prompting gets you a response. Good context engineering gets you the right\nresponse every time.\nAI doesn’t think in a vacuum. The better the context, the better the intelligence.\nHere’s the difference:",
          "char_count": 431,
          "ocr_used": false
        },
        {
          "page": 4,
          "text": "The Three Pillars\nContext Engineering isn’t just about words it’s about building the perfect\nenvironment for AI reasoning.\nContext assembly: Dynamically selecting and formatting the right info for the task.\n Context orchestration: Managing flow what comes first, what gets updated, what’s\npersistent memory vs. one-shot input.\n Context optimization: Compressing, summarizing, and restructuring to fit token\nlimits without losing meaning.\nThese pillars turn chaotic data into structured reasoning fuel.\nHere are the 3 pillars that make it work:\nBhavishya Pandit\nContext\nassembly\nContext\norchestration\nContext\noptimization",
          "char_count": 621,
          "ocr_used": false
        },
        {
          "page": 5,
          "text": "Why do they matter?\nWithout strong pillars, context engineering collapses into… well, just fancy\nprompting.\nHere’s why they matter:\nClarity: The AI knows exactly what’s important and what’s not.\nConsistency: You get reliable, repeatable results instead of guesswork.\nControl: You shape the AI’s reasoning instead of leaving it to chance.\nBottom line: Mastering the 3 pillars turns AI from a “text generator” into a precision\nproblem-solver.\nBhavishya Pandit\nClarity\nConsistency\nControl",
          "char_count": 486,
          "ocr_used": false
        },
        {
          "page": 6,
          "text": "Components of a Modern\nContext Window\nToday’s AI doesn’t just read your prompt it works inside a layered context window.\nHere’s what makes it tick:\nSystem instructions: High-level directives that define the model's persona, rules,\nand behavior (e.g., “You are an expert legal assistant”).\nConversation history: Record of previous turns in dialogue to maintain coherence.\nRetrieved knowledge: External facts pulled from databases, or APIs to ground the\nmodel's response in reality. This is where techniques like RAG come into play.\nTool definitions and outputs: Descriptions of available tools (like a calculator or a\nweb search API) and the results from recent tool calls.\nUser preferences and Long-Term memory: Information about the user that persists\nacross sessions, such as their communication style or previous support tickets.\nOutput schemas: Instructions, often in formats like JSON, that constrain the\nmodel's output to a specific structure.\nBhavishya Pandit\nComponents of Context Engineering",
          "char_count": 1001,
          "ocr_used": false
        },
        {
          "page": 7,
          "text": "4 Context Management\nStrategies\nManaging context isn’t about stuffing more data into AI it’s about shaping its\nmental workspace. These 4 strategies make the difference between noise and\nprecision:\n1.Write: Define the task with crystal-clear instructions and reasoning steps. (If you don’t\ntell AI how to think, it makes up its own rules.)\n2.Select: Pick only the information that matters. (More data ≠ better answers.\nRelevance wins every time.)\nBhavishya Pandit",
          "char_count": 463,
          "ocr_used": false
        },
        {
          "page": 8,
          "text": "3. Compress: Summarize large chunks of data without losing meaning. (Context\nwindows are limited make every token count.)\n4. Isolate: Protect critical facts or instructions so they never get overridden.\nBhavishya Pandit\n4 Context Management\nStrategies",
          "char_count": 252,
          "ocr_used": false
        },
        {
          "page": 9,
          "text": "The Architecture Stack\nThink of AI like a brain it only works well when the “mental workspace” is designed\nright. That’s where the Context Architecture Stack comes in:\nRaw data layer: Facts, knowledge base, documents, APIs. The building blocks.\nCuration layer: Filters and ranks the most relevant info for the task.\nStructuring layer: Formats context with instructions, examples, memory, and\nconstraints.\nDynamic assembly layer: Real-time orchestration of all the above to match the AI’s\nreasoning step-by-step.\nGreat prompts live here but great context stacks make prompts unstoppable.\nBhavishya Pandit\nRaw data\nlayer\nCuration\nlayer\nStructuring\nlayer\nDynamic\nassembly layer",
          "char_count": 675,
          "ocr_used": false
        },
        {
          "page": 10,
          "text": "Static vs Dynamic Layers\nNot all context is created equal. AI thinks better when you know what to “lock in”\nand what to “adapt.”\nStatic context: Fixed information that rarely changes. Example: Brand guidelines,\ntone of voice, core rules.\nDynamic context: Flexible, task-specific data that shifts in real time. Example: Live\nuser inputs, recent chat history, fresh search results.\nThe secret sauce? Blend static + dynamic layers to give AI both stability and agility.\nBhavishya Pandit",
          "char_count": 484,
          "ocr_used": false
        },
        {
          "page": 11,
          "text": "Flow of Context Engineering\nFilter\nStructure\nAssemble\nAdapt\nContext Engineering isn’t a one-shot trick it’s a flow that turns raw data into AI\nreasoning power.\nCollect: Gather relevant info, past work, and live inputs.\nFilter: Cut out noise, keep only what truly matters.\n Structure: Organize data into clear instructions, examples, and rules.\nAssemble: Combine static + dynamic layers to build the perfect context window.\nAdapt: Continuously update context as the task or conversation evolves\nBhavishya Pandit\nCollect",
          "char_count": 519,
          "ocr_used": false
        },
        {
          "page": 12,
          "text": "Bhavishya Pandit\nJoin our newsletter for:\n Step-by-step guides to mastering complex topics\n Industry trends & innovations delivered straight to your inbox\n Actionable tips to enhance your skills and stay competitive\nInsights on cutting-edge AI & software development\n💡 Whether you're a developer, researcher, or tech enthusiast, this newsletter is your\nshortcut to staying informed and ahead of the curve.\nStay Ahead with Our Tech Newsletter! 🚀\n👉 Join 1.1k+ leaders and professionals to stay ahead in GenAI!\n🔗 https://bhavishyapandit9.substack.com/",
          "char_count": 549,
          "ocr_used": false
        },
        {
          "page": 13,
          "text": "Bhavishya Pandit\nREPOST\nLIKE\nCOMMENT\nFollow to stay updated on\nGenerative AI",
          "char_count": 78,
          "ocr_used": false
        }
      ],
      "metadata": {
        "format": "PDF 1.4",
        "title": "Orange Modern Blog Tips Linkedin Carousel Ad",
        "author": "content team",
        "subject": "",
        "keywords": "DAGvNqxz-jo,BAGQ0G2eswY,0",
        "creator": "Canva",
        "producer": "Canva",
        "creationDate": "D:20250807033250+00'00'",
        "modDate": "D:20250807033245+00'00'",
        "trapped": "",
        "encryption": null
      },
      "char_count": 5931,
      "word_count": 873,
      "ocr_pages_count": 0,
      "error": null,
      "file_id": "10drGCpJ_X5410XS618J79ruMhmWmADgA",
      "filename": "Context Engineering 101.pdf",
      "filepath": "downloads/Context Engineering 101.pdf",
      "download_link": "https://drive.google.com/uc?export=download&id=10drGCpJ_X5410XS618J79ruMhmWmADgA"
    },
    {
      "success": true,
      "text": "Critical\nLogs to \nMonitor: A \nGuide for\nSOC \nAnalysts\nPage 2|36\nTable of Contents\n1. Introduction ......................................................................... 3\n1.\nImportance of Log Monitoring in SOC............................................................................ 3\n2.\nScope and Purpose of the Guide................................................................................... 4\n2. Key Types of Logs .................................................................. 6\n1.\nSystem Logs (Windows, Linux, macOS.......................................................................... 6\n2.\nNetwork Logs (Firewall, Router, IDS/IPS) ....................................................................... 9\n3.\nApplication and Database Logs .................................................................................. 15\n4.\nSecurity Logs (AV, EDR, XDR) ...................................................................................... 19\n5.\nCloud Logs (AWS, Azure, GCP) and Container Logs (Docker, Kubernetes)..................... 23\n6.\nIoT/SCADA/OT Logs .................................................................................................... 28\n3. Key Monitoring Practice....................................................... 33\n1.\nDetecting Anomalies and Incidents (Alerts, Correlation).............................................. 33\n2.\nLog Retention and Security......................................................................................... 34\n3.\nSupporting Tools (SIEM, SOAR) ................................................................................... 35\nPage 3|36\n1.Introduction\nLogs are the footprints of every digital activity, serving as a chronological record of events within \nsystems, networks, and applications. In a modern Security Operations Center (SOC), analysts \nrely on these logs to detect threats, investigate security incidents, and maintain an \norganization’s overall security posture. Without structured and well-monitored logs, even the \nmost advanced security solutions can miss critical indicators of compromise (IoCs) or fail to \ncorrelate suspicious behaviors across multiple systems. The goal of this guide is to highlight \nwhich logs matter most, why they are essential, and how to approach their monitoring in a way \nthat benefits both junior and mid-level SOC analysts.\n1.\nImportance of Log Monitoring in SOC\nIn any sizable IT infrastructure, raw data volumes can be massive—firewalls alone can generate\nthousands of log entries per second. While these logs may sometimes appear as unremarkable\nlines of text, they hold valuable insights that help detect and counteract security threats. Proper\nlog monitoring is crucial for several reasons:\n1. Visibility andContext\nLogs provide context by showing what happened, when it happened, and how it was \nexecuted. This visibility is essential in distinguishing normal behaviors from anomalies. \nFor example, an unexpected privilege escalation in a Windows system log can point to \nlateral movement by an attacker. Similarly, repeated authentication failures in a Linux \nenvironment might indicate a brute-force attack.\n2. Incident Detection and Response\nAutomated alerts from a SIEM (Security Information and Event Management) platform \noften originate from suspicious patterns in logs. These alerts enable SOC analysts to \nquickly identify and respond to potential incidents. For instance, correlation rules might \nflag a user logging in from two geographically distant locations within a short timeframe, \nsuggesting a stolen credential.\n3. Audit andCompliance\nMany regulatory frameworks, such as PCI DSS, HIPAA, or ISO 27001, mandate log \nretention and regular review. By monitoring logs, organizations ensure they meet \ncompliance requirements and can produce a clear audit trail during investigations or\naudits. Logs are often the first place auditors check to confirm security controls are in \nplace and functioning as intended.\n4. Threat Hunting\nBeyond detection, logs form the basis for proactive threat hunting. Analysts look for \nunusual patterns—like the execution of a PowerShell script in an environment where \nPowerShell usage is rare—to uncover stealthy attacks. By analyzing logs over time, \nthreat hunters can identify trends and adversary tactics that might be missed by \nautomated systems alone.\n5. Forensic Investigations\nWhen an incident does occur, well-structured logs are the key to forensic investigations. \nThey help recreate the timeline of an attack, show which systems were accessed, and \nhighlight the data that was exfiltrated. Detailed logs of user actions, network\nPage 4|36\nconnections, and system calls can be the difference between accurately attributing an \nincident and letting attackers remain undetected.\nReal-World Example\nConsider a scenario where a SOC analyst notices unusual outbound traffic from a critical \nserver. By reviewing firewall logs correlated with Windows Event Logs, the analyst uncovers a \nmalicious process communicating with an external IP address. Ǫuick analysis shows that the \ncommunication began right after a suspicious privilege escalation event. This correlation can \nguide incident response teams to isolate the server, contain the threat, and remediate the \nvulnerability before data is compromised.\nPractical Tip\nOn Linux systems, commands like journalctl -p warning -r can help you quickly locate higher-\npriority events in reverse chronological order, allowing faster triage of potential security issues. \nOn Windows, tools such as wevtutil qe Security /rd:true /f:text /q:\"*\" |findstr /i \"4624 4625 4634 \n4672\" can filter the security event log for specific Event IDs related to logons.\n1.2.Scope and Purpose of the Guide\nThis guide targets both new and mid-level SOC analysts who want to enhance their skills in log \nmonitoring. It covers common sources of logs—like operating systems, networks, applications, \nand security tools—and highlights what to look for in each. By focusing on the most critical logs \nand describing how they fit into the overall security strategy, this guide aims to streamline the \nday-to-day work of SOC professionals. Specifically, it aims to:\n•\nIdentifyKey LogSources\nWe will look at systemlogs (Windows, Linux, macOS), networklogs (firewall, router, \nIDS/IPS), application anddatabase logs, security logs (AV,EDR, XDR), cloud logs \n(AWS, Azure, GCP), container logs (Docker, Kubernetes), and IoT/SCADA/OTlogs. The\nprimary focus is on what makes each category critical, how to collect them, and which \nevents are most indicative of a security issue.\n•\nPresent Practical Monitoring Techniques\nFrom correlation rules to anomaly-based detection, we will discuss the practices that \ntranslate raw logs into actionable insights. We will also cover topics like logretention, \nlogsecurity, and best practices around data classification to ensure that sensitive logs \nremain protected.\n•\nShowcase Real-Life Use Cases\nEach log type comes with its unique set of challenges and attack vectors. We will walk\nthrough realistic scenarios—such as detecting lateral movement, privilege escalation,\nor malicious uploads—and demonstrate how the logs serve as vital evidence.\n•\nGuide on SupportingTools\nSIEM (Security Information and Event Management) and SOAR (Security Orchestration, \nAutomation, and Response) tools are at the heart of modern SOCs. The guide explains \nhow these platforms integrate with different log sources, automate alerting, and help \norchestrate response actions.\nPage 5|36\n•\nEncourage Continuous Learning\nCyber threats evolve rapidly, and so do best practices in logging and monitoring. With \nreferences to resources like NIST SP 800-92 (Guide to Computer Security Log \nManagement) and official vendor documentation (e.g., Microsoft’s Windows Event Log\ndocumentation), this guide points readers to reliable sources for ongoing education.\nBy focusing on these areas, the guide aims to equip analysts with the knowledge and skills to \nprioritize logs effectively and detect potential breaches before they escalate. Through a mix of \ntheoretical explanation and practical examples, readers will gain confidence in setting up \nlogging strategies, tuning alerts, and conducting thorough investigations.\nPage 6|36\n2.KeyTypes ofLogs\n1.\nSystem Logs (Windows, Linux, macOS\nSystem logs form the backbone of incident detection and response efforts, providing analysts \nwith the essential baseline data needed to investigate abnormal events, track user activities, \nand diagnose security threats. Across Windows, Linux, and macOS, these logs share the \ncommon goal of recording key operating system (OS) events, though each platform organizes \nand structures logs in its own way.Understanding how they work, what they log, and how to \ninterpret them is crucial for SOC analysts.\nWindows Logs\nCommon Log Sources\n•\nSystem Log:Captures events generated by the Windows operating system and its built-\nin services. It records driver issues, service startups and shutdowns, and kernel-level \nmessages.\n•\nApplication Log: Stores application-specific events, such as errors, warnings, or \ninformational messages from software installed on the system (e.g., database clients, \nproductivity tools).\n•\nSecurity Log: Focuses on security-related events: login attempts, account lockouts, \nand user right assignments. Often used for auditing and forensic investigations.\n•\nOther Logs: Windows also creates dedicated logs for specialized services, like DFS \nReplication and PowerShell, which can be viewed under the Applications and \nServices Logsin the Event Viewer.\nPractical Monitoring Tips\n1. Event Viewer:Built into Windows, Event Viewer offers a quick way to view and filter \nevents. Analysts can group events by severity (Critical, Error, Warning, Information) or by \nEvent ID.\n2. Filtering and Searching: Use XMLfiltering in Event Viewer or PowerShell commands to \nhunt for specific event IDs (e.g., 4624 for successful logins, 4625 for failed logins).\n3. Security Baselines: Monitor high-value Event IDs. For example:\no\n4624 (Successful account login)\no\n4625(Failed login)\no\n4672(Special privileges assigned to a user)\no\n4688(Anew process has been created)\no\n4648(Alogon was attempted using explicit credentials)\n4. PowerShell Logging: By enabling Module Loggingand Script Block Logging, analysts \ncan track suspicious or obfuscated commands. Refer to Microsoft Docs (PowerShell\nLogging) for guidelines.\nPage 7|36\nExample: Filtering Security Events viaPowerShell\nGet-WinEvent -LogName Security | Where-Object {\n$_.Id -in 4624, 4625\n}\nThis command pulls Security Log events for successful and failed logins, enabling quick \ndetection of abnormal activity.\nLinux Logs\nSyslog andJournald\nMost Linux distributions rely on syslog or systemd-journald to collect and manage log \nmessages:\n•\n/var/log/syslog or /var/log/messages: Contains informational and non-critical system \nevents.\n•\n/var/log/auth.log or /var/log/secure: Focuses on authentication-related messages. \nEssential for detecting brute-force login attempts, sudo activity, or SSH logins.\n•\n/var/log/kern.log: Stores kernel-level messages, useful for diagnosing driver issues or \nunusual kernel events.\n•\nJournal Logs (systemd-based distros): Consolidates logs in a binary format, accessible \nvia journalctl.\nKey AreastoMonitor\n1. Authentication: Watch for repeated failed login attempts, new user additions in\n/etc/passwd, or sudden changes in sudo usage.\n2. Cron Jobs: Check /var/log/cronor associated logs for unauthorized scheduled tasks. \nCron jobs can be used by adversaries for persistence.\n3. Kernel Messages: Investigate repeated kernel warnings or errors that could indicate \nhardware issues or potential rootkit activity.\n4. Service Logs: For services like Apache, Nginx, or SSH, monitor dedicated logs (e.g.,\n/var/log/apache2/access.log, /var/log/nginx/access.log, /var/log/secure) for unexpected \ntraffic or repeated authentication failures.\nExample: UsingJournalctl\n# View all logs related to SSH \njournalctl -u sshd\n# Filter logs for a specific time range\njournalctl --since \"2023-01-01\" --until \"2023-01-31\"\nThis approach helps analysts quickly search for anomalies within a particular service or \ntimeframe.\nPage 8|36\nmacOS Logs\nUnified LoggingSystem\nSince macOS Sierra (10.12), Apple introduced a unified logging system that stores log messages \nin a structured format:\n•\nConsole App: The built-in Console allows viewing of system logs, diagnostic reports, \nand crash logs.\n•\nLogCommands: The log utility in the terminal offers extensive filtering, streaming, and \nsearching capabilities. For example:\n#View live log messages (system-wide)\nlog stream --level=info\n#Search for specific processes or errors\nlog show --predicate 'process == \"sshd\" AND eventMessage \nCONTAINS \"Failed password\"'\n•\nSubsystems and Categories: macOS logs categorize messages by subsystem (e.g., \ncom.apple.networking) and category (e.g., connection). This helps analysts narrow \ndown events.\nSecurity-Specific Logs\n•\n/var/log/system.log:Retains many core system messages and is often the first stop \nwhen troubleshooting.\n•\nApple System Log (ASL):Legacy logging that coexists with the unified logging system, \naccessible via command-line tools for older macOS versions.\n•\nAuthentication Logs: Attempts to log in via SSH or local accounts can appear in\n/var/log/asl/ or through the unified logging interface.\nMonitoring andDetection\n1. Focus onRepeatedFailures: Like Linux, repeated failed SSH attempts or unexpected \nprocess launches warrant attention.\n2. Check Crash Reports: Attackers sometimes induce crashes of security tooling. Crash \nlogs in macOS can provide early indicators of tampering.\n3. Leverage Built-in Tools: Use the Console to filter logs by Process or Message Type.\nApple’s Developer documentation on Unified Logging provides details on advanced\nusage.\nCross-Platform Considerations\nAspect\nWindows\nLinux\nmacOS\nLog Files\nEvent Viewer (System, \nSecurity, etc.)\n/var/log/syslog,\n/var/log/auth.log, etc.\nUnified Logging System \n(log show, log stream)\nPage 9|36\nAspect\nWindows\nLinux\nmacOS\nCommon \nTools\nPowerShell, Event \nViewer, WMI\ntail, grep, awk, journalctl\nConsole App, log CLI\nAlert Focus\nEvent IDs (4624, 4625,\netc.), policy changes\nSSH failures, privilege\nescalations, systemd\nservice errors\nSSH failures, system\ncrashes, unexpected\nsubsystem messages\nCentralization\nWindows Event \nForwarding (WEF), \nSysmon logs to SIEM\nRsyslog, Syslog-ng, \nsystemd-journald to SIEM\nExport logs via the log \ncollect feature or \nstreaming to a SIEM\nLogging Agents and Centralization\nMany organizations opt to forward Windows, Linux, and macOS logs to a central SIEM or log \nmanagement platform:\n•\nWindows: Windows Event Forwarding (WEF), Sysmon for detailed process-level logging, \nor third-party agents like NXLog or Splunk Universal Forwarder.\n•\nLinux: Rsyslog, Syslog-ng, or systemd-journald can forward logs to remote servers. \nBeats (Filebeat, Metricbeat) from Elastic can also collect and ship logs.\n•\nmacOS: Use third-party agents (e.g., Osquery for query-based logging, or Splunk, \nDatadog agents) to unify logs under a single pane.\n2.2. Network Logs (Firewall, Router,IDS/IPS)\nLogs from network devices and security systems are among the most critical data sources in a \nSecurity Operations Center (SOC). By analyzing firewall, router, and IDS/IPS logs, SOC analysts \ngain visibility into traffic patterns, security events, and potential anomalies. This visibility is \ncrucial for identifying malicious behavior early and for responding to incidents before they can \npropagate within the environment. Below are the key concepts, best practices, and real-world \nscenarios that illustrate how to work effectively with network logs.\n1. Firewall Logs\nFirewalls are often the first line of defense, filtering traffic based on predefined rules. Monitoring \nfirewall logs provides insights into both permitted and denied network connections.\n1.\nCommon Fields in Firewall Logs\nTypical firewall logs will include fields such as:\n•\nTimestamp:The date and time the event was recorded.\n•\nSource IP/Destination IP:IPaddresses of the client and server.\n•\nSource Port/Destination Port:Ports used by the services or applications \ncommunicating.\n•\nProtocol: Network protocol in use (e.g., TCP, UDP, ICMP).\n•\nAction: Indicates whether the traffic was allowed, denied, dropped, or rejected.\nP a g e 10 |36\n•\nRule or Policy Name: Identifies which firewall rule triggered the log entry.\nFirewalls may also log additional details like interface names (e.g., eth0, WAN, LAN), packet\nsize, or the reason for a deny or reject action. Modern firewalls, especially Next-Generation \nFirewalls (NGFWs), can log application-level data and user information if integrated with identity \nmanagement systems.\nField\nDescription\nExample Value\nTimestamp\nDate and time of event\n2025-01-25 10:15:32\nSource IP\nOriginating IP address\n192.168.10.5\nDestination IP\nTarget IPaddress\n10.0.5.20\nSource Port\nOriginating TCP/UDP port\n53452\nDestination Port Target TCP/UDP port\n443\nProtocol\nNetwork protocol (TCP, UDP, ICMP) TCP\nAction\nAllowed, denied, dropped, etc.\nAllowed\nRule Name\nFirewall policy or rule name\nBlock_Telnet\n2.\nPractical UseCases\n•\nBlocked Connection Attempts: Monitoring repeated connection attempts on sensitive \nports (e.g., 22 for SSH or 3389 for RDP) can reveal brute-force attempts or port scans.\n•\nUnusual Traffic Volumes: A sudden spike in traffic from a single IPor subnet might \nindicate a DoS or DDoS attempt.\n•\nInboundvs. Outbound Monitoring: Outbound connections to suspicious IP addresses \nor countries where the organization does not conduct business can be early indicators \nof compromised hosts (e.g., malware calling home).\n3.\nExample Firewall LogAnalysis in SIEM\nBelow is an example Splunk query that filters for denied connections with a focus on TCP port \n3389 (RDP):\nindex=firewall_logs action=DENY dest_port=3389\n| stats count by src_ip, dest_ip, action, rule_name\nThis query helps highlight any source IPs that are repeatedly trying to reach RDP services but are \nbeing denied, which might indicate an attempted intrusion.\n2. Router Logs\nRouters primarily forward packets between networks and maintain routing tables. Logs \ngenerated by routers often focus on system messages, routing updates, and interface errors \nrather than application-specific data. However, they are still critical for overall visibility, \nespecially in environments with distributed architectures.\nPage 11|36\n1.\nTypes of Router Logs\n•\nSystemor Event Logs: Includes messages about device reboots, software crashes, or \nconfiguration changes.\n•\nRoutingProtocol Logs: Information related to BGP, OSPF, EIGRP, or other routing \nprotocols.\n•\nInterface Logs: Status changes on interfaces (up/down), packet errors (CRC errors, \ncollisions), and bandwidth usage.\n•\nAuthentication Logs: Successful or failed logins via SSH, Telnet, or console access to \nthe router.\nRouter logs often follow the standard Syslog format (e.g., Cisco routers with severity levels 0–7). \nIntegratingthese logs into a SIEM allows analysts to correlate network topology changes with \nsecurity events (for instance, if a router interface goes down just before a security incident on \nthat segment).\n2.\nExample: Cisco Router Syslog Messages\nCisco routers send Syslog messages with various severity levels. An example message might \nlook like this:\n<189>Jan 25 10:25:10 MY-ROUTER: %LINK-3-UPDOWN: Interface\nGigabitEthernet0/1, changed state to up\n•\n189 corresponds to the Syslog priority.\n•\n%LINK-3-UPDOWN indicates a link status change with severity level 3 (Error).\n•\nThe message indicates which interface changed state.\nIn a SIEM, you might filter for %LINK-3-UPDOWN events to track unexpected interface state \nchanges. If an interface goes down suddenly, it may indicate a physical issue, misconfiguration, \nor malicious activity aiming to disrupt network segments.\n3. IDS/IPS Logs\nIntrusion Detection Systems (IDS) and Intrusion Prevention Systems (IPS)monitor network \ntraffic for signs of malicious behavior, policy violations, or known attack signatures. While \nfirewalls typically operate at the transport or network layer, IDS/IPS solutions can inspect \npackets in more depth (Layer 7), providing richer context about application-level threats.\n1.\nIDSvs.IPS\n•\nIDS(IntrusionDetection System): Detects potential threats and generates alerts. It \ndoes not automatically block the traffic.\n•\nIPS(IntrusionPreventionSystem): Detects threats and can take preventive actions, \nsuch as dropping malicious packets or blocking IP addresses in real-time.\n2.\nCommon Fields in IDS/IPSLogs\n•\nSignature ID:A unique identifier for the rule or signature triggered (e.g., Snort rules have \nSID values).\nPage 12|36\n•\nEvent or Alert Message: The name or description of the suspicious activity (e.g., “ET \nTROJAN Zeus Tracker”).\n•\nSeverity or Priority: Indicates the criticality of the alert.\n•\nSource IP/Destination IP/Ports: Information about the traffic flow.\n•\nAction: Whether the traffic was detected, dropped, or allowed.\n3.3. Practical Example with Suricata\nSuricata is a popular open-source IDS/IPS engine. Suricata outputs JSON logs that can be \ningested by SIEM tools like Elasticsearch or Splunk. A typical Suricata alert entry in JSON format \nmight include:\n{\n\"timestamp\": \"2025-01-25T10:30:45.123456+0000\",\n\"flow_id\": 1234567890, \n\"event_type\": \"alert\", \n\"src_ip\": \"192.168.1.100\",\n\"src_port\": 53452,\n\"dest_ip\": \"10.0.5.20\",\n\"dest_port\": 80, \n\"proto\": \"TCP\", \n\"alert\": {\n\"action\": \"blocked\", \n\"gid\": 1,\n\"signature_id\": 2010935,\n\"rev\": 3,\n\"signature\": \"ET TROJAN Known Malicious Domain\", \n\"category\": \"Trojan Activity\",\n\"severity\": 2\n}\n}\nIn the above example:\n•\nsignature_id: 2010935 corresponds to a Suricata rule IDthat references a specific \nTrojan signature.\n•\naction: The traffic was blocked by Suricata (IPS mode).\n•\ncategory:“Trojan Activity” indicates the type of threat.\nPage 13|36\n3.4. Real-Life Attack Scenarios\n•\nSǪL InjectionAttempts: IDS/IPS solutions look for patterns in HTTP requests that \nmatch known SǪL injection techniques.\n•\nExploitKits: If a host attempts to download or connect to an exploit kit domain, IDS/IPS \nlogs can reveal the suspicious domain name and signature match.\n•\nLateral Movement: Attackers may try to move horizontally within a network. IDS/IPS \ncan detect unusual SMB or RDP traffic patterns.\n4. Parsing and Analyzing Network Logs in Practice\n1.\nLogManagement and SIEMIntegration\nSOC analysts typically centralize firewall, router, and IDS/IPS logs into a SIEM for correlation and \nanalysis. This allows cross-referencing of events from multiple sources. For instance, if an IDS \nalert indicates a Trojan signature and the firewall logs show outbound traffic to a suspicious IP, \nthe SIEM can generate a higher-priority alert.\nExample Logstash configuration snippet to parse Suricata JSON logs:\ninput { \nfile {\npath => \"/var/log/suricata/eve.json\" \ntype => \"suricata\"\ncodec => \"json\"\n}\n}\nfilter {\nif [event_type] == \"alert\" { \nmutate {\nadd_tag => [\"suricata_alert\"]\n}\n}\n}\noutput { \nelasticsearch {\nhosts => [\"localhost:9200\"]\nindex => \"suricata-alerts-%{+YYYY.MM.dd}\"\n}\n}\nPage 14|36\nThis configuration reads Suricata’s eve.json file, filters for alert events, and then tags them as \nsuricata_alert before sending them to Elasticsearch.\n4.2. Correlation Rules\nIn a SIEM, correlation rules can look for conditions such as:\n1. HighVolume of Denied Connections: If more than 100 firewall denies occur from the \nsame source IPin 5 minutes, generate an alert.\n2. Multiple IDSAlerts fortheSameHost: If a host triggers more than 3 different IDS \nsignatures within a short period, raise the priority of the incident.\n3. Router Interface Down +IDS Alerts: If a critical interface goes down and multiple IDS \nalerts are detected on adjacent network segments, investigate potential sabotage or \nwidespread compromise.\nBy creating correlation rules that combine different log types, SOC analysts can detect \ncoordinated attacks and reduce the volume of false positives.\n5. Challenges and Best Practices\n•\nLogVolume: Network devices can generate a massive amount of data. Using filters or \nsampling may be necessary, but be cautious not to discard important information.\n•\nNormalization: Different vendors (Cisco, Palo Alto, Fortinet, etc.) often have unique log \nformats. Normalizing fields (e.g., ensuring consistent naming of src_ip, dest_ip) is \ncrucial for effective correlation.\n•\nEncryption and Secure Transport: Ensure that log data is transmitted securely, for \ninstance using TLS for Syslog (Syslog over TLS). Unencrypted logs can be intercepted \nand manipulated byadversaries.\n•\nRegular Tuning:IDS/IPS rulesets need regular updates to reflect new threats. Similarly, \nfirewall policies should be reviewed to ensure they align with the evolving network \nenvironment.\n•\nTime Synchronization: NTP (Network Time Protocol) should be enabled and correctly \nconfigured on all devices to maintain consistent timestamps. Accurate timestamps are \ncritical for event correlation.\n6. References and Further Reading\n•\nOfficial Suricata Documentation: https://suricata-ids.org/docs/\n•\nSnort (IDS/IPS)Documentation: https://www.snort.org/\n•\nCisco Syslog Guide: https://www.cisco.com/c/en/us/support/docs/security-\nvpn/syslog/\n•\nFirewallBest Practices (NIST): \nhttps://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-41r1.pdf\nThese sources provide authoritative insights into configuration, logging standards, and threat \ndetection patterns for network devices and security solutions.\nPage 15|36\n2.3.Application and Database Logs\nApplication logs capture events and behaviors tied directly to an application’s functionality.\nThey may include user interactions, system operations, exceptions, debug details, and custom \nevents defined by developers. Database logs, on the other hand, record all actions related to \ndata transactions, schema changes, authentication, and potential errors or performance \nbottlenecks in a database system. Together, these logs provide a holistic view of how software is \nfunctioning and how data is being accessed or manipulated. This is crucial for detecting \nunauthorized activities, performance issues, and other anomalies. Below are the key \nconsiderations, best practices, and real-world examples to help you effectively monitor and \nanalyze both application and database logs.\nUnderstanding Application Logs\nCommon Types of Application Logs\n1. ErrorandExceptionLogs\nThese capture unexpected behaviors and stack traces. They’re typically generated by \nframeworks such as Java’s Log4j or Logback, Python’s logging library, or .NET’s built-in \nlogging features.\n2. Debug and Diagnostic Logs\nThese logs detail internal operations, often containing fine-grained information used \nduring development or troubleshooting. Debug logs can be extremely verbose, so \nthey’re typically enabled only in test or development environments unless a production \nissue requires deeper insights.\n3. Transactionor Event Logs\nApplications that handle user transactions—such as e-commerce checkouts—often \ngenerate transaction logs. These logs detail each step in the user flow (e.g., adding items \nto a cart, checking out, payment processing).\n4. Audit Logs\nSome applications produce audit logs for compliance or security reasons. These logs \ntrack user access, role changes, or critical configuration updates, helping you see who \ndid what and when.\nLogging Frameworks and Formats\nModern applications often rely on standardized logging frameworks:\n•\nJava (Log4j,Logback)\n•\n.NET (Serilog, NLog)\n•\nPython(logging)\n•\nNode.js (winston, pino)\nThese frameworks allow developers to configure log levels, structure log messages (for \nexample, in JSON), and specify output targets like console, files, or external aggregation \nservices. Consistency in format is important for parsing and correlation within a SIEM.\nPage 16|36\nExample Configuration (Log4j2 in Java)\n<Configuration status=\"warn\">\n<Appenders>\n<File name=\"FileLogger\" fileName=\"logs/application.log\">\n<PatternLayout pattern=\"%d{ISO8601} [%t] %-5level %logger{36}\n- %msg%n\" />\n</File>\n</Appenders>\n<Loggers>\n<Logger name=\"com.example.app\" level=\"info\" additivity=\"false\">\n<AppenderRef ref=\"FileLogger\"/>\n</Logger>\n<Root level=\"error\">\n<AppenderRef ref=\"FileLogger\"/>\n</Root>\n</Loggers>\n</Configuration>\nThis snippet specifies an application.log file, using a pattern to log timestamps, thread names, \nlog levels, and the actual message. By setting the root level to error and the application logger to \ninfo, you can avoid unnecessary noise.\nMonitoring Application Logs in Practice\n1. Establish Baseline Behavior\nKnowing the normal application behavior (e.g., average response times, typical error \nrates) helps detect anomalies, such as a sudden influx of specific exceptions that might \nindicate an attack or misconfiguration.\n2. Look forCommon Attack Patterns\nUnauthorized access attempts often show up as repeated login failures, suspicious \nparameters in URLs (e.g., SǪL injection probes), or unusual behaviors in session \nmanagement. Web applications might log 404 errors or suspicious HTTP methods in \nhigher frequency under attack.\n3. Integrationwith SIEM\nTocorrelate application logs with system or network events, use a SIEM tool like Splunk, \nIBMǪRadar, or ElasticSecurity. For example, by correlating an application’s “multiple \nfailed logins” event with a firewall log showing suspicious IP scanning, you can quickly \nconfirm or rule out an intrusion attempt.\n4. Alerting and Thresholds\nThreshold-based alerts on error rates, transaction volume drops, or spikes in exceptions\nPage 17|36\ncan catch incidents early. Machine learning-driven anomaly detection in tools such as \nAzure Sentinel or AWSSecurity Hub can further refine alerts by identifying patterns not \ncaptured by static rules.\n5.\nRetentionPolicies\nDue to volume, application logs can grow quickly. You need to define retention policies\nbalancing security requirements and storage costs. Compliance frameworks (e.g., PCI\nDSS, HIPAA) sometimes dictate minimum retention periods for specific log types.\nUnderstanding Database Logs\nKey Types of DatabaseLogs\n1. Transaction Logs\nCapture every change to the database’s data. They’re critical for recovery and forensic \nanalysis. For instance, in Microsoft SǪL Server, the transaction log tracks every \nmodification in the order they occur, enabling point-in-time recovery.\n2. ErrorLogs\nThese highlight critical events, such as server startup issues or serious errors that affect\ndatabase availability. Examples include MySǪL’s error.log or Oracle’s alert logs.\n3. General Ǫuery Logs(MySǪL)/Audit Logs(VariousVendors)\nRecord all queries received by the server or track account activity. They are invaluable in \ndetecting SǪL injection attacks, suspicious data extraction, or attempts at privilege \nescalation.\n4. SlowǪuery Logs\nFound in MySǪL or PostgreSǪL, these capture queries that exceed a certain execution\ntime threshold. Slow queries might indicate performance issues or potential denial-of-\nservice attempts if queries are being manipulated byan attacker.\nPractical Monitoring Strategies\n1. Regular ReviewforSuspicious Ǫueries\nMonitor queries that drop or alter critical tables without expected change-control \ntickets. Also look for wildcard searches or large data extracts happening at unusual \nhours.\n2. Privilege Abuse Detection\nIf a user with minimal privileges starts running queries typical of an administrator, it’s a \nstrong indicator of compromised credentials or privilege escalation. Enforcing least \nprivilege and then reviewing logs for anomalies is a potent strategy.\n3. ErrorPattern Analysis\nRepeated database error messages, such as “invalid column name” or “syntax error,”\ncan indicate attempts at SǪL injection. SOC analysts can configure SIEM correlation\nrules to flag repetitive errors from a single source IP.\n4. Performance Data\nLogs that point to high resource usage or timeouts can be an early warning of a brute-\nforce or denial-of-service attack at the database layer.\nPage 18|36\nExample Configurations and Ǫueries\nMySǪL\nToenable the general query log:\nSET GLOBAL general_log = 'ON';\nSET GLOBAL general_log_file = '/var/log/mysql/general.log'; \nTo enable the slow query log:\nSET GLOBAL slow_query_log = 'ON';\nSET GLOBAL long_query_time = 2; -- Queries taking longer than 2 \nseconds will be logged\nNote: Logging all queries can significantly impact performance, so only enable it temporarily for \ndiagnostics or funnel logs to a centralized system where you can parse and analyze them \nefficiently.\nPostgreSǪL\nPostgreSǪL has extensive logging configurations in postgresql.conf. For instance:\nlogging_collector = on \nlog_directory = 'pg_log' \nlog_filename = 'postgresql-%a.log' \nlog_statement = 'all'\nlog_min_duration_statement = 2000\n# logs queries over 2ms\nBy setting log_statement to all, you can see every statement, though this is typically too verbose \nfor production.\nReal-World Scenarios\n•\nDetection of Data Exfiltration\nA SOC analyst notices an application log showing unusual parameter values in a REST \nAPI call. By cross-referencing the database logs, the analyst confirms multiple large \nSELECT statements retrieving sensitive customer data. Additional correlation with\nnetwork logs shows a large data transfer to an external IP.The logs collectively point to \nan ongoing data exfiltration attempt.\n•\nAuditingfor Compliance\nIn a financial services application, compliance requirements mandate auditing every \ntransaction. By reviewing application logs (which capture the logic layer) and the \ndatabase’s transaction logs (which capture the final record changes), auditors can \nconfirm that every deposit or withdrawal is authorized and properly executed.\n•\nIdentifying Performance Attacks\nA series of slow queries might initially look like a performance bottleneck. However, \ninvestigating further reveals that attackers are intentionally crafting resource-intensive \nqueries to degrade the application’s responsiveness. Alerts in the SIEM correlate these \nslow queries with repeated 503 errors on the web server, confirming a denial-of-service \nattempt.\nPage 19|36\nAdditional Resources\n•\nOWASPCheat Sheet Series: https://cheatsheetseries.owasp.org/\nOffers guidelines on secure logging practices, specifically around sanitizing logs and \npreventing log forging.\n•\nMySǪL Official Docs: https://dev.mysql.com/doc/\nDetailed instructions on configuring error logs, general query logs, and slow query logs.\n•\nPostgreSǪL Documentation: https://www.postgresql.org/docs/\nContains comprehensive configuration guides for logging and auditing features.\n•\nMicrosoft SǪL Server Docs: https://docs.microsoft.com/en-us/sql/\nExplains how to manage and interpret transaction logs, error logs, and other diagnostic \ndata.\n•\nOracle Database Docs: https://docs.oracle.com/en/database/\nProvides details on the alert log, trace files, and advanced auditing configurations.\nComparisons and Data\nLogType\nExamples\nTypical Use Case\nPotential Security Insight\nError/Exception Stack traces, code \nline references\nDebugging application \ncrashes, identifying \nfaulty modules\nFrequent exceptions might \nhint at malicious input or \nattempts to exploit \nvulnerabilities\nTransaction\nE-commerce logs, \nbanking transactions\nAuditing success/failure \nof critical actions\nReal-time monitoring helps \ndetect fraudulent \ntransactions\nAudit (App C \nDB)\nUser actions, role\nchanges, schema\nmodifications\nCompliance with \nregulations, \naccountability\nPinpointing unauthorized\nadmin actions or privilege\nescalations\nSlow Ǫuery\nǪueries exceeding a \ntime threshold\nPerformance tuning or \nbottleneck analysis\nIdentifying possible DoS \nattempts or resource \nexhaustion attacks\nWhen collecting and analyzing these logs, consider normalizing fields (timestamps, user IDs, \nhostnames) so different log sources can be correlated effectively. Some SIEM platforms or \ncentralized logging solutions (e.g., the ELK Stack) allow youto define common field mappings \nand dashboards that unify application and database insights.\n2.4.Security Logs (AV,EDR, XDR)\nSecurity logs generated by Antivirus (AV),Endpoint Detection and Response (EDR), and \nExtended Detection and Response (XDR) solutions are crucial for modern SOC operations. They \noffer granular visibility into potential threats affecting endpoints and the broader environment. \nBelow is an exploration of the fundamentals, along with real-world examples and guidance for \neffective log monitoring.\nP a g e 20 |36\nUnderstanding the Components\nAntivirus (AV)Logs\nAntivirus solutions focus primarily on detecting known malware signatures and blocking \nsuspicious files. Their logs typically include:\n•\nMalware Detections: Alerts triggered when a file matches a known signature or exhibits \nmalicious behavior.\n•\nǪuarantineand Remediation Actions: Logs showing which files were quarantined, \ndeleted, or otherwise neutralized.\n•\nUpdate and Scan Events: Records of signature updates, scheduled scans, and on-\ndemand scan results.\nReal-World Example\nA traditional AV tool like Microsoft Defender Antivirus (part of Windows Security) generates \nlogs under the Windows Event Log:\n•\nEvent ID1116indicates malware detection.\n•\nEvent ID5001logs the scanning engine starting up.\nBy aggregating these event IDs in a SIEM, analysts can quickly see patterns of infection \nattempts and confirm that updates have been applied.\nEndpoint Detection and Response (EDR) Logs\nEDR solutions expand on basic antivirus features by providing in-depth endpoint telemetry, real-\ntime threat detection, and response capabilities. Common log data includes:\n•\nProcess Creationand Termination: Detailed tracking of command-line parameters, \nuser context, and file paths.\n•\nBehavioral Indicators: Observations related to suspicious activities such as code \ninjection, privilege escalations, or unusual registry modifications.\n•\nIsolation and Response Actions: Logs showing when and why an endpoint was\nisolated, network connections were blocked, or an automated script was run for\ncontainment.\nEDR logs often present a sequence of correlated events, making it easier for SOC analysts to \nreconstruct the timeline of an attack. Tools like CrowdStrike Falcon, SentinelOne, or Carbon \nBlackoffer dashboards that display triggered detection rules (e.g., MITRE ATTCCK techniques) \nalongside automated remediation actions.\nSample EDR LogAnalysis(Splunk)\nBelow is an example of how you might parse EDR logs in Splunk to identify suspicious child \nprocesses of PowerShell:\nindex=edr_logs parent_process=PowerShell.exe\n| stats count by child_process, user, host\n| where count > 3\nPage 21|36\nThis query looks for any child process spawned by PowerShell and flags any repeated \noccurrences, which might indicate malicious scripts or living-off-the-land techniques.\nExtended Detection and Response (XDR)Logs\nXDR solutions take the endpoint-centric approach of EDR and extend it to incorporate data from \nnetwork appliances, cloud workloads, and applications. The goal is to unify detection, \ninvestigation, and response across multiple layers of the ITenvironment.\n•\nCross-Source Correlation: XDR aggregates logs from endpoints, email gateways, \nidentity providers, and more, applying analytics to uncover hidden threats.\n•\nCloud andHybrid Integrations:Telemetry from cloud platforms and containerized \nworkloads often merges with endpoint data, offering a complete view of complex \nattacks.\n•\nAdaptive Response: Based on machine learning and correlation rules, XDR can trigger\nautomated playbooks that respond to threats in real time (e.g., disabling compromised\nuser accounts, isolating infected hosts, or blocking suspicious domains at the firewall).\nReference Architectures\n•\nMicrosoft 365 Defender integrates data from endpoints (Defender for Endpoint), email\n(Defender for Office 365), identities (Azure Active Directory), and cloud apps (Defender\nfor Cloud Apps).\n•\nPaloAlto Cortex XDRprocesses data from endpoints and integrates with network \nsensors or firewalls to provide enhanced correlation.\nLog Collection Best Practices\n1. Centralize Logsin aSIEM:Consolidate all AV,EDR, and XDR logs into a SIEM platform \nlike Splunk, Elastic Stack, or IBMǪRadar. This ensures a single view for threat hunting \nand alert triage.\n2. Use Consistent LogFormatting: Where possible, standardize the format (e.g., JSON, \nSyslog) to streamline parsing, correlation, and long-term storage.\n3. Retain Sufficient History: Depending on regulatory requirements and threat modeling, \nkeep historical logs long enough to investigate slow-moving attacks or advanced \npersistent threats.\n4. Correlate Across Multiple Sources: Antivirus alerts alone may provide minimal \ncontext. When cross-referenced with endpoint telemetry and user login patterns, they \nreveal the bigger picture—especially relevant for advanced or multi-stage attacks.\n5. Implement AutomatedDetection Rules: Leverage built-in detection capabilities of \nyour EDR/XDR solution and supplement them with custom rules tailored to your \nenvironment. For example, create an alert when a known safe process spawns an \nunusual child process (e.g., outlook.exe launching cmd.exe).\n6. Leverage Threat Intelligence: Enrich detection events with threat intelligence feeds \n(e.g., VirusTotal, AlienVault OTX). This helps validate suspicious activity, especially when \nan alert references a known malicious domain or file hash.\nPage 22|36\nCommon Security Events to Watch For\nEvent Type\nKey Indicators\nExample Tools\nMalware \nDetections\nFile hashes, known signatures, suspicious \nfile behaviors\nMicrosoft Defender, \nMcAfee, Symantec\nBehavioral \nAnomalies\nUnusual registry modifications, abnormal \nprocess trees\nCrowdStrike Falcon, \nSentinelOne\nPrivilege \nEscalations\nAttempts to change user privilegeor run \nprocesses as admin\nSysmon +EDR correlation\nExfiltration \nAttempts\nNetwork connections to suspicious \ndomains or large data transfers\nPalo Alto Cortex XDR, \nSplunk logs\nPersistence \nMechanisms\nNew services, startup items, scheduled \ntasks\nSysmon +EDR detection \nrules\nMonitoring these events in near-real time allows SOC analysts to prioritize the highest-risk alerts \nand initiate containment actions quickly.\nPractical Monitoring Tips\n•\nTrackFailed and Successful RemediationAttempts: If an AV tries to quarantine a file \nrepeatedly but fails, it could be a sign of advanced malware or user tampering.\n•\nMonitor EDRAgent Health: Regularly ensure that EDR agents are running on all \nendpoints. Unexpected agent downtime may be an early indicator of an attacker’s \nattempt to disable security controls.\n•\nReview Automated Playbook Outcomes: XDR platforms often run automated \nresponses. Confirm that these responses are both effective and aligned with your \norganization’s incident response procedures.\n•\nEngage With Vendor Documentation: Each AV, EDR, or XDR vendor has specific best \npractices for log collection and interpretation. For instance, Microsoft Defender for \nEndpoint publishes detailed logging guidelines at Microsoft Docs.\nExample Incident Flow Using AV,EDR, and XDR\n1. AVAlert: Triggers on a suspicious executable with a known malicious hash.\n2. EDR Correlation: Maps the suspicious executable back to a process tree, showing it \nwas launched by an unusual script.\n3. XDRVisibility: Confirms the script was downloaded from an unrecognized domain and \nties this domain to a known threat actor via threat intelligence feeds.\n4. AutomatedResponse: XDR or a SOAR platform quarantines the endpoint, blocks the \ndomain at the firewall, and opens a ticket in the incident management system.\n5. SOC Analyst Action: Investigates the entire chain of events, verifies threat removal, and \nupdates detection rules to prevent similar attacks.\nPage 23|36\nBy combining the strengths of AV,EDR, and XDR logs in a well-structured monitoring strategy, \nSOC analysts can respond swiftly to a wide range of threats—from commodity malware to \nsophisticated, persistent attacks.\n2.5.Cloud Logs (AWS,Azure, GCP) and Container Logs (Docker, \nKubernetes)\nCloud platforms and container orchestration systems have become an essential part of many \norganizations. In a SOC environment, monitoring logs from these platforms is critical for threat \ndetection, compliance, and troubleshooting. Below is an overview of the most important log \nsources and practical considerations for AWS, Azure, GCP, Docker, and Kubernetes.\nAWS Logs\nCommon LogTypes\n1. CloudTrail Logs\no\nPurpose: Track API calls and account activity across AWS services.\no\nKey Fields: eventName, eventSource, awsRegion, sourceIPAddress, userAgent, \nrequestParameters, responseElements.\no\nUse in Security: Identifies suspicious or unauthorized actions, such as \nunexpected changes to IAM policies, creation or deletion of critical resources, or \nunusual console logins.\n2. CloudWatch Logs\no\nPurpose: Centralized logging for AWS services (EC2 system logs, Lambda \nfunction logs, etc.).\no\nKey Fields: Vary depending on service-specific events; typically include \ntimestamps, log level (ERROR, WARNING, INFO), and custom application \nmessages.\no\nUse in Security: Helps correlate system-level events with higher-level activities. \nExample: correlating an EC2 instance’s system error logs with an unauthorized \naccess attempt shown in CloudTrail.\n3. VPC FlowLogs\no\nPurpose: Capture network flow information (source/destination IP,ports, traffic \nacceptance/rejection).\no\nKey Fields: version, account-id, interface-id, srcaddr, dstaddr, srcport, dstport, \nprotocol, action, log-status.\no\nUse inSecurity: Identifies unusual traffic patterns or data exfiltration attempts, \nsuch as large outbound data transfers or traffic from unknown IP ranges.\nPractical Example\nA typical workflow for ingestion involves forwarding CloudTrail and VPC Flow Logs to an S3 \nbucket, then using Amazon Kinesis or a third-party tool (e.g., Logstash) to parse and send events \nto a SIEM. For example, with AWS CLI youcan enable CloudTrail logging:\nPage 24|36\naws cloudtrail create-trail \\\n--name MySecurityTrail \\\n--s3-bucket-name my-security-logs \\\n--include-global-service-events\nFor more details, see the AWS CloudTrail Documentation.\nAzure Logs\nCommon LogTypes\n1. AzureActivity Logs\no\nPurpose: Provide insights into management operations (e.g., resource creation, \nmodification, or deletion).\no\nKey Fields: authorization, caller, category, operationName, resourceId, status.\no\nUse inSecurity: Detect unauthorized resource creation, changes to security \ngroups, or attempts to elevate privileges.\n2. Azure Monitor Logs(LogAnalytics)\no\nPurpose: Collect logs from Azure resources, containers, VMs, and applications.\no\nKey Fields: Vary based on the resource type; commonly include timestamps, \noperation IDs, user details, and other contextual data.\no\nUse in Security: Offers extensive querying and correlation capabilities. SOC \nteams can detect anomalies by combining signals from multiple sources \n(Activity Logs, VM logs, etc.).\n3. Diagnostics Logs\no\nPurpose: Detailed insights from specific Azure services, such as Key Vault \naccess logs, Azure App Service logs, or Azure Storage logs.\no\nKey Fields: Depend on the service but often include request endpoints, \nauthentication details, and result codes.\no\nUse in Security: Detects potential credential misuse, suspicious activity in data \nstorage, or unusual application behavior.\nPractical Example\nSending logs to Azure Monitor can be done by configuring a Diagnostic Setting for each \nresource. For instance, to route Activity Logs to Azure Monitor and a storage account:\nSet-AzDiagnosticSetting -ResourceId\n/subscriptions/<SUBSCRIPTION_ID>/resourceGroups/<RESOURCE_GROUP>/pro \nviders/Microsoft.Web/sites/<APP_NAME> `\n-WorkspaceId <AZURE_MONITOR_WORKSPACE_ID> `\n-StorageAccountId\n/subscriptions/<SUBSCRIPTION_ID>/resourceGroups/<RESOURCE_GROUP>/pro \nviders/Microsoft.Storage/storageAccounts/<STORAGE_ACCOUNT_NAME>\n`\nPage 25|36\n-Enabled $true\nRefer to Azure Monitor Documentation for more details.\nGCP Logs\nCommon LogTypes\n1. Cloud Audit Logs\no\nPurpose: Record admin and data access events for GCP services (similar to \nAWS CloudTrail).\no\nKey Fields: protoPayload.serviceName, protoPayload.methodName, \nresourceName, authenticationInfo, requestMetadata.\no\nUse in Security: Surface privilege escalation attempts or suspicious \nmodifications to GCP resources (e.g., enabling/disabling critical services).\n2. VPC FlowLogs\no\nPurpose: Collect network flow information for Google Cloud VPCs.\no\nKey Fields: srcIP, destIP, srcPort, destPort, protocol, connectionEstablished, \nbytesSent, bytesReceived.\no\nUse in Security: Spot reconnaissance or exfiltration activity by analyzing \ninbound and outbound traffic patterns.\n3. Cloud Logging\no\nPurpose: Central logging service for events from GCP services, containers, \ncustom applications.\no\nKey Fields: Service-specific data, timestamps, severity levels, resource labels \n(e.g., k8s_container, gce_instance).\no\nUse inSecurity: Enables correlation of application-level logs with \ninfrastructure-level events.\nPractical Example\nToexport GCP logs to a SIEM, youcan create a sink that routes logs to a Pub/Sub topic, which a \ncustom or third-party collector can then forward. An example using the gcloud CLI:\ngcloud logging sinks create my-security-sink \\\nstorage.googleapis.com/<BUCKET_NAME> \\\n--log-filter=\"resource.type=gce_instance AND severity>=WARNING\"\nFor detailed guidance, see Google Cloud Logging Documentation.\nContainer Logs (Docker, Kubernetes)\nContainers package applications and their dependencies into a single lightweight unit. Because \ncontainers often run ephemeral workloads, continuous and standardized logging is key to \nsecurity monitoring.\nPage 26|36\nDocker Logs\n1. Docker Engine Logs\no\nLocation: Typically stored in /var/log/docker.log on Linux hosts.\no\nKey Fields: Daemon-level events, such as container starts/stops, image pulls, \nerrors from container runtime.\no\nUse inSecurity: Identify unauthorized container creation or malicious images \nbeing pulled from untrusted registries.\n2. Container STDOUT/STDERRLogs\no\nLocation: By default, stored in\n/var/lib/docker/containers/<container_id>/<container_id>-json.log.\no\nUse in Security: Detect anomalies within running applications (e.g., repeated \nerror messages indicating a brute-force attempt or application misuse).\n1. Docker Logging Drivers\no\nTypes:json-file, syslog, fluentd, gelf, awslogs, and others.\no\nUse in Security: Can integrate with centralized logging solutions, reducing the \nchance of log tampering if the container is compromised.\nDocker Example\nUsing the syslog logging driver, youcan direct container logs to a remote syslog server:\ndocker run --log-driver=syslog --log-opt syslog-\naddress=tcp://192.168.1.10:514 \\\n--log-opt tag=\"{{.ImageName}}/{{.Name}}/{{.ID}}\" \\\nmy_secure_image\nRefer to Docker Logging Documentation for configuration details.\nKubernetes Logs\n1. Container Logs\no\nCollection: Typically gathered via kubectl logs <pod_name>or via a logging \nagent (Fluentd, Logstash, or a sidecar pattern).\no\nUse in Security: Detect suspicious application errors or specific triggers such \nas repeated 401/403 responses indicating an authentication brute-force \nattempt.\n2. Kubelet Logs\no\nLocation: Paths differ depending on the OS distribution; can include\n/var/log/kubelet.log.\no\nUse inSecurity: Track container scheduling issues, unauthorized attempts to \nschedule privileged pods, or interactions that could indicate a compromised \nnode.\nPage 27|36\n3. Control Plane Logs(APIServer, Scheduler, Controller Manager)\no\nLocation: Often under /var/log/ on the control plane node or aggregated using a \ncentralized logging solution.\no\nUse in Security: Identify unauthorized API calls, suspicious pod creations, or \nattempts to escalate privileges via Kubernetes role bindings.\n4. Audit Logs\no\nPurpose: Record every request to the Kubernetes API server.\no\nConfiguration: Enable auditing by modifying the --audit-log-path and --audit-\npolicy-file flags on the APIServer.\no\nUse in Security: Fundamental for investigating incidents. You can track \neverything from RBAC changes to privileged container spawns.\nKubernetes Example\nA simple audit policy file (audit-policy.yaml) might look like:\napiVersion: audit.k8s.io/v1 \nkind: Policy\nrules:\n- level: Metadata \nresources:\n- group: \"\"\nresources: [\"secrets\"]\n- level: RequestResponse \nresources:\n- group: \"\"\nresources: [\"pods/exec\"]\nYou can reference the Kubernetes Auditing Documentation for more advanced configurations.\nPractical Considerations and Real-World Tips\n•\nCentralization: Whether using AWS CloudWatch, Azure Monitor, Google Cloud Logging,\nor self-hosted ELK stacks, centralizing logs from multiple cloud providers and container\nplatforms is essential for correlation.\n•\nAccess Controls: Make sure that logs, especially those containing sensitive information \n(credentials, personal data), are stored in restricted areas. Configure IAM roles or \nequivalent to control who can read or export logs.\n•\nAlerting and Dashboards: Build targeted alerts. For example, create an alert if a new \nKubernetes cluster role binding is created that grants cluster admin privileges.\nPage 28|36\n•\nRetentionPolicies: Align with regulatory requirements. Some industries require keeping \nlogs for extended periods, whereas others may prioritize cost optimization.\n•\nLog Volume vs. Relevance: Filtering out excessive “noise” helps avoid data overload. \nSet up granular logging only where necessary, or implement log sampling for high-\nvolume events like container debug logs.\n•\nCross-PlatformCorrelation: When investigating an incident, cross-reference container \nlogs with the underlying cloud infrastructure logs. For instance, if a malicious container \nis identified, reviewing AWS CloudTrail or GCP Cloud Audit logs can show who deployed \nit and from where.\nBy covering these areas, SOC analysts gain better visibility into cloud-based and containerized \nenvironments. Each platform offers different types of logs and various ways to configure them, \nbut the overarching principle remains the same: you need complete, centralized, and reliable \nlogging to effectively detect and respond to security incidents.\n2.6.IoT/SCADA/OTLogs\nIoT(Internet of Things), SCADA (Supervisory Control and Data Acquisition), and OT(Operational \nTechnology) devices play a critical role in modern industries, from manufacturing floors to \nenergy grids. Logs generated by these systems provide valuable insights into operational status, \nperformance metrics, and potential security threats. Monitoring these logs effectively can be \nchallenging due to the diversity of protocols, the variety of operating systems and firmware \ninvolved, and the high availability requirements that often characterize industrial environments. \nBelow is an overview of the main considerations, examples of log sources, and best practices to \nensure comprehensive monitoring.\nUnderstanding IoT,SCADA, and OTEnvironments\nIoT Overview\nIoTdevices are typically embedded systems used in various contexts—smart homes, industrial \nsensors, healthcare devices, and more. They often have:\n•\nLimited resources (CPU, memory) making local storage of logs difficult.\n•\nCustom firmwarethat may or may not produce standardized logs.\n•\nNetwork constraints such as low-bandwidth or intermittent connectivity.\nSCADA and OTSystems\nSCADA and other OTsystems control and monitor industrial processes in energy, \nmanufacturing, transportation, and critical infrastructure. Key distinctions include:\n•\nReal-time or near-real-time processing with strict performance and availability \nrequirements.\n•\nUse of specialized protocols (e.g., Modbus, DNP3, OPC-UA) where logging capabilities \ncan differ from those found in ITenvironments.\n•\nLegacy components that might not support current cybersecurity standards or modern \nlogging frameworks.\nPage 29|36\nTypes of Logs to Monitor\n1. System Logs: Embedded operating systems or specialized OS variations used by \nindustrial controllers (PLCs, RTUs, HMIs) might produce kernel messages or standard \nsyslog entries when available.\n2. Network TrafficLogs: Many industrial protocols can be captured by network sensors or \nspecialized gateways. Anomalies in traffic—like unexpected Modbus function codes—\nmay indicate malicious activity or misconfiguration.\n3. Application Logs: SCADA software logs events such as operator actions, process \nthresholds, alarm states, and device connectivity issues. These logs can reveal \nunauthorized changes to critical setpoints.\n4. Firmware/Device Logs: IoTand OT devices often generate messages related to\nfirmware integrity checks or patch updates. Monitoring these can help detect suspicious \nattempts to install unauthorized firmware.\n5. Security Appliance Logs: When perimeter security is present in industrial networks, \nfirewalls, IDS/IPS, and dedicated OTsecurity appliances produce logs regarding \nintrusion attempts, blocked traffic, and detected threats.\nPractical Challenges in Log Collection\n1. Protocol Complexity\nMany IoTand industrial protocols are proprietary or only partially documented. \nInterpreting logs requires an understanding of the specific protocol version and vendor \nimplementation.\n2. Limited Storage and Processing Power\nSome devices rotate logs quickly due to limited disk space. SOC analysts may need to \nforward these logs to a central server in real time to avoid data loss.\n3. High Availability Requirements\nStopping or reconfiguring a production system to enable certain logs might be infeasible \nif it disrupts critical operations. Analysts must plan logging configurations carefully, \noften during scheduled downtimes.\n4. Segmentationand Air Gaps\nIndustrial networks are sometimes isolated (“air-gapped”) from corporate networks.\nSecure mechanisms (e.g., data diodes, jump hosts) are needed to relay logs without\nintroducing new vulnerabilities.\nBest Practices for Monitoring\n1. Standardize and Normalize Logs\nWhenever possible, configure devices to output logs in a standardized format, such as syslog or \nJSON. This step simplifies ingestion into a SIEM or log management solution.\nExample: Configuring syslog on a Linux-based industrial controller\nsudo apt-get install rsyslog \nsudo systemctl enable rsyslog\nP a g e 30 |36\n# Configure /etc/rsyslog.conf to forward logs\n*.* @192.168.100.10:514\nIn an OT environment, you may need vendor-specific documentation to enable syslog \nforwarding. If syslog is not an option, use an industrial gateway or a protocol converter that can \nparse native logs and send them in a common format.\n2. Correlate with Physical Process Data\nSCADA systems often track process variables (e.g., temperature, pressure, flow). Cross-\nreferencing these metrics with login attempts or configuration changes can reveal malicious or \nerroneous actions. Correlation rules in your SIEM might look for:\n•\nSudden setpoint changes followed by alarm acknowledgments.\n•\nUnauthorized user accounts created just before critical equipment is taken offline.\n•\nRepeated networkscans coinciding with elevated temperature readings on IoT \nsensors.\n3. Implement Least Privilege and Access Controls\nModern industrial solutions often include role-based access controls. Logs from identity and \naccess management tools show who accessed the system and what changes were made:\n•\nEnsure all logins and role escalations are recorded.\n•\nEnable multi-factor authentication (MFA) for remote connections to SCADA and OT \nconsoles.\n4. Monitor for Firmware Updates and Integrity\nUnscheduled or unauthorized firmware updates can be an early sign of compromise. Monitor \nlogs for:\n•\nFirmwareversion mismatch or unexpected reboots.\n•\nDevice reimaging events occurring outside normal maintenance windows.\nMany industrial device vendors provide integrity-check features. Leverage these and forward \nrelated events to the SOC for review.\n5. Leverage Specialized Threat Intelligence\nThreat intelligence feeds focusing on ICS/SCADA vulnerabilities can help enrich your log \nanalysis. For instance, MITRE ATTCCK for ICS \n(https://collaborate.mitre.org/attackics/index.php/Main_Page) lists techniques and tactics used \nby adversaries targeting operational technology. Incorporating these indicators into your \nmonitoring rules can enhance detection capabilities.\nReal-World Examples\nExample 1: Power Grid Manipulation Attempt\nAn attacker gains access to a SCADA workstation used to manage a regional power grid. Review \nof the logs shows:\n1.\nMultiple failed RDP logins from an external IP.\nPage 31|36\n2. Successful login under an admin account (possibly via stolen credentials).\n3. Sudden changes in circuit breaker open/close commands issued at unusual times.\nCross-referencing logs from the SCADA software with firewall logs reveals inbound connections \nbypassed normal VPN channels, indicating a compromise on the perimeter. The timely \ncorrelation of these logs prevented a large-scale outage.\nExample 2: Compromised IoT Sensor Network\nA manufacturing facility experiences irregular temperature readings from a cluster of IoT \nsensors. Logs collected from the device management platform show:\n1. Unusual spike in network traffic directed at the sensors.\n2. Firmwaretamperingattempts logged by the device’s built-in integrity checks.\n3. Outboundconnectionsfrom the sensors to unauthorized IPaddresses.\nInvestigationfinds that the sensors had outdated firmware with a known vulnerability. Patching \nthem quickly and blocking the malicious IP addresses at the firewall mitigated further data \nexfiltration and potential system damage.\nComparative Overview\nAspect\nIoT\nSCADA /OT\nPrimaryFocus\nSmart devices C sensors\nIndustrial process control\nLogging Formats Often proprietary or minimal\nSyslog, proprietary (e.g., PLC logs)\nProtocols\nMǪTT,CoAP, HTTP(S)\nModbus, DNP3, OPC-UA\nSecurity\nVaries widely; often unpatched Safety, availability, real-time ops\nChallenges\nResource constraints\nLegacy systems, air-gapped networks\nActionable Steps for SOC Analysts\n1. Identify Key Log Sources: Prioritize critical controllers (PLCs, RTUs) and high-impact \nIoTdevices.\n2. Establish Secure Log Forwarding: Use encrypted channels (e.g., TLS, SSH tunnels) \nwhen sending logs across network boundaries.\n3. Create BaselineProfiles: Understand normal operation of devices and detect \ndeviations. For instance, if a PLC typically receives commands only during business \nhours, an alert can trigger on after-hours changes.\n4. Combine Networkand Host-Based Monitoring: Many attacks against OTsystems \ninvolve lateral movement or pivot from the ITside. Include NetFlow, firewall, and \nendpoint logs in your analysis.\nPage 32|36\n5.\nReview VendorGuidance: Major industrial vendors like Siemens, Rockwell Automation, \nand Schneider Electric publish documentation on best practices for logging and\nsecurity. Stay up to date with vendor patches and advisories.\nPage 33|36\n3.KeyMonitoring Practice\nEffective log monitoring hinges on sound processes, properly configured tools, and clear \nobjectives. SOC analysts should focus on strategies that help distinguish normal from \nsuspicious behaviors, preserve and protect relevant data, and leverage automation where \npossible. The following practices outline core considerations for detecting anomalies, retaining \nlogs securely, and employing supporting technologies.\n1.\nDetectingAnomalies and Incidents (Alerts, Correlation)\nAnomaly Detection vs. Signature-Based Detection\nAnomaly detection involves establishing a baseline of normal operations and flagging \ndeviations. This is useful for identifying zero-day threats or unusual user behavior. In contrast, \nsignature-based detection relies on known indicators of compromise (IoCs), such as specific IP \naddresses, hash values, or attack patterns. Most modern SOCs use a hybrid approach to \ncapture both unknown threats (anomalies) and known malicious activity (signatures).\nContextual Analysis of Logs\nWhen investigating events, it is rarely enough to look at a single log source. Correlating data \nacross multiple sources (e.g., firewall, endpoint detection and response [EDR], and Active \nDirectory logs) can reveal sophisticated attacks. For instance, seeing repeated user \nauthentication failures in an Active Directory log and simultaneous unusual outbound \nconnections in a firewall log could point to a brute-force attempt followed by data exfiltration.\nAlert Thresholds and Fine-Tuning\nSOC teams often deploy alert rules on SIEM or IDS/IPS systems to notify them of suspicious \nactivities. Balancing these thresholds is critical:\n•\nToostrict: Risk flooding the SOC with false positives, causing alert fatigue and missed \nreal threats.\n•\nToorelaxed: Allows significant security incidents to gounnoticed, delaying response \nand remediation.\nFinding the right balance often requires iterative tuning based on historical data, environment \nspecifics, and known business processes.\nReal-World Example\nConsider a situation where a user account is suddenly accessing hundreds of files on a file \nserver at unusual hours. Anomaly detection rules might flag this behavior if it deviates from the \nnormal usage pattern of that user. Meanwhile, a signature-based rule might detect that some of \nthese files match known malicious toolkits (e.g., Mimikatz or similar). Correlating both alerts \nenables SOC analysts to identify a potential account compromise and data theft incident much \nfaster.\nSample SIEMǪuery (Splunk)\nindex=windows_logs sourcetype=WinEventLog:Security\nEventCode=4625 OR EventCode=4624\nPage 34|36\n| stats count by Account_Name, EventCode\n| where count > 20\nIn this example, the query checks for successful logons (4624) or failed logons (4625) and looks \nfor any account logging multiple times beyond a threshold, which could indicate brute force or \nlateral movement attempts.\n3.2.Log Retention and Security\nRetention Policies\nLogs must be kept for a specified duration, often defined by organizational policies, regulations \n(e.g., PCI DSS, HIPAA), and compliance standards. Typical retention periods range from 90 days \nto multiple years, depending on the data sensitivity and industry requirements. SOC analysts \nshould verify that retention policies align with both threat-hunting needs and legal obligations.\nLog Storage Considerations\n•\nCentralized Storage:Storing logs in a single repository (e.g., a SIEM or log management \nplatform) simplifies searching, correlation, and backup.\n•\nRedundancy: Using multiple storage locations or clustering ensures logs remain \navailable even if hardware fails.\n•\nEncryption: Encrypting logs at rest (e.g., using disk-level encryption) and in transit (e.g., \nTLS for log forwarding) prevents unauthorized access.\n•\nAccess Controls: Implement role-based access controls (RBAC) so that only \nauthorized personnel can view or manipulate sensitive logs.\nHandling Log Integrity\nTopreserve evidentiary value, organizations should ensure logs cannot be easily tampered with:\n1. Hashing: Generating hashes (e.g., using SHA-256) for log files and storing them \nseparately helps detect unauthorized modifications.\n2. Write-Once-Read-Many (WORM) Storage: Some platforms support WORM-like \nfunctionality where logs can be written but not altered afterward.\n3. Audit Trails: Keep track of who accessed the log repository, when they accessed it, and \nwhat changes (if any) were made.\nExample of Secure Log Storage\nA company might use an Amazon S3 bucket with versioning and server-side encryption enabled \nfor archiving logs from on-premises systems. The AWS Key Management Service (KMS) provides \nsecure key storage, while AWS Identity and Access Management (IAM) enforces strict \npermissions. Official documentation on this setup can be found on the AWS Documentation\npages.\nPage 35|36\n3.3. Supporting Tools (SIEM, SOAR)\nSIEM (Security Information and Event Management)\nSIEM solutions collect, parse, and normalize logs from various sources, allowing analysts to \nsearch, correlate, and generate alerts in near real time. Common SIEM platforms include \nSplunk Enterprise Security, IBM ǪRadar, and Microsoft Sentinel. Key features:\n•\nLog AggregationandNormalization: Standardizes events to a common format.\n•\nCorrelation Rules: Creates alerts when multiple indicators occur in a defined \nsequence.\n•\nDashboarding and Reporting: Offers visual interfaces for monitoring security posture \nand presenting metrics to management.\nSOAR (Security Orchestration, Automation, and Response)\nSOAR platforms automate tasks that analysts would otherwise perform manually. Examples \ninclude Palo Alto Networks Cortex XSOAR (formerly Demisto) and Splunk Phantom. These tools \ncan be configured to:\n1. Enrich Alerts: Automatically gather host or network information from threat intelligence \nsources.\n2. Contain Incidents: For instance, disable a compromised user account or isolate a \nmalicious endpoint.\n3. Orchestrate Responses: Trigger workflows that involve multiple security and IT \nsystems.\nAutomation and Playbooks\nA standard approach in SOAR is to develop playbooks—automated workflows that define how \nto respond to specific incidents. For example, if an alert indicates a suspicious PowerShell \nscript ran on a server, a playbook might:\n1. Retrieve relevant endpoint logs.\n2. Compare the script hash with a threat intelligence database.\n3. Ǫuarantine the host if the hash is malicious.\n4. Create a ticket in the incident management system.\nHigh-Level Comparison of SIEM vs. SOAR\nFeature\nSIEM\nSOAR\nPrimary Focus Centralizing logs, correlation, \nalerts\nAutomating and orchestrating responses\nData \nProcessing\nAggregation and analysis of large \nvolumes of log data\nIntegration with multiple security/IT tools \nto enrich and act on alerts\nPage 36|36\nFeature\nSIEM\nSOAR\nTypical Output Security alerts, dashboards, \nreports\nWorkflow automation, playbooks, and \ncontainment actions\nUsage \nComplexity\nMedium to High\nMedium to High (depends on desired \nautomation)\nIn many cases, SOCs integrate both SIEM and SOAR for comprehensive coverage. The SIEM \nhandles large-scale ingestion and correlation, while the SOAR platform automates investigation \nand response steps. This integration reduces mean time to detect (MTTD)and mean time to \nrespond (MTTR), ultimately strengthening the organization’s security posture.",
      "page_count": 36,
      "pages": [
        {
          "page": 1,
          "text": "Critical\nLogs to \nMonitor: A \nGuide for\nSOC \nAnalysts",
          "char_count": 54,
          "ocr_used": false
        },
        {
          "page": 2,
          "text": "Page 2|36\nTable of Contents\n1. Introduction ......................................................................... 3\n1.\nImportance of Log Monitoring in SOC............................................................................ 3\n2.\nScope and Purpose of the Guide................................................................................... 4\n2. Key Types of Logs .................................................................. 6\n1.\nSystem Logs (Windows, Linux, macOS.......................................................................... 6\n2.\nNetwork Logs (Firewall, Router, IDS/IPS) ....................................................................... 9\n3.\nApplication and Database Logs .................................................................................. 15\n4.\nSecurity Logs (AV, EDR, XDR) ...................................................................................... 19\n5.\nCloud Logs (AWS, Azure, GCP) and Container Logs (Docker, Kubernetes)..................... 23\n6.\nIoT/SCADA/OT Logs .................................................................................................... 28\n3. Key Monitoring Practice....................................................... 33\n1.\nDetecting Anomalies and Incidents (Alerts, Correlation).............................................. 33\n2.\nLog Retention and Security......................................................................................... 34\n3.\nSupporting Tools (SIEM, SOAR) ................................................................................... 35",
          "char_count": 1575,
          "ocr_used": false
        },
        {
          "page": 3,
          "text": "Page 3|36\n1.Introduction\nLogs are the footprints of every digital activity, serving as a chronological record of events within \nsystems, networks, and applications. In a modern Security Operations Center (SOC), analysts \nrely on these logs to detect threats, investigate security incidents, and maintain an \norganization’s overall security posture. Without structured and well-monitored logs, even the \nmost advanced security solutions can miss critical indicators of compromise (IoCs) or fail to \ncorrelate suspicious behaviors across multiple systems. The goal of this guide is to highlight \nwhich logs matter most, why they are essential, and how to approach their monitoring in a way \nthat benefits both junior and mid-level SOC analysts.\n1.\nImportance of Log Monitoring in SOC\nIn any sizable IT infrastructure, raw data volumes can be massive—firewalls alone can generate\nthousands of log entries per second. While these logs may sometimes appear as unremarkable\nlines of text, they hold valuable insights that help detect and counteract security threats. Proper\nlog monitoring is crucial for several reasons:\n1. Visibility andContext\nLogs provide context by showing what happened, when it happened, and how it was \nexecuted. This visibility is essential in distinguishing normal behaviors from anomalies. \nFor example, an unexpected privilege escalation in a Windows system log can point to \nlateral movement by an attacker. Similarly, repeated authentication failures in a Linux \nenvironment might indicate a brute-force attack.\n2. Incident Detection and Response\nAutomated alerts from a SIEM (Security Information and Event Management) platform \noften originate from suspicious patterns in logs. These alerts enable SOC analysts to \nquickly identify and respond to potential incidents. For instance, correlation rules might \nflag a user logging in from two geographically distant locations within a short timeframe, \nsuggesting a stolen credential.\n3. Audit andCompliance\nMany regulatory frameworks, such as PCI DSS, HIPAA, or ISO 27001, mandate log \nretention and regular review. By monitoring logs, organizations ensure they meet \ncompliance requirements and can produce a clear audit trail during investigations or\naudits. Logs are often the first place auditors check to confirm security controls are in \nplace and functioning as intended.\n4. Threat Hunting\nBeyond detection, logs form the basis for proactive threat hunting. Analysts look for \nunusual patterns—like the execution of a PowerShell script in an environment where \nPowerShell usage is rare—to uncover stealthy attacks. By analyzing logs over time, \nthreat hunters can identify trends and adversary tactics that might be missed by \nautomated systems alone.\n5. Forensic Investigations\nWhen an incident does occur, well-structured logs are the key to forensic investigations. \nThey help recreate the timeline of an attack, show which systems were accessed, and \nhighlight the data that was exfiltrated. Detailed logs of user actions, network",
          "char_count": 3015,
          "ocr_used": false
        },
        {
          "page": 4,
          "text": "Page 4|36\nconnections, and system calls can be the difference between accurately attributing an \nincident and letting attackers remain undetected.\nReal-World Example\nConsider a scenario where a SOC analyst notices unusual outbound traffic from a critical \nserver. By reviewing firewall logs correlated with Windows Event Logs, the analyst uncovers a \nmalicious process communicating with an external IP address. Ǫuick analysis shows that the \ncommunication began right after a suspicious privilege escalation event. This correlation can \nguide incident response teams to isolate the server, contain the threat, and remediate the \nvulnerability before data is compromised.\nPractical Tip\nOn Linux systems, commands like journalctl -p warning -r can help you quickly locate higher-\npriority events in reverse chronological order, allowing faster triage of potential security issues. \nOn Windows, tools such as wevtutil qe Security /rd:true /f:text /q:\"*\" |findstr /i \"4624 4625 4634 \n4672\" can filter the security event log for specific Event IDs related to logons.\n1.2.Scope and Purpose of the Guide\nThis guide targets both new and mid-level SOC analysts who want to enhance their skills in log \nmonitoring. It covers common sources of logs—like operating systems, networks, applications, \nand security tools—and highlights what to look for in each. By focusing on the most critical logs \nand describing how they fit into the overall security strategy, this guide aims to streamline the \nday-to-day work of SOC professionals. Specifically, it aims to:\n•\nIdentifyKey LogSources\nWe will look at systemlogs (Windows, Linux, macOS), networklogs (firewall, router, \nIDS/IPS), application anddatabase logs, security logs (AV,EDR, XDR), cloud logs \n(AWS, Azure, GCP), container logs (Docker, Kubernetes), and IoT/SCADA/OTlogs. The\nprimary focus is on what makes each category critical, how to collect them, and which \nevents are most indicative of a security issue.\n•\nPresent Practical Monitoring Techniques\nFrom correlation rules to anomaly-based detection, we will discuss the practices that \ntranslate raw logs into actionable insights. We will also cover topics like logretention, \nlogsecurity, and best practices around data classification to ensure that sensitive logs \nremain protected.\n•\nShowcase Real-Life Use Cases\nEach log type comes with its unique set of challenges and attack vectors. We will walk\nthrough realistic scenarios—such as detecting lateral movement, privilege escalation,\nor malicious uploads—and demonstrate how the logs serve as vital evidence.\n•\nGuide on SupportingTools\nSIEM (Security Information and Event Management) and SOAR (Security Orchestration, \nAutomation, and Response) tools are at the heart of modern SOCs. The guide explains \nhow these platforms integrate with different log sources, automate alerting, and help \norchestrate response actions.",
          "char_count": 2877,
          "ocr_used": false
        },
        {
          "page": 5,
          "text": "Page 5|36\n•\nEncourage Continuous Learning\nCyber threats evolve rapidly, and so do best practices in logging and monitoring. With \nreferences to resources like NIST SP 800-92 (Guide to Computer Security Log \nManagement) and official vendor documentation (e.g., Microsoft’s Windows Event Log\ndocumentation), this guide points readers to reliable sources for ongoing education.\nBy focusing on these areas, the guide aims to equip analysts with the knowledge and skills to \nprioritize logs effectively and detect potential breaches before they escalate. Through a mix of \ntheoretical explanation and practical examples, readers will gain confidence in setting up \nlogging strategies, tuning alerts, and conducting thorough investigations.",
          "char_count": 735,
          "ocr_used": false
        },
        {
          "page": 6,
          "text": "Page 6|36\n2.KeyTypes ofLogs\n1.\nSystem Logs (Windows, Linux, macOS\nSystem logs form the backbone of incident detection and response efforts, providing analysts \nwith the essential baseline data needed to investigate abnormal events, track user activities, \nand diagnose security threats. Across Windows, Linux, and macOS, these logs share the \ncommon goal of recording key operating system (OS) events, though each platform organizes \nand structures logs in its own way.Understanding how they work, what they log, and how to \ninterpret them is crucial for SOC analysts.\nWindows Logs\nCommon Log Sources\n•\nSystem Log:Captures events generated by the Windows operating system and its built-\nin services. It records driver issues, service startups and shutdowns, and kernel-level \nmessages.\n•\nApplication Log: Stores application-specific events, such as errors, warnings, or \ninformational messages from software installed on the system (e.g., database clients, \nproductivity tools).\n•\nSecurity Log: Focuses on security-related events: login attempts, account lockouts, \nand user right assignments. Often used for auditing and forensic investigations.\n•\nOther Logs: Windows also creates dedicated logs for specialized services, like DFS \nReplication and PowerShell, which can be viewed under the Applications and \nServices Logsin the Event Viewer.\nPractical Monitoring Tips\n1. Event Viewer:Built into Windows, Event Viewer offers a quick way to view and filter \nevents. Analysts can group events by severity (Critical, Error, Warning, Information) or by \nEvent ID.\n2. Filtering and Searching: Use XMLfiltering in Event Viewer or PowerShell commands to \nhunt for specific event IDs (e.g., 4624 for successful logins, 4625 for failed logins).\n3. Security Baselines: Monitor high-value Event IDs. For example:\no\n4624 (Successful account login)\no\n4625(Failed login)\no\n4672(Special privileges assigned to a user)\no\n4688(Anew process has been created)\no\n4648(Alogon was attempted using explicit credentials)\n4. PowerShell Logging: By enabling Module Loggingand Script Block Logging, analysts \ncan track suspicious or obfuscated commands. Refer to Microsoft Docs (PowerShell\nLogging) for guidelines.",
          "char_count": 2188,
          "ocr_used": false
        },
        {
          "page": 7,
          "text": "Page 7|36\nExample: Filtering Security Events viaPowerShell\nGet-WinEvent -LogName Security | Where-Object {\n$_.Id -in 4624, 4625\n}\nThis command pulls Security Log events for successful and failed logins, enabling quick \ndetection of abnormal activity.\nLinux Logs\nSyslog andJournald\nMost Linux distributions rely on syslog or systemd-journald to collect and manage log \nmessages:\n•\n/var/log/syslog or /var/log/messages: Contains informational and non-critical system \nevents.\n•\n/var/log/auth.log or /var/log/secure: Focuses on authentication-related messages. \nEssential for detecting brute-force login attempts, sudo activity, or SSH logins.\n•\n/var/log/kern.log: Stores kernel-level messages, useful for diagnosing driver issues or \nunusual kernel events.\n•\nJournal Logs (systemd-based distros): Consolidates logs in a binary format, accessible \nvia journalctl.\nKey AreastoMonitor\n1. Authentication: Watch for repeated failed login attempts, new user additions in\n/etc/passwd, or sudden changes in sudo usage.\n2. Cron Jobs: Check /var/log/cronor associated logs for unauthorized scheduled tasks. \nCron jobs can be used by adversaries for persistence.\n3. Kernel Messages: Investigate repeated kernel warnings or errors that could indicate \nhardware issues or potential rootkit activity.\n4. Service Logs: For services like Apache, Nginx, or SSH, monitor dedicated logs (e.g.,\n/var/log/apache2/access.log, /var/log/nginx/access.log, /var/log/secure) for unexpected \ntraffic or repeated authentication failures.\nExample: UsingJournalctl\n# View all logs related to SSH \njournalctl -u sshd\n# Filter logs for a specific time range\njournalctl --since \"2023-01-01\" --until \"2023-01-31\"\nThis approach helps analysts quickly search for anomalies within a particular service or \ntimeframe.",
          "char_count": 1777,
          "ocr_used": false
        },
        {
          "page": 8,
          "text": "Page 8|36\nmacOS Logs\nUnified LoggingSystem\nSince macOS Sierra (10.12), Apple introduced a unified logging system that stores log messages \nin a structured format:\n•\nConsole App: The built-in Console allows viewing of system logs, diagnostic reports, \nand crash logs.\n•\nLogCommands: The log utility in the terminal offers extensive filtering, streaming, and \nsearching capabilities. For example:\n#View live log messages (system-wide)\nlog stream --level=info\n#Search for specific processes or errors\nlog show --predicate 'process == \"sshd\" AND eventMessage \nCONTAINS \"Failed password\"'\n•\nSubsystems and Categories: macOS logs categorize messages by subsystem (e.g., \ncom.apple.networking) and category (e.g., connection). This helps analysts narrow \ndown events.\nSecurity-Specific Logs\n•\n/var/log/system.log:Retains many core system messages and is often the first stop \nwhen troubleshooting.\n•\nApple System Log (ASL):Legacy logging that coexists with the unified logging system, \naccessible via command-line tools for older macOS versions.\n•\nAuthentication Logs: Attempts to log in via SSH or local accounts can appear in\n/var/log/asl/ or through the unified logging interface.\nMonitoring andDetection\n1. Focus onRepeatedFailures: Like Linux, repeated failed SSH attempts or unexpected \nprocess launches warrant attention.\n2. Check Crash Reports: Attackers sometimes induce crashes of security tooling. Crash \nlogs in macOS can provide early indicators of tampering.\n3. Leverage Built-in Tools: Use the Console to filter logs by Process or Message Type.\nApple’s Developer documentation on Unified Logging provides details on advanced\nusage.\nCross-Platform Considerations\nAspect\nWindows\nLinux\nmacOS\nLog Files\nEvent Viewer (System, \nSecurity, etc.)\n/var/log/syslog,\n/var/log/auth.log, etc.\nUnified Logging System \n(log show, log stream)",
          "char_count": 1834,
          "ocr_used": false
        },
        {
          "page": 9,
          "text": "Page 9|36\nAspect\nWindows\nLinux\nmacOS\nCommon \nTools\nPowerShell, Event \nViewer, WMI\ntail, grep, awk, journalctl\nConsole App, log CLI\nAlert Focus\nEvent IDs (4624, 4625,\netc.), policy changes\nSSH failures, privilege\nescalations, systemd\nservice errors\nSSH failures, system\ncrashes, unexpected\nsubsystem messages\nCentralization\nWindows Event \nForwarding (WEF), \nSysmon logs to SIEM\nRsyslog, Syslog-ng, \nsystemd-journald to SIEM\nExport logs via the log \ncollect feature or \nstreaming to a SIEM\nLogging Agents and Centralization\nMany organizations opt to forward Windows, Linux, and macOS logs to a central SIEM or log \nmanagement platform:\n•\nWindows: Windows Event Forwarding (WEF), Sysmon for detailed process-level logging, \nor third-party agents like NXLog or Splunk Universal Forwarder.\n•\nLinux: Rsyslog, Syslog-ng, or systemd-journald can forward logs to remote servers. \nBeats (Filebeat, Metricbeat) from Elastic can also collect and ship logs.\n•\nmacOS: Use third-party agents (e.g., Osquery for query-based logging, or Splunk, \nDatadog agents) to unify logs under a single pane.\n2.2. Network Logs (Firewall, Router,IDS/IPS)\nLogs from network devices and security systems are among the most critical data sources in a \nSecurity Operations Center (SOC). By analyzing firewall, router, and IDS/IPS logs, SOC analysts \ngain visibility into traffic patterns, security events, and potential anomalies. This visibility is \ncrucial for identifying malicious behavior early and for responding to incidents before they can \npropagate within the environment. Below are the key concepts, best practices, and real-world \nscenarios that illustrate how to work effectively with network logs.\n1. Firewall Logs\nFirewalls are often the first line of defense, filtering traffic based on predefined rules. Monitoring \nfirewall logs provides insights into both permitted and denied network connections.\n1.\nCommon Fields in Firewall Logs\nTypical firewall logs will include fields such as:\n•\nTimestamp:The date and time the event was recorded.\n•\nSource IP/Destination IP:IPaddresses of the client and server.\n•\nSource Port/Destination Port:Ports used by the services or applications \ncommunicating.\n•\nProtocol: Network protocol in use (e.g., TCP, UDP, ICMP).\n•\nAction: Indicates whether the traffic was allowed, denied, dropped, or rejected.",
          "char_count": 2320,
          "ocr_used": false
        },
        {
          "page": 10,
          "text": "P a g e 10 |36\n•\nRule or Policy Name: Identifies which firewall rule triggered the log entry.\nFirewalls may also log additional details like interface names (e.g., eth0, WAN, LAN), packet\nsize, or the reason for a deny or reject action. Modern firewalls, especially Next-Generation \nFirewalls (NGFWs), can log application-level data and user information if integrated with identity \nmanagement systems.\nField\nDescription\nExample Value\nTimestamp\nDate and time of event\n2025-01-25 10:15:32\nSource IP\nOriginating IP address\n192.168.10.5\nDestination IP\nTarget IPaddress\n10.0.5.20\nSource Port\nOriginating TCP/UDP port\n53452\nDestination Port Target TCP/UDP port\n443\nProtocol\nNetwork protocol (TCP, UDP, ICMP) TCP\nAction\nAllowed, denied, dropped, etc.\nAllowed\nRule Name\nFirewall policy or rule name\nBlock_Telnet\n2.\nPractical UseCases\n•\nBlocked Connection Attempts: Monitoring repeated connection attempts on sensitive \nports (e.g., 22 for SSH or 3389 for RDP) can reveal brute-force attempts or port scans.\n•\nUnusual Traffic Volumes: A sudden spike in traffic from a single IPor subnet might \nindicate a DoS or DDoS attempt.\n•\nInboundvs. Outbound Monitoring: Outbound connections to suspicious IP addresses \nor countries where the organization does not conduct business can be early indicators \nof compromised hosts (e.g., malware calling home).\n3.\nExample Firewall LogAnalysis in SIEM\nBelow is an example Splunk query that filters for denied connections with a focus on TCP port \n3389 (RDP):\nindex=firewall_logs action=DENY dest_port=3389\n| stats count by src_ip, dest_ip, action, rule_name\nThis query helps highlight any source IPs that are repeatedly trying to reach RDP services but are \nbeing denied, which might indicate an attempted intrusion.\n2. Router Logs\nRouters primarily forward packets between networks and maintain routing tables. Logs \ngenerated by routers often focus on system messages, routing updates, and interface errors \nrather than application-specific data. However, they are still critical for overall visibility, \nespecially in environments with distributed architectures.",
          "char_count": 2093,
          "ocr_used": false
        },
        {
          "page": 11,
          "text": "Page 11|36\n1.\nTypes of Router Logs\n•\nSystemor Event Logs: Includes messages about device reboots, software crashes, or \nconfiguration changes.\n•\nRoutingProtocol Logs: Information related to BGP, OSPF, EIGRP, or other routing \nprotocols.\n•\nInterface Logs: Status changes on interfaces (up/down), packet errors (CRC errors, \ncollisions), and bandwidth usage.\n•\nAuthentication Logs: Successful or failed logins via SSH, Telnet, or console access to \nthe router.\nRouter logs often follow the standard Syslog format (e.g., Cisco routers with severity levels 0–7). \nIntegratingthese logs into a SIEM allows analysts to correlate network topology changes with \nsecurity events (for instance, if a router interface goes down just before a security incident on \nthat segment).\n2.\nExample: Cisco Router Syslog Messages\nCisco routers send Syslog messages with various severity levels. An example message might \nlook like this:\n<189>Jan 25 10:25:10 MY-ROUTER: %LINK-3-UPDOWN: Interface\nGigabitEthernet0/1, changed state to up\n•\n189 corresponds to the Syslog priority.\n•\n%LINK-3-UPDOWN indicates a link status change with severity level 3 (Error).\n•\nThe message indicates which interface changed state.\nIn a SIEM, you might filter for %LINK-3-UPDOWN events to track unexpected interface state \nchanges. If an interface goes down suddenly, it may indicate a physical issue, misconfiguration, \nor malicious activity aiming to disrupt network segments.\n3. IDS/IPS Logs\nIntrusion Detection Systems (IDS) and Intrusion Prevention Systems (IPS)monitor network \ntraffic for signs of malicious behavior, policy violations, or known attack signatures. While \nfirewalls typically operate at the transport or network layer, IDS/IPS solutions can inspect \npackets in more depth (Layer 7), providing richer context about application-level threats.\n1.\nIDSvs.IPS\n•\nIDS(IntrusionDetection System): Detects potential threats and generates alerts. It \ndoes not automatically block the traffic.\n•\nIPS(IntrusionPreventionSystem): Detects threats and can take preventive actions, \nsuch as dropping malicious packets or blocking IP addresses in real-time.\n2.\nCommon Fields in IDS/IPSLogs\n•\nSignature ID:A unique identifier for the rule or signature triggered (e.g., Snort rules have \nSID values).",
          "char_count": 2262,
          "ocr_used": false
        },
        {
          "page": 12,
          "text": "Page 12|36\n•\nEvent or Alert Message: The name or description of the suspicious activity (e.g., “ET \nTROJAN Zeus Tracker”).\n•\nSeverity or Priority: Indicates the criticality of the alert.\n•\nSource IP/Destination IP/Ports: Information about the traffic flow.\n•\nAction: Whether the traffic was detected, dropped, or allowed.\n3.3. Practical Example with Suricata\nSuricata is a popular open-source IDS/IPS engine. Suricata outputs JSON logs that can be \ningested by SIEM tools like Elasticsearch or Splunk. A typical Suricata alert entry in JSON format \nmight include:\n{\n\"timestamp\": \"2025-01-25T10:30:45.123456+0000\",\n\"flow_id\": 1234567890, \n\"event_type\": \"alert\", \n\"src_ip\": \"192.168.1.100\",\n\"src_port\": 53452,\n\"dest_ip\": \"10.0.5.20\",\n\"dest_port\": 80, \n\"proto\": \"TCP\", \n\"alert\": {\n\"action\": \"blocked\", \n\"gid\": 1,\n\"signature_id\": 2010935,\n\"rev\": 3,\n\"signature\": \"ET TROJAN Known Malicious Domain\", \n\"category\": \"Trojan Activity\",\n\"severity\": 2\n}\n}\nIn the above example:\n•\nsignature_id: 2010935 corresponds to a Suricata rule IDthat references a specific \nTrojan signature.\n•\naction: The traffic was blocked by Suricata (IPS mode).\n•\ncategory:“Trojan Activity” indicates the type of threat.",
          "char_count": 1186,
          "ocr_used": false
        },
        {
          "page": 13,
          "text": "Page 13|36\n3.4. Real-Life Attack Scenarios\n•\nSǪL InjectionAttempts: IDS/IPS solutions look for patterns in HTTP requests that \nmatch known SǪL injection techniques.\n•\nExploitKits: If a host attempts to download or connect to an exploit kit domain, IDS/IPS \nlogs can reveal the suspicious domain name and signature match.\n•\nLateral Movement: Attackers may try to move horizontally within a network. IDS/IPS \ncan detect unusual SMB or RDP traffic patterns.\n4. Parsing and Analyzing Network Logs in Practice\n1.\nLogManagement and SIEMIntegration\nSOC analysts typically centralize firewall, router, and IDS/IPS logs into a SIEM for correlation and \nanalysis. This allows cross-referencing of events from multiple sources. For instance, if an IDS \nalert indicates a Trojan signature and the firewall logs show outbound traffic to a suspicious IP, \nthe SIEM can generate a higher-priority alert.\nExample Logstash configuration snippet to parse Suricata JSON logs:\ninput { \nfile {\npath => \"/var/log/suricata/eve.json\" \ntype => \"suricata\"\ncodec => \"json\"\n}\n}\nfilter {\nif [event_type] == \"alert\" { \nmutate {\nadd_tag => [\"suricata_alert\"]\n}\n}\n}\noutput { \nelasticsearch {\nhosts => [\"localhost:9200\"]\nindex => \"suricata-alerts-%{+YYYY.MM.dd}\"\n}\n}",
          "char_count": 1234,
          "ocr_used": false
        },
        {
          "page": 14,
          "text": "Page 14|36\nThis configuration reads Suricata’s eve.json file, filters for alert events, and then tags them as \nsuricata_alert before sending them to Elasticsearch.\n4.2. Correlation Rules\nIn a SIEM, correlation rules can look for conditions such as:\n1. HighVolume of Denied Connections: If more than 100 firewall denies occur from the \nsame source IPin 5 minutes, generate an alert.\n2. Multiple IDSAlerts fortheSameHost: If a host triggers more than 3 different IDS \nsignatures within a short period, raise the priority of the incident.\n3. Router Interface Down +IDS Alerts: If a critical interface goes down and multiple IDS \nalerts are detected on adjacent network segments, investigate potential sabotage or \nwidespread compromise.\nBy creating correlation rules that combine different log types, SOC analysts can detect \ncoordinated attacks and reduce the volume of false positives.\n5. Challenges and Best Practices\n•\nLogVolume: Network devices can generate a massive amount of data. Using filters or \nsampling may be necessary, but be cautious not to discard important information.\n•\nNormalization: Different vendors (Cisco, Palo Alto, Fortinet, etc.) often have unique log \nformats. Normalizing fields (e.g., ensuring consistent naming of src_ip, dest_ip) is \ncrucial for effective correlation.\n•\nEncryption and Secure Transport: Ensure that log data is transmitted securely, for \ninstance using TLS for Syslog (Syslog over TLS). Unencrypted logs can be intercepted \nand manipulated byadversaries.\n•\nRegular Tuning:IDS/IPS rulesets need regular updates to reflect new threats. Similarly, \nfirewall policies should be reviewed to ensure they align with the evolving network \nenvironment.\n•\nTime Synchronization: NTP (Network Time Protocol) should be enabled and correctly \nconfigured on all devices to maintain consistent timestamps. Accurate timestamps are \ncritical for event correlation.\n6. References and Further Reading\n•\nOfficial Suricata Documentation: https://suricata-ids.org/docs/\n•\nSnort (IDS/IPS)Documentation: https://www.snort.org/\n•\nCisco Syslog Guide: https://www.cisco.com/c/en/us/support/docs/security-\nvpn/syslog/\n•\nFirewallBest Practices (NIST): \nhttps://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-41r1.pdf\nThese sources provide authoritative insights into configuration, logging standards, and threat \ndetection patterns for network devices and security solutions.",
          "char_count": 2408,
          "ocr_used": false
        },
        {
          "page": 15,
          "text": "Page 15|36\n2.3.Application and Database Logs\nApplication logs capture events and behaviors tied directly to an application’s functionality.\nThey may include user interactions, system operations, exceptions, debug details, and custom \nevents defined by developers. Database logs, on the other hand, record all actions related to \ndata transactions, schema changes, authentication, and potential errors or performance \nbottlenecks in a database system. Together, these logs provide a holistic view of how software is \nfunctioning and how data is being accessed or manipulated. This is crucial for detecting \nunauthorized activities, performance issues, and other anomalies. Below are the key \nconsiderations, best practices, and real-world examples to help you effectively monitor and \nanalyze both application and database logs.\nUnderstanding Application Logs\nCommon Types of Application Logs\n1. ErrorandExceptionLogs\nThese capture unexpected behaviors and stack traces. They’re typically generated by \nframeworks such as Java’s Log4j or Logback, Python’s logging library, or .NET’s built-in \nlogging features.\n2. Debug and Diagnostic Logs\nThese logs detail internal operations, often containing fine-grained information used \nduring development or troubleshooting. Debug logs can be extremely verbose, so \nthey’re typically enabled only in test or development environments unless a production \nissue requires deeper insights.\n3. Transactionor Event Logs\nApplications that handle user transactions—such as e-commerce checkouts—often \ngenerate transaction logs. These logs detail each step in the user flow (e.g., adding items \nto a cart, checking out, payment processing).\n4. Audit Logs\nSome applications produce audit logs for compliance or security reasons. These logs \ntrack user access, role changes, or critical configuration updates, helping you see who \ndid what and when.\nLogging Frameworks and Formats\nModern applications often rely on standardized logging frameworks:\n•\nJava (Log4j,Logback)\n•\n.NET (Serilog, NLog)\n•\nPython(logging)\n•\nNode.js (winston, pino)\nThese frameworks allow developers to configure log levels, structure log messages (for \nexample, in JSON), and specify output targets like console, files, or external aggregation \nservices. Consistency in format is important for parsing and correlation within a SIEM.",
          "char_count": 2335,
          "ocr_used": false
        },
        {
          "page": 16,
          "text": "Page 16|36\nExample Configuration (Log4j2 in Java)\n<Configuration status=\"warn\">\n<Appenders>\n<File name=\"FileLogger\" fileName=\"logs/application.log\">\n<PatternLayout pattern=\"%d{ISO8601} [%t] %-5level %logger{36}\n- %msg%n\" />\n</File>\n</Appenders>\n<Loggers>\n<Logger name=\"com.example.app\" level=\"info\" additivity=\"false\">\n<AppenderRef ref=\"FileLogger\"/>\n</Logger>\n<Root level=\"error\">\n<AppenderRef ref=\"FileLogger\"/>\n</Root>\n</Loggers>\n</Configuration>\nThis snippet specifies an application.log file, using a pattern to log timestamps, thread names, \nlog levels, and the actual message. By setting the root level to error and the application logger to \ninfo, you can avoid unnecessary noise.\nMonitoring Application Logs in Practice\n1. Establish Baseline Behavior\nKnowing the normal application behavior (e.g., average response times, typical error \nrates) helps detect anomalies, such as a sudden influx of specific exceptions that might \nindicate an attack or misconfiguration.\n2. Look forCommon Attack Patterns\nUnauthorized access attempts often show up as repeated login failures, suspicious \nparameters in URLs (e.g., SǪL injection probes), or unusual behaviors in session \nmanagement. Web applications might log 404 errors or suspicious HTTP methods in \nhigher frequency under attack.\n3. Integrationwith SIEM\nTocorrelate application logs with system or network events, use a SIEM tool like Splunk, \nIBMǪRadar, or ElasticSecurity. For example, by correlating an application’s “multiple \nfailed logins” event with a firewall log showing suspicious IP scanning, you can quickly \nconfirm or rule out an intrusion attempt.\n4. Alerting and Thresholds\nThreshold-based alerts on error rates, transaction volume drops, or spikes in exceptions",
          "char_count": 1736,
          "ocr_used": false
        },
        {
          "page": 17,
          "text": "Page 17|36\ncan catch incidents early. Machine learning-driven anomaly detection in tools such as \nAzure Sentinel or AWSSecurity Hub can further refine alerts by identifying patterns not \ncaptured by static rules.\n5.\nRetentionPolicies\nDue to volume, application logs can grow quickly. You need to define retention policies\nbalancing security requirements and storage costs. Compliance frameworks (e.g., PCI\nDSS, HIPAA) sometimes dictate minimum retention periods for specific log types.\nUnderstanding Database Logs\nKey Types of DatabaseLogs\n1. Transaction Logs\nCapture every change to the database’s data. They’re critical for recovery and forensic \nanalysis. For instance, in Microsoft SǪL Server, the transaction log tracks every \nmodification in the order they occur, enabling point-in-time recovery.\n2. ErrorLogs\nThese highlight critical events, such as server startup issues or serious errors that affect\ndatabase availability. Examples include MySǪL’s error.log or Oracle’s alert logs.\n3. General Ǫuery Logs(MySǪL)/Audit Logs(VariousVendors)\nRecord all queries received by the server or track account activity. They are invaluable in \ndetecting SǪL injection attacks, suspicious data extraction, or attempts at privilege \nescalation.\n4. SlowǪuery Logs\nFound in MySǪL or PostgreSǪL, these capture queries that exceed a certain execution\ntime threshold. Slow queries might indicate performance issues or potential denial-of-\nservice attempts if queries are being manipulated byan attacker.\nPractical Monitoring Strategies\n1. Regular ReviewforSuspicious Ǫueries\nMonitor queries that drop or alter critical tables without expected change-control \ntickets. Also look for wildcard searches or large data extracts happening at unusual \nhours.\n2. Privilege Abuse Detection\nIf a user with minimal privileges starts running queries typical of an administrator, it’s a \nstrong indicator of compromised credentials or privilege escalation. Enforcing least \nprivilege and then reviewing logs for anomalies is a potent strategy.\n3. ErrorPattern Analysis\nRepeated database error messages, such as “invalid column name” or “syntax error,”\ncan indicate attempts at SǪL injection. SOC analysts can configure SIEM correlation\nrules to flag repetitive errors from a single source IP.\n4. Performance Data\nLogs that point to high resource usage or timeouts can be an early warning of a brute-\nforce or denial-of-service attack at the database layer.",
          "char_count": 2433,
          "ocr_used": false
        },
        {
          "page": 18,
          "text": "Page 18|36\nExample Configurations and Ǫueries\nMySǪL\nToenable the general query log:\nSET GLOBAL general_log = 'ON';\nSET GLOBAL general_log_file = '/var/log/mysql/general.log'; \nTo enable the slow query log:\nSET GLOBAL slow_query_log = 'ON';\nSET GLOBAL long_query_time = 2; -- Queries taking longer than 2 \nseconds will be logged\nNote: Logging all queries can significantly impact performance, so only enable it temporarily for \ndiagnostics or funnel logs to a centralized system where you can parse and analyze them \nefficiently.\nPostgreSǪL\nPostgreSǪL has extensive logging configurations in postgresql.conf. For instance:\nlogging_collector = on \nlog_directory = 'pg_log' \nlog_filename = 'postgresql-%a.log' \nlog_statement = 'all'\nlog_min_duration_statement = 2000\n# logs queries over 2ms\nBy setting log_statement to all, you can see every statement, though this is typically too verbose \nfor production.\nReal-World Scenarios\n•\nDetection of Data Exfiltration\nA SOC analyst notices an application log showing unusual parameter values in a REST \nAPI call. By cross-referencing the database logs, the analyst confirms multiple large \nSELECT statements retrieving sensitive customer data. Additional correlation with\nnetwork logs shows a large data transfer to an external IP.The logs collectively point to \nan ongoing data exfiltration attempt.\n•\nAuditingfor Compliance\nIn a financial services application, compliance requirements mandate auditing every \ntransaction. By reviewing application logs (which capture the logic layer) and the \ndatabase’s transaction logs (which capture the final record changes), auditors can \nconfirm that every deposit or withdrawal is authorized and properly executed.\n•\nIdentifying Performance Attacks\nA series of slow queries might initially look like a performance bottleneck. However, \ninvestigating further reveals that attackers are intentionally crafting resource-intensive \nqueries to degrade the application’s responsiveness. Alerts in the SIEM correlate these \nslow queries with repeated 503 errors on the web server, confirming a denial-of-service \nattempt.",
          "char_count": 2097,
          "ocr_used": false
        },
        {
          "page": 19,
          "text": "Page 19|36\nAdditional Resources\n•\nOWASPCheat Sheet Series: https://cheatsheetseries.owasp.org/\nOffers guidelines on secure logging practices, specifically around sanitizing logs and \npreventing log forging.\n•\nMySǪL Official Docs: https://dev.mysql.com/doc/\nDetailed instructions on configuring error logs, general query logs, and slow query logs.\n•\nPostgreSǪL Documentation: https://www.postgresql.org/docs/\nContains comprehensive configuration guides for logging and auditing features.\n•\nMicrosoft SǪL Server Docs: https://docs.microsoft.com/en-us/sql/\nExplains how to manage and interpret transaction logs, error logs, and other diagnostic \ndata.\n•\nOracle Database Docs: https://docs.oracle.com/en/database/\nProvides details on the alert log, trace files, and advanced auditing configurations.\nComparisons and Data\nLogType\nExamples\nTypical Use Case\nPotential Security Insight\nError/Exception Stack traces, code \nline references\nDebugging application \ncrashes, identifying \nfaulty modules\nFrequent exceptions might \nhint at malicious input or \nattempts to exploit \nvulnerabilities\nTransaction\nE-commerce logs, \nbanking transactions\nAuditing success/failure \nof critical actions\nReal-time monitoring helps \ndetect fraudulent \ntransactions\nAudit (App C \nDB)\nUser actions, role\nchanges, schema\nmodifications\nCompliance with \nregulations, \naccountability\nPinpointing unauthorized\nadmin actions or privilege\nescalations\nSlow Ǫuery\nǪueries exceeding a \ntime threshold\nPerformance tuning or \nbottleneck analysis\nIdentifying possible DoS \nattempts or resource \nexhaustion attacks\nWhen collecting and analyzing these logs, consider normalizing fields (timestamps, user IDs, \nhostnames) so different log sources can be correlated effectively. Some SIEM platforms or \ncentralized logging solutions (e.g., the ELK Stack) allow youto define common field mappings \nand dashboards that unify application and database insights.\n2.4.Security Logs (AV,EDR, XDR)\nSecurity logs generated by Antivirus (AV),Endpoint Detection and Response (EDR), and \nExtended Detection and Response (XDR) solutions are crucial for modern SOC operations. They \noffer granular visibility into potential threats affecting endpoints and the broader environment. \nBelow is an exploration of the fundamentals, along with real-world examples and guidance for \neffective log monitoring.",
          "char_count": 2343,
          "ocr_used": false
        },
        {
          "page": 20,
          "text": "P a g e 20 |36\nUnderstanding the Components\nAntivirus (AV)Logs\nAntivirus solutions focus primarily on detecting known malware signatures and blocking \nsuspicious files. Their logs typically include:\n•\nMalware Detections: Alerts triggered when a file matches a known signature or exhibits \nmalicious behavior.\n•\nǪuarantineand Remediation Actions: Logs showing which files were quarantined, \ndeleted, or otherwise neutralized.\n•\nUpdate and Scan Events: Records of signature updates, scheduled scans, and on-\ndemand scan results.\nReal-World Example\nA traditional AV tool like Microsoft Defender Antivirus (part of Windows Security) generates \nlogs under the Windows Event Log:\n•\nEvent ID1116indicates malware detection.\n•\nEvent ID5001logs the scanning engine starting up.\nBy aggregating these event IDs in a SIEM, analysts can quickly see patterns of infection \nattempts and confirm that updates have been applied.\nEndpoint Detection and Response (EDR) Logs\nEDR solutions expand on basic antivirus features by providing in-depth endpoint telemetry, real-\ntime threat detection, and response capabilities. Common log data includes:\n•\nProcess Creationand Termination: Detailed tracking of command-line parameters, \nuser context, and file paths.\n•\nBehavioral Indicators: Observations related to suspicious activities such as code \ninjection, privilege escalations, or unusual registry modifications.\n•\nIsolation and Response Actions: Logs showing when and why an endpoint was\nisolated, network connections were blocked, or an automated script was run for\ncontainment.\nEDR logs often present a sequence of correlated events, making it easier for SOC analysts to \nreconstruct the timeline of an attack. Tools like CrowdStrike Falcon, SentinelOne, or Carbon \nBlackoffer dashboards that display triggered detection rules (e.g., MITRE ATTCCK techniques) \nalongside automated remediation actions.\nSample EDR LogAnalysis(Splunk)\nBelow is an example of how you might parse EDR logs in Splunk to identify suspicious child \nprocesses of PowerShell:\nindex=edr_logs parent_process=PowerShell.exe\n| stats count by child_process, user, host\n| where count > 3",
          "char_count": 2139,
          "ocr_used": false
        },
        {
          "page": 21,
          "text": "Page 21|36\nThis query looks for any child process spawned by PowerShell and flags any repeated \noccurrences, which might indicate malicious scripts or living-off-the-land techniques.\nExtended Detection and Response (XDR)Logs\nXDR solutions take the endpoint-centric approach of EDR and extend it to incorporate data from \nnetwork appliances, cloud workloads, and applications. The goal is to unify detection, \ninvestigation, and response across multiple layers of the ITenvironment.\n•\nCross-Source Correlation: XDR aggregates logs from endpoints, email gateways, \nidentity providers, and more, applying analytics to uncover hidden threats.\n•\nCloud andHybrid Integrations:Telemetry from cloud platforms and containerized \nworkloads often merges with endpoint data, offering a complete view of complex \nattacks.\n•\nAdaptive Response: Based on machine learning and correlation rules, XDR can trigger\nautomated playbooks that respond to threats in real time (e.g., disabling compromised\nuser accounts, isolating infected hosts, or blocking suspicious domains at the firewall).\nReference Architectures\n•\nMicrosoft 365 Defender integrates data from endpoints (Defender for Endpoint), email\n(Defender for Office 365), identities (Azure Active Directory), and cloud apps (Defender\nfor Cloud Apps).\n•\nPaloAlto Cortex XDRprocesses data from endpoints and integrates with network \nsensors or firewalls to provide enhanced correlation.\nLog Collection Best Practices\n1. Centralize Logsin aSIEM:Consolidate all AV,EDR, and XDR logs into a SIEM platform \nlike Splunk, Elastic Stack, or IBMǪRadar. This ensures a single view for threat hunting \nand alert triage.\n2. Use Consistent LogFormatting: Where possible, standardize the format (e.g., JSON, \nSyslog) to streamline parsing, correlation, and long-term storage.\n3. Retain Sufficient History: Depending on regulatory requirements and threat modeling, \nkeep historical logs long enough to investigate slow-moving attacks or advanced \npersistent threats.\n4. Correlate Across Multiple Sources: Antivirus alerts alone may provide minimal \ncontext. When cross-referenced with endpoint telemetry and user login patterns, they \nreveal the bigger picture—especially relevant for advanced or multi-stage attacks.\n5. Implement AutomatedDetection Rules: Leverage built-in detection capabilities of \nyour EDR/XDR solution and supplement them with custom rules tailored to your \nenvironment. For example, create an alert when a known safe process spawns an \nunusual child process (e.g., outlook.exe launching cmd.exe).\n6. Leverage Threat Intelligence: Enrich detection events with threat intelligence feeds \n(e.g., VirusTotal, AlienVault OTX). This helps validate suspicious activity, especially when \nan alert references a known malicious domain or file hash.",
          "char_count": 2783,
          "ocr_used": false
        },
        {
          "page": 22,
          "text": "Page 22|36\nCommon Security Events to Watch For\nEvent Type\nKey Indicators\nExample Tools\nMalware \nDetections\nFile hashes, known signatures, suspicious \nfile behaviors\nMicrosoft Defender, \nMcAfee, Symantec\nBehavioral \nAnomalies\nUnusual registry modifications, abnormal \nprocess trees\nCrowdStrike Falcon, \nSentinelOne\nPrivilege \nEscalations\nAttempts to change user privilegeor run \nprocesses as admin\nSysmon +EDR correlation\nExfiltration \nAttempts\nNetwork connections to suspicious \ndomains or large data transfers\nPalo Alto Cortex XDR, \nSplunk logs\nPersistence \nMechanisms\nNew services, startup items, scheduled \ntasks\nSysmon +EDR detection \nrules\nMonitoring these events in near-real time allows SOC analysts to prioritize the highest-risk alerts \nand initiate containment actions quickly.\nPractical Monitoring Tips\n•\nTrackFailed and Successful RemediationAttempts: If an AV tries to quarantine a file \nrepeatedly but fails, it could be a sign of advanced malware or user tampering.\n•\nMonitor EDRAgent Health: Regularly ensure that EDR agents are running on all \nendpoints. Unexpected agent downtime may be an early indicator of an attacker’s \nattempt to disable security controls.\n•\nReview Automated Playbook Outcomes: XDR platforms often run automated \nresponses. Confirm that these responses are both effective and aligned with your \norganization’s incident response procedures.\n•\nEngage With Vendor Documentation: Each AV, EDR, or XDR vendor has specific best \npractices for log collection and interpretation. For instance, Microsoft Defender for \nEndpoint publishes detailed logging guidelines at Microsoft Docs.\nExample Incident Flow Using AV,EDR, and XDR\n1. AVAlert: Triggers on a suspicious executable with a known malicious hash.\n2. EDR Correlation: Maps the suspicious executable back to a process tree, showing it \nwas launched by an unusual script.\n3. XDRVisibility: Confirms the script was downloaded from an unrecognized domain and \nties this domain to a known threat actor via threat intelligence feeds.\n4. AutomatedResponse: XDR or a SOAR platform quarantines the endpoint, blocks the \ndomain at the firewall, and opens a ticket in the incident management system.\n5. SOC Analyst Action: Investigates the entire chain of events, verifies threat removal, and \nupdates detection rules to prevent similar attacks.",
          "char_count": 2324,
          "ocr_used": false
        },
        {
          "page": 23,
          "text": "Page 23|36\nBy combining the strengths of AV,EDR, and XDR logs in a well-structured monitoring strategy, \nSOC analysts can respond swiftly to a wide range of threats—from commodity malware to \nsophisticated, persistent attacks.\n2.5.Cloud Logs (AWS,Azure, GCP) and Container Logs (Docker, \nKubernetes)\nCloud platforms and container orchestration systems have become an essential part of many \norganizations. In a SOC environment, monitoring logs from these platforms is critical for threat \ndetection, compliance, and troubleshooting. Below is an overview of the most important log \nsources and practical considerations for AWS, Azure, GCP, Docker, and Kubernetes.\nAWS Logs\nCommon LogTypes\n1. CloudTrail Logs\no\nPurpose: Track API calls and account activity across AWS services.\no\nKey Fields: eventName, eventSource, awsRegion, sourceIPAddress, userAgent, \nrequestParameters, responseElements.\no\nUse in Security: Identifies suspicious or unauthorized actions, such as \nunexpected changes to IAM policies, creation or deletion of critical resources, or \nunusual console logins.\n2. CloudWatch Logs\no\nPurpose: Centralized logging for AWS services (EC2 system logs, Lambda \nfunction logs, etc.).\no\nKey Fields: Vary depending on service-specific events; typically include \ntimestamps, log level (ERROR, WARNING, INFO), and custom application \nmessages.\no\nUse in Security: Helps correlate system-level events with higher-level activities. \nExample: correlating an EC2 instance’s system error logs with an unauthorized \naccess attempt shown in CloudTrail.\n3. VPC FlowLogs\no\nPurpose: Capture network flow information (source/destination IP,ports, traffic \nacceptance/rejection).\no\nKey Fields: version, account-id, interface-id, srcaddr, dstaddr, srcport, dstport, \nprotocol, action, log-status.\no\nUse inSecurity: Identifies unusual traffic patterns or data exfiltration attempts, \nsuch as large outbound data transfers or traffic from unknown IP ranges.\nPractical Example\nA typical workflow for ingestion involves forwarding CloudTrail and VPC Flow Logs to an S3 \nbucket, then using Amazon Kinesis or a third-party tool (e.g., Logstash) to parse and send events \nto a SIEM. For example, with AWS CLI youcan enable CloudTrail logging:",
          "char_count": 2223,
          "ocr_used": false
        },
        {
          "page": 24,
          "text": "Page 24|36\naws cloudtrail create-trail \\\n--name MySecurityTrail \\\n--s3-bucket-name my-security-logs \\\n--include-global-service-events\nFor more details, see the AWS CloudTrail Documentation.\nAzure Logs\nCommon LogTypes\n1. AzureActivity Logs\no\nPurpose: Provide insights into management operations (e.g., resource creation, \nmodification, or deletion).\no\nKey Fields: authorization, caller, category, operationName, resourceId, status.\no\nUse inSecurity: Detect unauthorized resource creation, changes to security \ngroups, or attempts to elevate privileges.\n2. Azure Monitor Logs(LogAnalytics)\no\nPurpose: Collect logs from Azure resources, containers, VMs, and applications.\no\nKey Fields: Vary based on the resource type; commonly include timestamps, \noperation IDs, user details, and other contextual data.\no\nUse in Security: Offers extensive querying and correlation capabilities. SOC \nteams can detect anomalies by combining signals from multiple sources \n(Activity Logs, VM logs, etc.).\n3. Diagnostics Logs\no\nPurpose: Detailed insights from specific Azure services, such as Key Vault \naccess logs, Azure App Service logs, or Azure Storage logs.\no\nKey Fields: Depend on the service but often include request endpoints, \nauthentication details, and result codes.\no\nUse in Security: Detects potential credential misuse, suspicious activity in data \nstorage, or unusual application behavior.\nPractical Example\nSending logs to Azure Monitor can be done by configuring a Diagnostic Setting for each \nresource. For instance, to route Activity Logs to Azure Monitor and a storage account:\nSet-AzDiagnosticSetting -ResourceId\n/subscriptions/<SUBSCRIPTION_ID>/resourceGroups/<RESOURCE_GROUP>/pro \nviders/Microsoft.Web/sites/<APP_NAME> `\n-WorkspaceId <AZURE_MONITOR_WORKSPACE_ID> `\n-StorageAccountId\n/subscriptions/<SUBSCRIPTION_ID>/resourceGroups/<RESOURCE_GROUP>/pro \nviders/Microsoft.Storage/storageAccounts/<STORAGE_ACCOUNT_NAME>\n`",
          "char_count": 1923,
          "ocr_used": false
        },
        {
          "page": 25,
          "text": "Page 25|36\n-Enabled $true\nRefer to Azure Monitor Documentation for more details.\nGCP Logs\nCommon LogTypes\n1. Cloud Audit Logs\no\nPurpose: Record admin and data access events for GCP services (similar to \nAWS CloudTrail).\no\nKey Fields: protoPayload.serviceName, protoPayload.methodName, \nresourceName, authenticationInfo, requestMetadata.\no\nUse in Security: Surface privilege escalation attempts or suspicious \nmodifications to GCP resources (e.g., enabling/disabling critical services).\n2. VPC FlowLogs\no\nPurpose: Collect network flow information for Google Cloud VPCs.\no\nKey Fields: srcIP, destIP, srcPort, destPort, protocol, connectionEstablished, \nbytesSent, bytesReceived.\no\nUse in Security: Spot reconnaissance or exfiltration activity by analyzing \ninbound and outbound traffic patterns.\n3. Cloud Logging\no\nPurpose: Central logging service for events from GCP services, containers, \ncustom applications.\no\nKey Fields: Service-specific data, timestamps, severity levels, resource labels \n(e.g., k8s_container, gce_instance).\no\nUse inSecurity: Enables correlation of application-level logs with \ninfrastructure-level events.\nPractical Example\nToexport GCP logs to a SIEM, youcan create a sink that routes logs to a Pub/Sub topic, which a \ncustom or third-party collector can then forward. An example using the gcloud CLI:\ngcloud logging sinks create my-security-sink \\\nstorage.googleapis.com/<BUCKET_NAME> \\\n--log-filter=\"resource.type=gce_instance AND severity>=WARNING\"\nFor detailed guidance, see Google Cloud Logging Documentation.\nContainer Logs (Docker, Kubernetes)\nContainers package applications and their dependencies into a single lightweight unit. Because \ncontainers often run ephemeral workloads, continuous and standardized logging is key to \nsecurity monitoring.",
          "char_count": 1781,
          "ocr_used": false
        },
        {
          "page": 26,
          "text": "Page 26|36\nDocker Logs\n1. Docker Engine Logs\no\nLocation: Typically stored in /var/log/docker.log on Linux hosts.\no\nKey Fields: Daemon-level events, such as container starts/stops, image pulls, \nerrors from container runtime.\no\nUse inSecurity: Identify unauthorized container creation or malicious images \nbeing pulled from untrusted registries.\n2. Container STDOUT/STDERRLogs\no\nLocation: By default, stored in\n/var/lib/docker/containers/<container_id>/<container_id>-json.log.\no\nUse in Security: Detect anomalies within running applications (e.g., repeated \nerror messages indicating a brute-force attempt or application misuse).\n1. Docker Logging Drivers\no\nTypes:json-file, syslog, fluentd, gelf, awslogs, and others.\no\nUse in Security: Can integrate with centralized logging solutions, reducing the \nchance of log tampering if the container is compromised.\nDocker Example\nUsing the syslog logging driver, youcan direct container logs to a remote syslog server:\ndocker run --log-driver=syslog --log-opt syslog-\naddress=tcp://192.168.1.10:514 \\\n--log-opt tag=\"{{.ImageName}}/{{.Name}}/{{.ID}}\" \\\nmy_secure_image\nRefer to Docker Logging Documentation for configuration details.\nKubernetes Logs\n1. Container Logs\no\nCollection: Typically gathered via kubectl logs <pod_name>or via a logging \nagent (Fluentd, Logstash, or a sidecar pattern).\no\nUse in Security: Detect suspicious application errors or specific triggers such \nas repeated 401/403 responses indicating an authentication brute-force \nattempt.\n2. Kubelet Logs\no\nLocation: Paths differ depending on the OS distribution; can include\n/var/log/kubelet.log.\no\nUse inSecurity: Track container scheduling issues, unauthorized attempts to \nschedule privileged pods, or interactions that could indicate a compromised \nnode.",
          "char_count": 1773,
          "ocr_used": false
        },
        {
          "page": 27,
          "text": "Page 27|36\n3. Control Plane Logs(APIServer, Scheduler, Controller Manager)\no\nLocation: Often under /var/log/ on the control plane node or aggregated using a \ncentralized logging solution.\no\nUse in Security: Identify unauthorized API calls, suspicious pod creations, or \nattempts to escalate privileges via Kubernetes role bindings.\n4. Audit Logs\no\nPurpose: Record every request to the Kubernetes API server.\no\nConfiguration: Enable auditing by modifying the --audit-log-path and --audit-\npolicy-file flags on the APIServer.\no\nUse in Security: Fundamental for investigating incidents. You can track \neverything from RBAC changes to privileged container spawns.\nKubernetes Example\nA simple audit policy file (audit-policy.yaml) might look like:\napiVersion: audit.k8s.io/v1 \nkind: Policy\nrules:\n- level: Metadata \nresources:\n- group: \"\"\nresources: [\"secrets\"]\n- level: RequestResponse \nresources:\n- group: \"\"\nresources: [\"pods/exec\"]\nYou can reference the Kubernetes Auditing Documentation for more advanced configurations.\nPractical Considerations and Real-World Tips\n•\nCentralization: Whether using AWS CloudWatch, Azure Monitor, Google Cloud Logging,\nor self-hosted ELK stacks, centralizing logs from multiple cloud providers and container\nplatforms is essential for correlation.\n•\nAccess Controls: Make sure that logs, especially those containing sensitive information \n(credentials, personal data), are stored in restricted areas. Configure IAM roles or \nequivalent to control who can read or export logs.\n•\nAlerting and Dashboards: Build targeted alerts. For example, create an alert if a new \nKubernetes cluster role binding is created that grants cluster admin privileges.",
          "char_count": 1678,
          "ocr_used": false
        },
        {
          "page": 28,
          "text": "Page 28|36\n•\nRetentionPolicies: Align with regulatory requirements. Some industries require keeping \nlogs for extended periods, whereas others may prioritize cost optimization.\n•\nLog Volume vs. Relevance: Filtering out excessive “noise” helps avoid data overload. \nSet up granular logging only where necessary, or implement log sampling for high-\nvolume events like container debug logs.\n•\nCross-PlatformCorrelation: When investigating an incident, cross-reference container \nlogs with the underlying cloud infrastructure logs. For instance, if a malicious container \nis identified, reviewing AWS CloudTrail or GCP Cloud Audit logs can show who deployed \nit and from where.\nBy covering these areas, SOC analysts gain better visibility into cloud-based and containerized \nenvironments. Each platform offers different types of logs and various ways to configure them, \nbut the overarching principle remains the same: you need complete, centralized, and reliable \nlogging to effectively detect and respond to security incidents.\n2.6.IoT/SCADA/OTLogs\nIoT(Internet of Things), SCADA (Supervisory Control and Data Acquisition), and OT(Operational \nTechnology) devices play a critical role in modern industries, from manufacturing floors to \nenergy grids. Logs generated by these systems provide valuable insights into operational status, \nperformance metrics, and potential security threats. Monitoring these logs effectively can be \nchallenging due to the diversity of protocols, the variety of operating systems and firmware \ninvolved, and the high availability requirements that often characterize industrial environments. \nBelow is an overview of the main considerations, examples of log sources, and best practices to \nensure comprehensive monitoring.\nUnderstanding IoT,SCADA, and OTEnvironments\nIoT Overview\nIoTdevices are typically embedded systems used in various contexts—smart homes, industrial \nsensors, healthcare devices, and more. They often have:\n•\nLimited resources (CPU, memory) making local storage of logs difficult.\n•\nCustom firmwarethat may or may not produce standardized logs.\n•\nNetwork constraints such as low-bandwidth or intermittent connectivity.\nSCADA and OTSystems\nSCADA and other OTsystems control and monitor industrial processes in energy, \nmanufacturing, transportation, and critical infrastructure. Key distinctions include:\n•\nReal-time or near-real-time processing with strict performance and availability \nrequirements.\n•\nUse of specialized protocols (e.g., Modbus, DNP3, OPC-UA) where logging capabilities \ncan differ from those found in ITenvironments.\n•\nLegacy components that might not support current cybersecurity standards or modern \nlogging frameworks.",
          "char_count": 2691,
          "ocr_used": false
        },
        {
          "page": 29,
          "text": "Page 29|36\nTypes of Logs to Monitor\n1. System Logs: Embedded operating systems or specialized OS variations used by \nindustrial controllers (PLCs, RTUs, HMIs) might produce kernel messages or standard \nsyslog entries when available.\n2. Network TrafficLogs: Many industrial protocols can be captured by network sensors or \nspecialized gateways. Anomalies in traffic—like unexpected Modbus function codes—\nmay indicate malicious activity or misconfiguration.\n3. Application Logs: SCADA software logs events such as operator actions, process \nthresholds, alarm states, and device connectivity issues. These logs can reveal \nunauthorized changes to critical setpoints.\n4. Firmware/Device Logs: IoTand OT devices often generate messages related to\nfirmware integrity checks or patch updates. Monitoring these can help detect suspicious \nattempts to install unauthorized firmware.\n5. Security Appliance Logs: When perimeter security is present in industrial networks, \nfirewalls, IDS/IPS, and dedicated OTsecurity appliances produce logs regarding \nintrusion attempts, blocked traffic, and detected threats.\nPractical Challenges in Log Collection\n1. Protocol Complexity\nMany IoTand industrial protocols are proprietary or only partially documented. \nInterpreting logs requires an understanding of the specific protocol version and vendor \nimplementation.\n2. Limited Storage and Processing Power\nSome devices rotate logs quickly due to limited disk space. SOC analysts may need to \nforward these logs to a central server in real time to avoid data loss.\n3. High Availability Requirements\nStopping or reconfiguring a production system to enable certain logs might be infeasible \nif it disrupts critical operations. Analysts must plan logging configurations carefully, \noften during scheduled downtimes.\n4. Segmentationand Air Gaps\nIndustrial networks are sometimes isolated (“air-gapped”) from corporate networks.\nSecure mechanisms (e.g., data diodes, jump hosts) are needed to relay logs without\nintroducing new vulnerabilities.\nBest Practices for Monitoring\n1. Standardize and Normalize Logs\nWhenever possible, configure devices to output logs in a standardized format, such as syslog or \nJSON. This step simplifies ingestion into a SIEM or log management solution.\nExample: Configuring syslog on a Linux-based industrial controller\nsudo apt-get install rsyslog \nsudo systemctl enable rsyslog",
          "char_count": 2387,
          "ocr_used": false
        },
        {
          "page": 30,
          "text": "P a g e 30 |36\n# Configure /etc/rsyslog.conf to forward logs\n*.* @192.168.100.10:514\nIn an OT environment, you may need vendor-specific documentation to enable syslog \nforwarding. If syslog is not an option, use an industrial gateway or a protocol converter that can \nparse native logs and send them in a common format.\n2. Correlate with Physical Process Data\nSCADA systems often track process variables (e.g., temperature, pressure, flow). Cross-\nreferencing these metrics with login attempts or configuration changes can reveal malicious or \nerroneous actions. Correlation rules in your SIEM might look for:\n•\nSudden setpoint changes followed by alarm acknowledgments.\n•\nUnauthorized user accounts created just before critical equipment is taken offline.\n•\nRepeated networkscans coinciding with elevated temperature readings on IoT \nsensors.\n3. Implement Least Privilege and Access Controls\nModern industrial solutions often include role-based access controls. Logs from identity and \naccess management tools show who accessed the system and what changes were made:\n•\nEnsure all logins and role escalations are recorded.\n•\nEnable multi-factor authentication (MFA) for remote connections to SCADA and OT \nconsoles.\n4. Monitor for Firmware Updates and Integrity\nUnscheduled or unauthorized firmware updates can be an early sign of compromise. Monitor \nlogs for:\n•\nFirmwareversion mismatch or unexpected reboots.\n•\nDevice reimaging events occurring outside normal maintenance windows.\nMany industrial device vendors provide integrity-check features. Leverage these and forward \nrelated events to the SOC for review.\n5. Leverage Specialized Threat Intelligence\nThreat intelligence feeds focusing on ICS/SCADA vulnerabilities can help enrich your log \nanalysis. For instance, MITRE ATTCCK for ICS \n(https://collaborate.mitre.org/attackics/index.php/Main_Page) lists techniques and tactics used \nby adversaries targeting operational technology. Incorporating these indicators into your \nmonitoring rules can enhance detection capabilities.\nReal-World Examples\nExample 1: Power Grid Manipulation Attempt\nAn attacker gains access to a SCADA workstation used to manage a regional power grid. Review \nof the logs shows:\n1.\nMultiple failed RDP logins from an external IP.",
          "char_count": 2263,
          "ocr_used": false
        },
        {
          "page": 31,
          "text": "Page 31|36\n2. Successful login under an admin account (possibly via stolen credentials).\n3. Sudden changes in circuit breaker open/close commands issued at unusual times.\nCross-referencing logs from the SCADA software with firewall logs reveals inbound connections \nbypassed normal VPN channels, indicating a compromise on the perimeter. The timely \ncorrelation of these logs prevented a large-scale outage.\nExample 2: Compromised IoT Sensor Network\nA manufacturing facility experiences irregular temperature readings from a cluster of IoT \nsensors. Logs collected from the device management platform show:\n1. Unusual spike in network traffic directed at the sensors.\n2. Firmwaretamperingattempts logged by the device’s built-in integrity checks.\n3. Outboundconnectionsfrom the sensors to unauthorized IPaddresses.\nInvestigationfinds that the sensors had outdated firmware with a known vulnerability. Patching \nthem quickly and blocking the malicious IP addresses at the firewall mitigated further data \nexfiltration and potential system damage.\nComparative Overview\nAspect\nIoT\nSCADA /OT\nPrimaryFocus\nSmart devices C sensors\nIndustrial process control\nLogging Formats Often proprietary or minimal\nSyslog, proprietary (e.g., PLC logs)\nProtocols\nMǪTT,CoAP, HTTP(S)\nModbus, DNP3, OPC-UA\nSecurity\nVaries widely; often unpatched Safety, availability, real-time ops\nChallenges\nResource constraints\nLegacy systems, air-gapped networks\nActionable Steps for SOC Analysts\n1. Identify Key Log Sources: Prioritize critical controllers (PLCs, RTUs) and high-impact \nIoTdevices.\n2. Establish Secure Log Forwarding: Use encrypted channels (e.g., TLS, SSH tunnels) \nwhen sending logs across network boundaries.\n3. Create BaselineProfiles: Understand normal operation of devices and detect \ndeviations. For instance, if a PLC typically receives commands only during business \nhours, an alert can trigger on after-hours changes.\n4. Combine Networkand Host-Based Monitoring: Many attacks against OTsystems \ninvolve lateral movement or pivot from the ITside. Include NetFlow, firewall, and \nendpoint logs in your analysis.",
          "char_count": 2103,
          "ocr_used": false
        },
        {
          "page": 32,
          "text": "Page 32|36\n5.\nReview VendorGuidance: Major industrial vendors like Siemens, Rockwell Automation, \nand Schneider Electric publish documentation on best practices for logging and\nsecurity. Stay up to date with vendor patches and advisories.",
          "char_count": 239,
          "ocr_used": false
        },
        {
          "page": 33,
          "text": "Page 33|36\n3.KeyMonitoring Practice\nEffective log monitoring hinges on sound processes, properly configured tools, and clear \nobjectives. SOC analysts should focus on strategies that help distinguish normal from \nsuspicious behaviors, preserve and protect relevant data, and leverage automation where \npossible. The following practices outline core considerations for detecting anomalies, retaining \nlogs securely, and employing supporting technologies.\n1.\nDetectingAnomalies and Incidents (Alerts, Correlation)\nAnomaly Detection vs. Signature-Based Detection\nAnomaly detection involves establishing a baseline of normal operations and flagging \ndeviations. This is useful for identifying zero-day threats or unusual user behavior. In contrast, \nsignature-based detection relies on known indicators of compromise (IoCs), such as specific IP \naddresses, hash values, or attack patterns. Most modern SOCs use a hybrid approach to \ncapture both unknown threats (anomalies) and known malicious activity (signatures).\nContextual Analysis of Logs\nWhen investigating events, it is rarely enough to look at a single log source. Correlating data \nacross multiple sources (e.g., firewall, endpoint detection and response [EDR], and Active \nDirectory logs) can reveal sophisticated attacks. For instance, seeing repeated user \nauthentication failures in an Active Directory log and simultaneous unusual outbound \nconnections in a firewall log could point to a brute-force attempt followed by data exfiltration.\nAlert Thresholds and Fine-Tuning\nSOC teams often deploy alert rules on SIEM or IDS/IPS systems to notify them of suspicious \nactivities. Balancing these thresholds is critical:\n•\nToostrict: Risk flooding the SOC with false positives, causing alert fatigue and missed \nreal threats.\n•\nToorelaxed: Allows significant security incidents to gounnoticed, delaying response \nand remediation.\nFinding the right balance often requires iterative tuning based on historical data, environment \nspecifics, and known business processes.\nReal-World Example\nConsider a situation where a user account is suddenly accessing hundreds of files on a file \nserver at unusual hours. Anomaly detection rules might flag this behavior if it deviates from the \nnormal usage pattern of that user. Meanwhile, a signature-based rule might detect that some of \nthese files match known malicious toolkits (e.g., Mimikatz or similar). Correlating both alerts \nenables SOC analysts to identify a potential account compromise and data theft incident much \nfaster.\nSample SIEMǪuery (Splunk)\nindex=windows_logs sourcetype=WinEventLog:Security\nEventCode=4625 OR EventCode=4624",
          "char_count": 2640,
          "ocr_used": false
        },
        {
          "page": 34,
          "text": "Page 34|36\n| stats count by Account_Name, EventCode\n| where count > 20\nIn this example, the query checks for successful logons (4624) or failed logons (4625) and looks \nfor any account logging multiple times beyond a threshold, which could indicate brute force or \nlateral movement attempts.\n3.2.Log Retention and Security\nRetention Policies\nLogs must be kept for a specified duration, often defined by organizational policies, regulations \n(e.g., PCI DSS, HIPAA), and compliance standards. Typical retention periods range from 90 days \nto multiple years, depending on the data sensitivity and industry requirements. SOC analysts \nshould verify that retention policies align with both threat-hunting needs and legal obligations.\nLog Storage Considerations\n•\nCentralized Storage:Storing logs in a single repository (e.g., a SIEM or log management \nplatform) simplifies searching, correlation, and backup.\n•\nRedundancy: Using multiple storage locations or clustering ensures logs remain \navailable even if hardware fails.\n•\nEncryption: Encrypting logs at rest (e.g., using disk-level encryption) and in transit (e.g., \nTLS for log forwarding) prevents unauthorized access.\n•\nAccess Controls: Implement role-based access controls (RBAC) so that only \nauthorized personnel can view or manipulate sensitive logs.\nHandling Log Integrity\nTopreserve evidentiary value, organizations should ensure logs cannot be easily tampered with:\n1. Hashing: Generating hashes (e.g., using SHA-256) for log files and storing them \nseparately helps detect unauthorized modifications.\n2. Write-Once-Read-Many (WORM) Storage: Some platforms support WORM-like \nfunctionality where logs can be written but not altered afterward.\n3. Audit Trails: Keep track of who accessed the log repository, when they accessed it, and \nwhat changes (if any) were made.\nExample of Secure Log Storage\nA company might use an Amazon S3 bucket with versioning and server-side encryption enabled \nfor archiving logs from on-premises systems. The AWS Key Management Service (KMS) provides \nsecure key storage, while AWS Identity and Access Management (IAM) enforces strict \npermissions. Official documentation on this setup can be found on the AWS Documentation\npages.",
          "char_count": 2221,
          "ocr_used": false
        },
        {
          "page": 35,
          "text": "Page 35|36\n3.3. Supporting Tools (SIEM, SOAR)\nSIEM (Security Information and Event Management)\nSIEM solutions collect, parse, and normalize logs from various sources, allowing analysts to \nsearch, correlate, and generate alerts in near real time. Common SIEM platforms include \nSplunk Enterprise Security, IBM ǪRadar, and Microsoft Sentinel. Key features:\n•\nLog AggregationandNormalization: Standardizes events to a common format.\n•\nCorrelation Rules: Creates alerts when multiple indicators occur in a defined \nsequence.\n•\nDashboarding and Reporting: Offers visual interfaces for monitoring security posture \nand presenting metrics to management.\nSOAR (Security Orchestration, Automation, and Response)\nSOAR platforms automate tasks that analysts would otherwise perform manually. Examples \ninclude Palo Alto Networks Cortex XSOAR (formerly Demisto) and Splunk Phantom. These tools \ncan be configured to:\n1. Enrich Alerts: Automatically gather host or network information from threat intelligence \nsources.\n2. Contain Incidents: For instance, disable a compromised user account or isolate a \nmalicious endpoint.\n3. Orchestrate Responses: Trigger workflows that involve multiple security and IT \nsystems.\nAutomation and Playbooks\nA standard approach in SOAR is to develop playbooks—automated workflows that define how \nto respond to specific incidents. For example, if an alert indicates a suspicious PowerShell \nscript ran on a server, a playbook might:\n1. Retrieve relevant endpoint logs.\n2. Compare the script hash with a threat intelligence database.\n3. Ǫuarantine the host if the hash is malicious.\n4. Create a ticket in the incident management system.\nHigh-Level Comparison of SIEM vs. SOAR\nFeature\nSIEM\nSOAR\nPrimary Focus Centralizing logs, correlation, \nalerts\nAutomating and orchestrating responses\nData \nProcessing\nAggregation and analysis of large \nvolumes of log data\nIntegration with multiple security/IT tools \nto enrich and act on alerts",
          "char_count": 1953,
          "ocr_used": false
        },
        {
          "page": 36,
          "text": "Page 36|36\nFeature\nSIEM\nSOAR\nTypical Output Security alerts, dashboards, \nreports\nWorkflow automation, playbooks, and \ncontainment actions\nUsage \nComplexity\nMedium to High\nMedium to High (depends on desired \nautomation)\nIn many cases, SOCs integrate both SIEM and SOAR for comprehensive coverage. The SIEM \nhandles large-scale ingestion and correlation, while the SOAR platform automates investigation \nand response steps. This integration reduces mean time to detect (MTTD)and mean time to \nrespond (MTTR), ultimately strengthening the organization’s security posture.",
          "char_count": 570,
          "ocr_used": false
        }
      ],
      "metadata": {
        "format": "PDF 1.7",
        "title": "",
        "author": "",
        "subject": "",
        "keywords": "",
        "creator": "",
        "producer": "",
        "creationDate": "",
        "modDate": "",
        "trapped": "",
        "encryption": null
      },
      "char_count": 70193,
      "word_count": 9496,
      "ocr_pages_count": 0,
      "error": null,
      "file_id": "12OKxOsjzXNPSRb9-9bTUDZ2KC0BiSmUR",
      "filename": "Critical Logs to Monitor_A Guide for SOC Analysts .pdf",
      "filepath": "downloads/Critical Logs to Monitor_A Guide for SOC Analysts .pdf",
      "download_link": "https://drive.google.com/uc?export=download&id=12OKxOsjzXNPSRb9-9bTUDZ2KC0BiSmUR"
    },
    {
      "success": true,
      "text": "8\nBROUGHT TO YOU IN PARTNERSHIP WITH\n  Chain of Responsibility\n  Command\n  Interpreter\n  Iterator\n  Mediator\n  Observer\nC O N T E N T S\nDZONE.COM  |  © DZONE, INC.\nVISIT DZONE.COM/REFCARDZ FOR MORE!\nDesign Patterns\nBY JASON MCDONALD\nABOUT DESIGN PATTERNS\nThis Design Patterns refcard provides a quick reference to the original \n23 Gang of Four (GoF) design patterns, as listed in \nthe book Design Patterns: Elements of Reusable Object-Oriented \nSoftware. Each pattern includes class diagrams, explanation, usage \ninformation, and a real world example.\nCreational Patterns: Used to construct objects such that they \ncan be decoupled from their implementing system.\nStructural Patterns: Used to form large object \nstructures between many disparate objects.\nBehavioral Patterns: Used to manage algorithms, \nrelationships, and responsibilities between objects.\n\nObject Scope: Deals with object relationships that can \nbe changed at runtime.\nClass Scope: Deals with class relationships that can be changed at \ncompile time.\nC  Abstract Factory\nS  Adapter\nS  Bridge\nC  Builder\nB  Chain of  \n        Responsibility\nB  Command\nS  Composite\nS \t Decorator\nS \t Facade\nC \t Factory Method\nS \t Flyweight\nB \t Interpreter\nB \t Iterator\nB \t Mediator\nB \t Memento\nC \t Prototype\nS \t Proxy\nB \t Observer\nC \t Singleton\nB \t State\nB \t Strategy\nB \t Template Method\nB \t Visitor\nCHAIN OF RESPONSIBILITY \t             Object Behavioral\nsuccessor\nClient\n<<interface>>\nHandler\n+handlerequest()\nConcreteHandler 1\n+handlerequest()\nConcreteHandler 2\n+handlerequest()\nPurpose\nGives more than one object an opportunity to handle a \nrequest by linking receiving objects together.\nUse \nWhen\n•\t\nMultiple objects may handle a request and the \nhandler doesn’t have to be a specific object.\n•\t A set of objects should be able to handle a \nrequest with the handler determined at runtime\n•\t A request not being handled is an acceptable \npotential outcome.\nExample\nException handling in some languages implements \nthis pattern. When an exception is thrown in a \nmethod the runtime checks to see if the method has \na mechanism to handle the exception or if it should \nbe passed up the call stack. When passed up the call \nstack the process repeats until code to handle the \nexception is encountered or until there are no more \nparent objects to hand the request to.\nCOMMAND\t\n\t\n\t\n            Object Behavioral\nReceiver\nInvoker\nCommand\n+execute()\nClient\nConcreteCommand\n+execute()\nPurpose\nEncapsulates a request allowing it to be treated as \nan object. This allows the request to be handled \nin traditionally object based relationships such as \nqueuing and callbacks.\nUse \nWhen\n•\t You need callback functionality.\n•\t Requests need to be handled at variant times or \nin variant orders.\n•\t A history of requests is needed.\n•\t The invoker should be decoupled from the \nobject handling the invocation.\nExample\nJob queues are widely used to facilitate the \nasynchronous processing of algorithms. By utilizing \nthe command pattern the functionality to be \nexecuted can be given to a job queue for processing \nwithout any need for the queue to have knowledge \nof the actual implementation it is invoking. The \ncommand object that is enqueued implements \nits particular algorithm within the confines of the \ninterface the queue is expecting.\n2\nDZONE.COM  |  © DZONE, INC.\nDESIGN PATTERNS\n INTERPRETER\t             \t                                  Object Behavioral \nClient\nContext\n<<interface>>\nAbstractExpression\n+interpret()\nTerminalExpression\n+interpret( ) : Context\nNonterminalExpression\n+interpret( ) : Context\n◆\nPurpose\nDefines a representation for a grammar as well as a \nmechanism to understand and act upon the grammar.\nUse \nWhen\n•\t There is grammar to interpret that can be \nrepresened as large syntax trees.\n•\t The grammar is simple.\n•\t Efficiency is not important.\n•\t Decoupling grammar from underlying \nexpressions is desired.\nExample\nText based adventures, wildly popular in the 1980’s, \nprovide a good example of this. Many had simple \ncommands, such as “step down” that allowed traversal \nof the game. These commands could be nested such \nthat it altered their meaning. For example, “go in” \nwould result in a different outcome than “go up”. By \ncreating a hierarchy of commands based upon the \ncommand and the qualifier (non-terminal and terminal \nexpressions) the application could easily map many \ncommand variations to a relating tree of actions.\n\nITERATOR\t\n\t\n\t\n            Object Behavioral\nClient\n<<interface>>\nAggregate\n+createIterator( )\n<<interface>>\nIterator\n+next()\nConcrete Aggregate\n+createIterator( ) : Context\nConcreteIterator\n+next( ) : Context\nPurpose\nAllows for access to the elements of an aggregate \nobject without allowing access to its underlying \nrepresentation.\nUse \nWhen\n•\t Access to elements is needed without access to \nthe entire representation.\n•\t Multiple or concurrent traversals of the \nelements are needed.\n•\t A uniform interface for traversal is needed.\n•\t Subtle differences exist between the \nimplementation details of various iterators.\nExample\nThe Java implementation of the iterator pattern allows \nusers to traverse various types of data sets without \nworrying about the underlying implementation of the \ncollection. Since clients simply interact with the iterator \ninterface, collections are left to define the appropriate \niterator for themselves. Some will allow full access to \nthe underlying data set while others may restrict certain \nfunctionalities, such as removing items.\nMEDIATOR\t\n\t\n\t\n            Object Behavioral\nMediator\n<<interface>>\nColleague\nConcreteMediator\nConcreteColleague\ninforms\nupdates\nPurpose\nAllows loose coupling by encapsulating the way dis­\nparate sets of objects interact and communicate with \neach other. Allows for the actions of each object set to \nvary independently of one another.\nUse \nWhen\n•\t Communication between sets of objects is well \ndefined and complex.\n•\t Too many relationships exist and common point \nof control or communication is needed.\nExample\nMailing list software keeps track of who is signed up \nto the mailing list and provides a single point of access \nthrough which any one person can communicate with \nthe entire list. Without a mediator implementation a \nperson wanting to send a message to the group would \nhave to constantly keep track of who was signed \nup and who was not. By implementing the mediator \npattern the system is able to receive messages from \nany point then determine which recipients to forward \nthe message on to, without the sender of the message \nhaving to be concerned with the actual recipient list.\n\nMEMENTO\t\n\t\n\t\n            Object Behavioral \nCaretaker\nMemento\n-state\nOriginator\n-state\n+setMemento(in m : Memento)\n+createMem ento()\nPurpose\nAllows for capturing and externalizing an object’s \ninternal state so that it can be restored later, all \nwithout violating encapsulation.\nUse \nWhen\n•\t The internal state of an object must be saved \nand restored at a later time.\n•\t Internal state cannot be exposed by interfaces \nwithout exposing implementation.\n•\t Encapsulation boundaries must be preserved.\nExample\nUndo functionality can nicely be implemented using \nthe memento pattern. By serializing and deserializing \nthe state of an object before the change occurs we \ncan preserve a snapshot of it that can later be restored \nshould the user choose to undo the operation.\nOBSERVER\t\n\t\n\t\n            Object Behavioral \nnotifies\nobserves\n<<interface>>\nObserver\n+update( )\nConcreteSubject\n-subjectState\nConcreteObserver\n-observerState\n+update()\n<<interface>>\nSubject\n+attach(in o : Observer)\n+detach(in o : Observer)\n+notify()\nPurpose\nLets one or more objects be notified of state changes \nin other objects within the system.\nUse \nWhen\n•\t State changes in one or more objects should \ntrigger behavior in other objects\n•\t\nBroadcasting capabilities are required.\n•\t An understanding exists that objects will be \nblind to the expense of notification.\nExample\nThis pattern can be found in almost every GUI \nenvironment. When buttons, text, and other fields \nare placed in applications the application typically \nregisters as a listener for those controls. When a user \ntriggers an event, such as clicking a button, the control \niterates through its registered observers and sends a \nnotification to each.\n\n STATE\t\t\n\t\n                           Object Behavioral \n<<interface>>\nState\n+handle( )\nContext\n+request( )\n◆\nConcreteState 1\n+handle( )\nConcreteState 2\n+handle( )\n\nPurpose\nTies object circumstances to its behavior, allowing \nthe object to behave in different ways based upon \nits internal state.\nUse \nWhen\n•\t The behavior of an object should be influenced \nby its state.\n•\t Complex conditions tie object behavior to \nits state.\n•\t Transitions between states need to be explicit.\nExample\nAn email object can have various states, all of \nwhich will change how the object handles different \nfunctions. If the state is “not sent” then the call to \nsend() is going to send the message while a call \nto recallMessage() will either throw an error or do \nnothing. However, if the state is “sent” then the call to \nsend() would either throw an error or do nothing while \nthe call to recallMessage() would attempt to send a \nrecall notification to recipients. To avoid conditional \nstatements in most or all methods there would be \nmultiple state objects that handle the implementation \nwith respect to their particular state. The calls within \nthe Email object would then be delegated down to the \nappropriate state object for handling.\n\nSTRATEGY\t\n\t\n\t\n            Object Behavioral \nContext\n◆\nConcreteStrategyA\n+execute( )\nConcreteStrategyB\n+execute( )\n<<interface>>\nStrategy\n+execute( )\nPurpose\nDefines a set of encapsulated algorithms that can be \nswapped to carry out a specific behavior.\nUse \nWhen\n•\t The only difference between many related classes is \ntheir behavior.\n•\t\nMultiple versions or variations of an algorithm are \nrequired.\n•\t Algorithms access or utilize data that calling code \nshouldn’t be exposed to.\n•\t The behavior of a class should be defined at runtime.\n•\t Conditional statements are complex and hard to \nmaintain.\nExample\nWhen importing data into a new system different \nvalidation algorithms may be run based on the data \nset. By configuring the import to utilize strategies the \nconditional logic to determine what validation set to \nrun can be removed and the import can be decoupled \nfrom the actual validation code. This will allow us to \ndynamically call one or more strategies during the import.\n\n TEMPLATE METHOD\t\n\t\n            Object Behavioral\nConcreteClass\n+subMethod( )\nAbstractClass\n+templateMethod( )\n#subMethod( )\nPurpose\nIdentifies the framework of an algorithm, allowing \nimplementing classes to define the actual behavior.\nUse \nWhen\n•\t A single abstract implementation of an algorithm is \nneeded.\n•\t Common behavior among subclasses should be \nlocalized to a common class.\n•\t\nParent classes should be able to uniformly invoke \nbehavior in their subclasses.\n•\t\nMost or all subclasses need to implement the \nbehavior.\nExample\nA parent class, InstantMessage, will likely have all \nthe methods required to handle sending a message. \nHowever, the actual serialization of the data to send \nmay vary depending on the implementation. A \nvideo message and a plain text message will require \ndifferent algorithms in order to serialize the data \ncorrectly. Subclassaes of InstantMessage can provide \ntheir own implementation of the serialization method, \nallowing the parent class to work with them without \nunderstanding their implementation details.\n3\nDZONE.COM  |  © DZONE, INC.\nDESIGN PATTERNS\nVISITOR\t\n\t\n\t\n            Object Behavioral \n<<interface>>\nElement\n+accept(in v : Visitor)\nConcreteElementA\n+accept(in v : Visitor)\nConcreteElementB\n+accept(in v : Visitor)\n<<interface>>\nVisitor\n+visitElementA(in a : ConcreteElementA)\n+visitElementB(in b : ConcreteElementB)\nClient\nConcreteVisitor\n+visitElementA(in a : ConcreteElementA)\n+visitElementB(in b : ConcreteElementB)\n\n\nPurpose\nAllows for one or more operations to be applied to a \nset of objects at runtime, decoupling the operations \nfrom the object structure.\nUse \nWhen\n•\t An object structure must have many unrelated \noperations performedupon it.\n•\t The object structure can’t change but operations \nperformed on it can.\n•\t Operations must be performed on the concrete \nclasses of an object structure.\n•\t\nExposing internal state or operations of the \nobject structure is acceptable.\n•\t Operations should be able to operate on multiple \nobject structures that implement the same \ninterface sets.\nExample\nCalculating taxes in different regions on sets of \ninvoices would require many different variations of \ncalculation logic. Implementing a visitor allows the \nlogic to be decoupled from the invoices and line \nitems. This allows the hierarchy of items to be visited \nby calculation code that can then apply the proper \nrates for the region. Changing regions is as simple as \nsubstituting a different visitor.\n\nADAPTER\t\n\t\n                   Class and Object Structural\n<<interface>>\nAdapter\n+operation( )\nClient\nConcreteAdapter\n-adaptee\n+operation()\nAdaptee\n+adaptedOperation()\n\n\nPurpose\nPermits classes with disparate interfaces to work \ntogether by creating a common object by which they \nmay communicate and interact. \nUse \nWhen\n•\t A class to be used doesn’t meet interface \nrequirements.\n•\t Complex conditions tie object behavior to\nits state.\n•\t Transitions between states need to be explicit.\nExample\nA billing application needs to interface with an \nHR application in order to exchange employee \ndata, however each has its own interface and \nimplementation for the Employee object. In addition, \nthe SSN is stored in different formats by each system. \nBy creating an adapter we can create a common \ninterface between the two applications that allows \nthem to communicate using their native objects and is \nable to transform the SSN format in the process.\n\nBRIDGE \t\n\t\n                                       Object Structural \nAbstraction\n+operation( )\n◆\nConcreteImplementorA\n+operationImp( )\nConcreteImplementorB\n+operationImp( )\n<<interface>>\nImplementor\n+operationImp( )\n\nPurpose\nDefines an abstract object structure independently of \nthe implementation object structure in order to limit \ncoupling. \nUse \nWhen\n•\t Abstractions and implementations should not be \nbound at compile time.\n•\t Abstractions and implementations should be \nindependently extensible.\n•\t Changes in the implementation of an abstraction \nshould have no impact on clients.\n•\t\nImplementation details should be hidden from \nthe client.\nExample\nThe Java Virtual Machine (JVM) has its own native \nset of functions that abstract the use of windowing, \nsystem logging, and byte code execution but the \nactual implementation of these functions is delegated \nto the operating system the JVM is running on. When \nan application instructs the JVM to render a window \nit delegates the rendering call to the concrete \nimplementation of the JVM that knows how to \ncommunicate with the operating system in order to \nrender the window.\n\nCOMPOSITE\t\n\t\n                                       Object Structural \nLeaf\n+operation( )\n<<interface>>\nComponent\nchildren\n+operation( )\n+add(in c : Component)\n+remove(in c : Component)\n+getChild(in i : int)\nComponent\n+operation( )\n+add(in c : Component)\n+remove(in c : Component)\n+getChild(in i : int)\nPurpose\nFacilitates the creation of object hierarchies where \neach object can be treated independently or as a set \nof nested objects through the same interface. \n4\nDZONE.COM  |  © DZONE, INC.\nDESIGN PATTERNS\nUse \nWhen\n•\t\nHierarchical representations of objects are needed.\n•\t Objects and compositions of objects should be \ntreated uniformly.\nExample\nSometimes the information displayed in a shopping \ncart is the product of a single item while other times it \nis an aggregation of multiple items. By implementing \nitems as composites we can treat the aggregates and \nthe items in the same way, allowing us to simply iterate \nover the tree and invoke functionality on each item. By \ncalling the getCost() method on any given node we \nwould get the cost of that item plus the cost of all child \nitems, allowing items to be uniformly treated whether \nthey were single items or groups of items.\n\n DECORATOR\t\n                       \t\n                   Object Structural \nConcreteComponent\n+operation( )\nConcreteDecorator\n-addedState\n+operation()\n+addedBehavior()\nDecorator\n+operation( )\n<<interface>>\nComponent\n+operation( )\n\n\nPurpose\nAllows for the dynamic wrapping of objects in order to \nmodify their existing responsibilities and behaviors. \nUse \nWhen\n•\t Object responsibilities and behaviors should be \ndynamically modifiable.\n•\t Concrete implementations should be decoupled \nfrom responsibilities and behaviors.\n•\t Subclassing to achieve modification is \nimpractical or impossible.\n•\t Specific functionality should not reside high in \nthe object hierarchy.\n•\t A lot of little objects surrounding a concrete \nimplementation is  acceptable.\nExample\nMany businesses set up their mail systems to take \nadvantage of decorators. When messages are sent \nfrom someone in the company to an external address \nthe mail server decorates the original message with \ncopyright and confidentiality information. As long as \nthe message remains internal the information is not \nattached. This decoration allows the message itself to \nremain unchanged until a runtime decision is made to \nwrap the message with additional information.\n\n\n FACADE\t\n\t\n                                       Object Structural\n\nComplex System\nFacade\nPurpose\nSupplies a single interface to a set of interfaces within \na system. \nUse \nWhen\n•\t A simple interface is needed to provide access to \na complex system.\n•\t There are many dependencies between system \nimplementations and clients.\n•\t Systems and subsystems should be layered.\nExample\nBy exposing a set of functionalities through a web \nservice the client code needs to only worry about \nthe simple interface being exposed to them and not \nthe complex relationships that may or may not exist \nbehind the web service layer. A single web service \ncall to update a system with new data may actually \ninvolve communication with a number of databases \nand systems, however this detail is hidden due to the \nimplementation of the façade pattern.\n\n FLYWEIGHT\t\n\t\n                                       Object Structural\n\nClient\nFlyweightFactory\n+getFlyweight(in key)\n◆\nConcreteFlyweight\n-intrinsicState\n+operation(in extrinsicState)\nUnsharedConcreteFlyweight\n-allState\n+operation(in extrinsicState)\n<<interface>>\nFlyweight\n+operation( in extrinsicState)\n\nPurpose\nFacilitates the reuse of many fine grained objects, \nmaking the utilization of large numbers of objects \nmore efficient. \nUse \nWhen\n•\t\nMany like objects are used and storage cost is high.\n•\t The majority of each object’s state can be made \nextrinsic.\n•\t A few shared objects can replace many unshared \nones.\n•\t The identity of each object does not matter.\nExample\nSystems that allow users to define their own application \nflows and layouts often have a need to keep track of \nlarge numbers of fields, pages, and other items that are \nalmost identical to each other. By making these items \ninto flyweights all instances of each object can share \nthe intrinsic state while keeping the extrinsic state \nseparate. The intrinsic state would store the shared \nproperties, such as how a textbox looks, how much data \nit can hold, and what events it exposes. The extrinsic \nstate would store the unshared properties, such as \nwhere the item belongs, how to react to a user click, \nand how to handle events.\n\n PROXY\t\n\t\n                                       Object Structural\n\nClient\n<<interface>>\nSubject\n+request( )\nrepresents\nRealSubject\n+request( )\nProxy\n+request( )\n5\nDZONE.COM  |  © DZONE, INC.\nDESIGN PATTERNS\nPurpose\nAllows for object level access control by acting as a \npass through entity or a placeholder object. \nUse \nWhen\n•\t The object being represented is external to the \nsystem.\n•\t Objects need to be created on demand.\n•\t Access control for the original object is required.\n•\t Added functionality is required when an object is \naccessed.\nExample\nLedger applications often provide a way for users to \nreconcile their bank statements with their ledger data \non demand, automating much of the process. The \nactual operation of communicating with a third party is \na relatively expensive operation that should be limited. \nBy using a proxy to represent the communications \nobject we can limit the number of times or the intervals \nthe communication is invoked. In addition, we can wrap \nthe complex instantiation of the communication object \ninside the proxy class, decoupling calling code from the \nimplementation details.\n\nABSRACT FACTORY \t \t\n             Object Creational \n<<interface>>\nAbstractFactory\n+createProductA( )\n+createProductB( )\n<<interface>>\nAbstractProduct\nClient\nConcreteFactory\n+createProductA( )\n+createProductB( )\nConcreteProduct\n\n\nPurpose\nProvide an interface that delegates creation calls \nto one or more concrete classes in order to deliver \nspecific objects. \nUse \nWhen\n•\t The creation of objects should be independent of \nthe system utilizing them.\n•\t Systems should be capable of using multiple \nfamilies of objects.\n•\t\nFamilies of objects must be used together.\n•\t\nLibraries must be published without exposing \nimplementation details.\n•\t Concrete classes should be decoupled from clients.\nExample\nEmail editors will allow for editing in multiple formats \nincluding plain text, rich text, and HTML. Depending \non the format being used, different objects will need \nto be created. If the message is plain text then there \ncould be a body object that represented just plain \ntext and an attachment object that simply encrypted \nthe attachment into Base64. If the message is HTML \nthen the body object would represent HTML encoded \ntext and the attachment object would allow for inline \nrepresentation and a standard attachment. By utilizing \nan abstract factory for creation we can then ensure that \nthe appropriate object sets are created based upon the \nstyle of email that is being sent.\n\n BUILDER\t\n\t\n           \t              Object Creational \nDirector\n+construct( )\n◆\n<<interface>>\nBuilder\n+buildPart( )\nConcreteBuilder\n+buildPart( )\n+getResult( )\nPurpose\nAllows for the dynamic creation of objects based upon \neasily  interchangeable algorithms. \nUse \nWhen\n•\t Object creation algorithms should be decoupled \nfrom the system.\n•\t\nMultiple representations of creation algorithms \nare required.\n•\t The addition of new creation functionality without \nchanging the core code is necessary.\n•\t\nRuntime control over the creation process \nis required.\nExample\nA file transfer application could possibly use many \ndifferent protocols to send files and the actual transfer \nobject that will be created will be directly dependent on \nthe chosen protocol. Using a builder we can determine \nthe right builder to use to instantiate the right object. If \nthe setting is FTP then the FTP builder would be used \nwhen creating the object.\n\nFACTORY METHOD\t\n\t\n             Object Creational \n<<interface>>\nProduct\nConcreteProduct\nCreator\n+factoryMethod( )\n+anOperation( )\nConcreteCreator\n+factoryMethod( )\n\nPurpose\nExposes a method for creating objects, allowing \nsubclasses to control the actual creation process.  \nUse \nWhen\n•\t A class will not know what classes it will be \nrequired to create.\n•\t Subclasses may specify what objects should \nbe created.\n•\t\nParent classes wish to defer creation to \ntheir subclasses.\nExample\nMany applications have some form of user and group \nstructure for security. When the application needs to \ncreate a user it will typically delegate the creation of \nthe user to multiple user implementations. The parent \nuser object will handle most operations for each user \nbut the subclasses will define the factory method that \nhandles the distinctions in the creation of each type of \nuser. A system may have AdminUser and StandardUser \nobjects each of which extend the User object. The \nAdminUser object may perform some extra tasks to \nensure access while the StandardUser may do the \nsame to limit access.\n\n6\nDESIGN PATTERNS\nDZONE.COM  |  © DZONE, INC.\n7\nDESIGN PATTERNS\nPROTOTYPE\t \t\n                           Object Creational \n<<interface>>\nPrototype\n+clone( )\nClient\nConcretePrototype 1\n+clone( )\nConcretePrototype 2\n+clone( )\nPurpose\nExposes a method for creating objects, allowing \nsubclasses to control the actual creation process.   \nUse \nWhen\n•\t A class will not know what classes it will be \nrequired to create.\n•\t Subclasses may specify what objects should\nbe created.\n•\t\nParent classes wish to defer creation to \ntheir subclasses.\nExample\nMany applications have some form of user and group \nstructure for security. When the application needs to \ncreate a user it will typically delegate the creation of \nthe user to multiple user implementations. The parent \nuser object will handle most operations for each user \nbut the subclasses will define the factory method that \nhandles the distinctions in the creation of each type of \nuser. A system may have AdminUser and StandardUser \nobjects each of which extend the User object. The \nAdminUser object may perform some extra tasks to \nensure access while the StandardUser may do the \nsame to limit access.\nSINGLETON\t\n\t\n                           Object Creational\nSingleton\n-static uniqueInstance\n-singletonData\n+static instance()\n+singletonOperation()\nPurpose\nEnsures that only one instance of a class is allowed \nwithin a system.\nUse \nWhen\n•\t\nExactly one instance of a class is required.\n•\t Controlled access to a single object is necessary.\nExample\nMost languages provide some sort of system or \nenvironment object that allows the language to \ninteract with the native operating system. Since the \napplication is physically running on only one operating \nsystem there is only ever a need for a single instance \nof this system object. The singleton pattern would \nbe implemented by the language runtime to ensure \nthat only a single copy of the system object is created \nand to ensure only appropriate processes are allowed \naccess to it.\nABOUT THE AUTHOR\nJASON MCDONALD is a career software engineer, manager, and \nexecutive with over 20 years of professional software experience. \nHe comes from a software family and started writing code at the \nage of seven. He spent his high school and college years writing \ncode, though he holds a Business Administration degree. He has \nowned multiple small consulting companies, worked for large and \nsmall companies, and has spent time in engineering, architecture, \nand management roles since 2000. He lives in Charleston, \nSC with his wife and three children. Website: mcdonaldland.\ninfo; Twitter: @jason_mc_donald; LinkedIn: linkedin.com/in/\njasonshaunmcdonald\nCapturing a wealth of experi­\nence about the design of \nobject-oriented software, four \ntop-notch designers present a \ncatalog of simpleand succinct \nsolutions to commonly occurring design \nproblems. Previously undocumented, these \n23 patterns allow designers to create more \nflexible, elegant, and ultimately reusable \ndesigns without having to rediscover the \ndesign solutions themselves. \nRECOMMENDED BOOK\nDZONE, INC.\n150 PRESTON EXECUTIVE DR.\nCARY, NC 27513\n888.678.0399\n919.678.0300\nCopyright © 2017 DZone, Inc. All rights reserved. No part of this publication may be reproduced, stored in a retrieval \nsystem, or transmitted, in any form or by means electronic, mechanical, photocopying, or otherwise, without prior \nwritten permission of the publisher. \nDZone communities deliver over 6 million pages each month to more than 3.3 million \nsoftware developers, architects and decision makers. DZone offers something for \neveryone, including news, tutorials, cheat sheets, research guides, feature articles, \nsource code and more. \n\"DZone is a developer's dream,\" says PC Magazine.\nREFCARDZ FEEDBACK\nWELCOME\nrefcardz@dzone.com \nSPONSORSHIP\nOPPORTUNITIES \nsales@dzone.com",
      "page_count": 7,
      "pages": [
        {
          "page": 1,
          "text": "8\nBROUGHT TO YOU IN PARTNERSHIP WITH\n  Chain of Responsibility\n  Command\n  Interpreter\n  Iterator\n  Mediator\n  Observer\nC O N T E N T S\nDZONE.COM  |  © DZONE, INC.\nVISIT DZONE.COM/REFCARDZ FOR MORE!\nDesign Patterns\nBY JASON MCDONALD\nABOUT DESIGN PATTERNS\nThis Design Patterns refcard provides a quick reference to the original \n23 Gang of Four (GoF) design patterns, as listed in \nthe book Design Patterns: Elements of Reusable Object-Oriented \nSoftware. Each pattern includes class diagrams, explanation, usage \ninformation, and a real world example.\nCreational Patterns: Used to construct objects such that they \ncan be decoupled from their implementing system.\nStructural Patterns: Used to form large object \nstructures between many disparate objects.\nBehavioral Patterns: Used to manage algorithms, \nrelationships, and responsibilities between objects.\n\nObject Scope: Deals with object relationships that can \nbe changed at runtime.\nClass Scope: Deals with class relationships that can be changed at \ncompile time.\nC  Abstract Factory\nS  Adapter\nS  Bridge\nC  Builder\nB  Chain of  \n        Responsibility\nB  Command\nS  Composite\nS \t Decorator\nS \t Facade\nC \t Factory Method\nS \t Flyweight\nB \t Interpreter\nB \t Iterator\nB \t Mediator\nB \t Memento\nC \t Prototype\nS \t Proxy\nB \t Observer\nC \t Singleton\nB \t State\nB \t Strategy\nB \t Template Method\nB \t Visitor\nCHAIN OF RESPONSIBILITY \t             Object Behavioral\nsuccessor\nClient\n<<interface>>\nHandler\n+handlerequest()\nConcreteHandler 1\n+handlerequest()\nConcreteHandler 2\n+handlerequest()\nPurpose\nGives more than one object an opportunity to handle a \nrequest by linking receiving objects together.\nUse \nWhen\n•\t\nMultiple objects may handle a request and the \nhandler doesn’t have to be a specific object.\n•\t A set of objects should be able to handle a \nrequest with the handler determined at runtime\n•\t A request not being handled is an acceptable \npotential outcome.\nExample\nException handling in some languages implements \nthis pattern. When an exception is thrown in a \nmethod the runtime checks to see if the method has \na mechanism to handle the exception or if it should \nbe passed up the call stack. When passed up the call \nstack the process repeats until code to handle the \nexception is encountered or until there are no more \nparent objects to hand the request to.\nCOMMAND\t\n\t\n\t\n            Object Behavioral\nReceiver\nInvoker\nCommand\n+execute()\nClient\nConcreteCommand\n+execute()\nPurpose\nEncapsulates a request allowing it to be treated as \nan object. This allows the request to be handled \nin traditionally object based relationships such as \nqueuing and callbacks.\nUse \nWhen\n•\t You need callback functionality.\n•\t Requests need to be handled at variant times or \nin variant orders.\n•\t A history of requests is needed.\n•\t The invoker should be decoupled from the \nobject handling the invocation.\nExample\nJob queues are widely used to facilitate the \nasynchronous processing of algorithms. By utilizing \nthe command pattern the functionality to be \nexecuted can be given to a job queue for processing \nwithout any need for the queue to have knowledge \nof the actual implementation it is invoking. The \ncommand object that is enqueued implements \nits particular algorithm within the confines of the \ninterface the queue is expecting.",
          "char_count": 3291,
          "ocr_used": false
        },
        {
          "page": 2,
          "text": "2\nDZONE.COM  |  © DZONE, INC.\nDESIGN PATTERNS\n INTERPRETER\t             \t                                  Object Behavioral \nClient\nContext\n<<interface>>\nAbstractExpression\n+interpret()\nTerminalExpression\n+interpret( ) : Context\nNonterminalExpression\n+interpret( ) : Context\n◆\nPurpose\nDefines a representation for a grammar as well as a \nmechanism to understand and act upon the grammar.\nUse \nWhen\n•\t There is grammar to interpret that can be \nrepresened as large syntax trees.\n•\t The grammar is simple.\n•\t Efficiency is not important.\n•\t Decoupling grammar from underlying \nexpressions is desired.\nExample\nText based adventures, wildly popular in the 1980’s, \nprovide a good example of this. Many had simple \ncommands, such as “step down” that allowed traversal \nof the game. These commands could be nested such \nthat it altered their meaning. For example, “go in” \nwould result in a different outcome than “go up”. By \ncreating a hierarchy of commands based upon the \ncommand and the qualifier (non-terminal and terminal \nexpressions) the application could easily map many \ncommand variations to a relating tree of actions.\n\nITERATOR\t\n\t\n\t\n            Object Behavioral\nClient\n<<interface>>\nAggregate\n+createIterator( )\n<<interface>>\nIterator\n+next()\nConcrete Aggregate\n+createIterator( ) : Context\nConcreteIterator\n+next( ) : Context\nPurpose\nAllows for access to the elements of an aggregate \nobject without allowing access to its underlying \nrepresentation.\nUse \nWhen\n•\t Access to elements is needed without access to \nthe entire representation.\n•\t Multiple or concurrent traversals of the \nelements are needed.\n•\t A uniform interface for traversal is needed.\n•\t Subtle differences exist between the \nimplementation details of various iterators.\nExample\nThe Java implementation of the iterator pattern allows \nusers to traverse various types of data sets without \nworrying about the underlying implementation of the \ncollection. Since clients simply interact with the iterator \ninterface, collections are left to define the appropriate \niterator for themselves. Some will allow full access to \nthe underlying data set while others may restrict certain \nfunctionalities, such as removing items.\nMEDIATOR\t\n\t\n\t\n            Object Behavioral\nMediator\n<<interface>>\nColleague\nConcreteMediator\nConcreteColleague\ninforms\nupdates\nPurpose\nAllows loose coupling by encapsulating the way dis­\nparate sets of objects interact and communicate with \neach other. Allows for the actions of each object set to \nvary independently of one another.\nUse \nWhen\n•\t Communication between sets of objects is well \ndefined and complex.\n•\t Too many relationships exist and common point \nof control or communication is needed.\nExample\nMailing list software keeps track of who is signed up \nto the mailing list and provides a single point of access \nthrough which any one person can communicate with \nthe entire list. Without a mediator implementation a \nperson wanting to send a message to the group would \nhave to constantly keep track of who was signed \nup and who was not. By implementing the mediator \npattern the system is able to receive messages from \nany point then determine which recipients to forward \nthe message on to, without the sender of the message \nhaving to be concerned with the actual recipient list.\n\nMEMENTO\t\n\t\n\t\n            Object Behavioral \nCaretaker\nMemento\n-state\nOriginator\n-state\n+setMemento(in m : Memento)\n+createMem ento()\nPurpose\nAllows for capturing and externalizing an object’s \ninternal state so that it can be restored later, all \nwithout violating encapsulation.\nUse \nWhen\n•\t The internal state of an object must be saved \nand restored at a later time.\n•\t Internal state cannot be exposed by interfaces \nwithout exposing implementation.\n•\t Encapsulation boundaries must be preserved.\nExample\nUndo functionality can nicely be implemented using \nthe memento pattern. By serializing and deserializing \nthe state of an object before the change occurs we \ncan preserve a snapshot of it that can later be restored \nshould the user choose to undo the operation.",
          "char_count": 4071,
          "ocr_used": false
        },
        {
          "page": 3,
          "text": "OBSERVER\t\n\t\n\t\n            Object Behavioral \nnotifies\nobserves\n<<interface>>\nObserver\n+update( )\nConcreteSubject\n-subjectState\nConcreteObserver\n-observerState\n+update()\n<<interface>>\nSubject\n+attach(in o : Observer)\n+detach(in o : Observer)\n+notify()\nPurpose\nLets one or more objects be notified of state changes \nin other objects within the system.\nUse \nWhen\n•\t State changes in one or more objects should \ntrigger behavior in other objects\n•\t\nBroadcasting capabilities are required.\n•\t An understanding exists that objects will be \nblind to the expense of notification.\nExample\nThis pattern can be found in almost every GUI \nenvironment. When buttons, text, and other fields \nare placed in applications the application typically \nregisters as a listener for those controls. When a user \ntriggers an event, such as clicking a button, the control \niterates through its registered observers and sends a \nnotification to each.\n\n STATE\t\t\n\t\n                           Object Behavioral \n<<interface>>\nState\n+handle( )\nContext\n+request( )\n◆\nConcreteState 1\n+handle( )\nConcreteState 2\n+handle( )\n\nPurpose\nTies object circumstances to its behavior, allowing \nthe object to behave in different ways based upon \nits internal state.\nUse \nWhen\n•\t The behavior of an object should be influenced \nby its state.\n•\t Complex conditions tie object behavior to \nits state.\n•\t Transitions between states need to be explicit.\nExample\nAn email object can have various states, all of \nwhich will change how the object handles different \nfunctions. If the state is “not sent” then the call to \nsend() is going to send the message while a call \nto recallMessage() will either throw an error or do \nnothing. However, if the state is “sent” then the call to \nsend() would either throw an error or do nothing while \nthe call to recallMessage() would attempt to send a \nrecall notification to recipients. To avoid conditional \nstatements in most or all methods there would be \nmultiple state objects that handle the implementation \nwith respect to their particular state. The calls within \nthe Email object would then be delegated down to the \nappropriate state object for handling.\n\nSTRATEGY\t\n\t\n\t\n            Object Behavioral \nContext\n◆\nConcreteStrategyA\n+execute( )\nConcreteStrategyB\n+execute( )\n<<interface>>\nStrategy\n+execute( )\nPurpose\nDefines a set of encapsulated algorithms that can be \nswapped to carry out a specific behavior.\nUse \nWhen\n•\t The only difference between many related classes is \ntheir behavior.\n•\t\nMultiple versions or variations of an algorithm are \nrequired.\n•\t Algorithms access or utilize data that calling code \nshouldn’t be exposed to.\n•\t The behavior of a class should be defined at runtime.\n•\t Conditional statements are complex and hard to \nmaintain.\nExample\nWhen importing data into a new system different \nvalidation algorithms may be run based on the data \nset. By configuring the import to utilize strategies the \nconditional logic to determine what validation set to \nrun can be removed and the import can be decoupled \nfrom the actual validation code. This will allow us to \ndynamically call one or more strategies during the import.\n\n TEMPLATE METHOD\t\n\t\n            Object Behavioral\nConcreteClass\n+subMethod( )\nAbstractClass\n+templateMethod( )\n#subMethod( )\nPurpose\nIdentifies the framework of an algorithm, allowing \nimplementing classes to define the actual behavior.\nUse \nWhen\n•\t A single abstract implementation of an algorithm is \nneeded.\n•\t Common behavior among subclasses should be \nlocalized to a common class.\n•\t\nParent classes should be able to uniformly invoke \nbehavior in their subclasses.\n•\t\nMost or all subclasses need to implement the \nbehavior.\nExample\nA parent class, InstantMessage, will likely have all \nthe methods required to handle sending a message. \nHowever, the actual serialization of the data to send \nmay vary depending on the implementation. A \nvideo message and a plain text message will require \ndifferent algorithms in order to serialize the data \ncorrectly. Subclassaes of InstantMessage can provide \ntheir own implementation of the serialization method, \nallowing the parent class to work with them without \nunderstanding their implementation details.\n3\nDZONE.COM  |  © DZONE, INC.\nDESIGN PATTERNS",
          "char_count": 4248,
          "ocr_used": false
        },
        {
          "page": 4,
          "text": "VISITOR\t\n\t\n\t\n            Object Behavioral \n<<interface>>\nElement\n+accept(in v : Visitor)\nConcreteElementA\n+accept(in v : Visitor)\nConcreteElementB\n+accept(in v : Visitor)\n<<interface>>\nVisitor\n+visitElementA(in a : ConcreteElementA)\n+visitElementB(in b : ConcreteElementB)\nClient\nConcreteVisitor\n+visitElementA(in a : ConcreteElementA)\n+visitElementB(in b : ConcreteElementB)\n\n\nPurpose\nAllows for one or more operations to be applied to a \nset of objects at runtime, decoupling the operations \nfrom the object structure.\nUse \nWhen\n•\t An object structure must have many unrelated \noperations performedupon it.\n•\t The object structure can’t change but operations \nperformed on it can.\n•\t Operations must be performed on the concrete \nclasses of an object structure.\n•\t\nExposing internal state or operations of the \nobject structure is acceptable.\n•\t Operations should be able to operate on multiple \nobject structures that implement the same \ninterface sets.\nExample\nCalculating taxes in different regions on sets of \ninvoices would require many different variations of \ncalculation logic. Implementing a visitor allows the \nlogic to be decoupled from the invoices and line \nitems. This allows the hierarchy of items to be visited \nby calculation code that can then apply the proper \nrates for the region. Changing regions is as simple as \nsubstituting a different visitor.\n\nADAPTER\t\n\t\n                   Class and Object Structural\n<<interface>>\nAdapter\n+operation( )\nClient\nConcreteAdapter\n-adaptee\n+operation()\nAdaptee\n+adaptedOperation()\n\n\nPurpose\nPermits classes with disparate interfaces to work \ntogether by creating a common object by which they \nmay communicate and interact. \nUse \nWhen\n•\t A class to be used doesn’t meet interface \nrequirements.\n•\t Complex conditions tie object behavior to\nits state.\n•\t Transitions between states need to be explicit.\nExample\nA billing application needs to interface with an \nHR application in order to exchange employee \ndata, however each has its own interface and \nimplementation for the Employee object. In addition, \nthe SSN is stored in different formats by each system. \nBy creating an adapter we can create a common \ninterface between the two applications that allows \nthem to communicate using their native objects and is \nable to transform the SSN format in the process.\n\nBRIDGE \t\n\t\n                                       Object Structural \nAbstraction\n+operation( )\n◆\nConcreteImplementorA\n+operationImp( )\nConcreteImplementorB\n+operationImp( )\n<<interface>>\nImplementor\n+operationImp( )\n\nPurpose\nDefines an abstract object structure independently of \nthe implementation object structure in order to limit \ncoupling. \nUse \nWhen\n•\t Abstractions and implementations should not be \nbound at compile time.\n•\t Abstractions and implementations should be \nindependently extensible.\n•\t Changes in the implementation of an abstraction \nshould have no impact on clients.\n•\t\nImplementation details should be hidden from \nthe client.\nExample\nThe Java Virtual Machine (JVM) has its own native \nset of functions that abstract the use of windowing, \nsystem logging, and byte code execution but the \nactual implementation of these functions is delegated \nto the operating system the JVM is running on. When \nan application instructs the JVM to render a window \nit delegates the rendering call to the concrete \nimplementation of the JVM that knows how to \ncommunicate with the operating system in order to \nrender the window.\n\nCOMPOSITE\t\n\t\n                                       Object Structural \nLeaf\n+operation( )\n<<interface>>\nComponent\nchildren\n+operation( )\n+add(in c : Component)\n+remove(in c : Component)\n+getChild(in i : int)\nComponent\n+operation( )\n+add(in c : Component)\n+remove(in c : Component)\n+getChild(in i : int)\nPurpose\nFacilitates the creation of object hierarchies where \neach object can be treated independently or as a set \nof nested objects through the same interface. \n4\nDZONE.COM  |  © DZONE, INC.\nDESIGN PATTERNS",
          "char_count": 3977,
          "ocr_used": false
        },
        {
          "page": 5,
          "text": "Use \nWhen\n•\t\nHierarchical representations of objects are needed.\n•\t Objects and compositions of objects should be \ntreated uniformly.\nExample\nSometimes the information displayed in a shopping \ncart is the product of a single item while other times it \nis an aggregation of multiple items. By implementing \nitems as composites we can treat the aggregates and \nthe items in the same way, allowing us to simply iterate \nover the tree and invoke functionality on each item. By \ncalling the getCost() method on any given node we \nwould get the cost of that item plus the cost of all child \nitems, allowing items to be uniformly treated whether \nthey were single items or groups of items.\n\n DECORATOR\t\n                       \t\n                   Object Structural \nConcreteComponent\n+operation( )\nConcreteDecorator\n-addedState\n+operation()\n+addedBehavior()\nDecorator\n+operation( )\n<<interface>>\nComponent\n+operation( )\n\n\nPurpose\nAllows for the dynamic wrapping of objects in order to \nmodify their existing responsibilities and behaviors. \nUse \nWhen\n•\t Object responsibilities and behaviors should be \ndynamically modifiable.\n•\t Concrete implementations should be decoupled \nfrom responsibilities and behaviors.\n•\t Subclassing to achieve modification is \nimpractical or impossible.\n•\t Specific functionality should not reside high in \nthe object hierarchy.\n•\t A lot of little objects surrounding a concrete \nimplementation is  acceptable.\nExample\nMany businesses set up their mail systems to take \nadvantage of decorators. When messages are sent \nfrom someone in the company to an external address \nthe mail server decorates the original message with \ncopyright and confidentiality information. As long as \nthe message remains internal the information is not \nattached. This decoration allows the message itself to \nremain unchanged until a runtime decision is made to \nwrap the message with additional information.\n\n\n FACADE\t\n\t\n                                       Object Structural\n\nComplex System\nFacade\nPurpose\nSupplies a single interface to a set of interfaces within \na system. \nUse \nWhen\n•\t A simple interface is needed to provide access to \na complex system.\n•\t There are many dependencies between system \nimplementations and clients.\n•\t Systems and subsystems should be layered.\nExample\nBy exposing a set of functionalities through a web \nservice the client code needs to only worry about \nthe simple interface being exposed to them and not \nthe complex relationships that may or may not exist \nbehind the web service layer. A single web service \ncall to update a system with new data may actually \ninvolve communication with a number of databases \nand systems, however this detail is hidden due to the \nimplementation of the façade pattern.\n\n FLYWEIGHT\t\n\t\n                                       Object Structural\n\nClient\nFlyweightFactory\n+getFlyweight(in key)\n◆\nConcreteFlyweight\n-intrinsicState\n+operation(in extrinsicState)\nUnsharedConcreteFlyweight\n-allState\n+operation(in extrinsicState)\n<<interface>>\nFlyweight\n+operation( in extrinsicState)\n\nPurpose\nFacilitates the reuse of many fine grained objects, \nmaking the utilization of large numbers of objects \nmore efficient. \nUse \nWhen\n•\t\nMany like objects are used and storage cost is high.\n•\t The majority of each object’s state can be made \nextrinsic.\n•\t A few shared objects can replace many unshared \nones.\n•\t The identity of each object does not matter.\nExample\nSystems that allow users to define their own application \nflows and layouts often have a need to keep track of \nlarge numbers of fields, pages, and other items that are \nalmost identical to each other. By making these items \ninto flyweights all instances of each object can share \nthe intrinsic state while keeping the extrinsic state \nseparate. The intrinsic state would store the shared \nproperties, such as how a textbox looks, how much data \nit can hold, and what events it exposes. The extrinsic \nstate would store the unshared properties, such as \nwhere the item belongs, how to react to a user click, \nand how to handle events.\n\n PROXY\t\n\t\n                                       Object Structural\n\nClient\n<<interface>>\nSubject\n+request( )\nrepresents\nRealSubject\n+request( )\nProxy\n+request( )\n5\nDZONE.COM  |  © DZONE, INC.\nDESIGN PATTERNS",
          "char_count": 4270,
          "ocr_used": false
        },
        {
          "page": 6,
          "text": "Purpose\nAllows for object level access control by acting as a \npass through entity or a placeholder object. \nUse \nWhen\n•\t The object being represented is external to the \nsystem.\n•\t Objects need to be created on demand.\n•\t Access control for the original object is required.\n•\t Added functionality is required when an object is \naccessed.\nExample\nLedger applications often provide a way for users to \nreconcile their bank statements with their ledger data \non demand, automating much of the process. The \nactual operation of communicating with a third party is \na relatively expensive operation that should be limited. \nBy using a proxy to represent the communications \nobject we can limit the number of times or the intervals \nthe communication is invoked. In addition, we can wrap \nthe complex instantiation of the communication object \ninside the proxy class, decoupling calling code from the \nimplementation details.\n\nABSRACT FACTORY \t \t\n             Object Creational \n<<interface>>\nAbstractFactory\n+createProductA( )\n+createProductB( )\n<<interface>>\nAbstractProduct\nClient\nConcreteFactory\n+createProductA( )\n+createProductB( )\nConcreteProduct\n\n\nPurpose\nProvide an interface that delegates creation calls \nto one or more concrete classes in order to deliver \nspecific objects. \nUse \nWhen\n•\t The creation of objects should be independent of \nthe system utilizing them.\n•\t Systems should be capable of using multiple \nfamilies of objects.\n•\t\nFamilies of objects must be used together.\n•\t\nLibraries must be published without exposing \nimplementation details.\n•\t Concrete classes should be decoupled from clients.\nExample\nEmail editors will allow for editing in multiple formats \nincluding plain text, rich text, and HTML. Depending \non the format being used, different objects will need \nto be created. If the message is plain text then there \ncould be a body object that represented just plain \ntext and an attachment object that simply encrypted \nthe attachment into Base64. If the message is HTML \nthen the body object would represent HTML encoded \ntext and the attachment object would allow for inline \nrepresentation and a standard attachment. By utilizing \nan abstract factory for creation we can then ensure that \nthe appropriate object sets are created based upon the \nstyle of email that is being sent.\n\n BUILDER\t\n\t\n           \t              Object Creational \nDirector\n+construct( )\n◆\n<<interface>>\nBuilder\n+buildPart( )\nConcreteBuilder\n+buildPart( )\n+getResult( )\nPurpose\nAllows for the dynamic creation of objects based upon \neasily  interchangeable algorithms. \nUse \nWhen\n•\t Object creation algorithms should be decoupled \nfrom the system.\n•\t\nMultiple representations of creation algorithms \nare required.\n•\t The addition of new creation functionality without \nchanging the core code is necessary.\n•\t\nRuntime control over the creation process \nis required.\nExample\nA file transfer application could possibly use many \ndifferent protocols to send files and the actual transfer \nobject that will be created will be directly dependent on \nthe chosen protocol. Using a builder we can determine \nthe right builder to use to instantiate the right object. If \nthe setting is FTP then the FTP builder would be used \nwhen creating the object.\n\nFACTORY METHOD\t\n\t\n             Object Creational \n<<interface>>\nProduct\nConcreteProduct\nCreator\n+factoryMethod( )\n+anOperation( )\nConcreteCreator\n+factoryMethod( )\n\nPurpose\nExposes a method for creating objects, allowing \nsubclasses to control the actual creation process.  \nUse \nWhen\n•\t A class will not know what classes it will be \nrequired to create.\n•\t Subclasses may specify what objects should \nbe created.\n•\t\nParent classes wish to defer creation to \ntheir subclasses.\nExample\nMany applications have some form of user and group \nstructure for security. When the application needs to \ncreate a user it will typically delegate the creation of \nthe user to multiple user implementations. The parent \nuser object will handle most operations for each user \nbut the subclasses will define the factory method that \nhandles the distinctions in the creation of each type of \nuser. A system may have AdminUser and StandardUser \nobjects each of which extend the User object. The \nAdminUser object may perform some extra tasks to \nensure access while the StandardUser may do the \nsame to limit access.\n\n6\nDESIGN PATTERNS\nDZONE.COM  |  © DZONE, INC.",
          "char_count": 4395,
          "ocr_used": false
        },
        {
          "page": 7,
          "text": "7\nDESIGN PATTERNS\nPROTOTYPE\t \t\n                           Object Creational \n<<interface>>\nPrototype\n+clone( )\nClient\nConcretePrototype 1\n+clone( )\nConcretePrototype 2\n+clone( )\nPurpose\nExposes a method for creating objects, allowing \nsubclasses to control the actual creation process.   \nUse \nWhen\n•\t A class will not know what classes it will be \nrequired to create.\n•\t Subclasses may specify what objects should\nbe created.\n•\t\nParent classes wish to defer creation to \ntheir subclasses.\nExample\nMany applications have some form of user and group \nstructure for security. When the application needs to \ncreate a user it will typically delegate the creation of \nthe user to multiple user implementations. The parent \nuser object will handle most operations for each user \nbut the subclasses will define the factory method that \nhandles the distinctions in the creation of each type of \nuser. A system may have AdminUser and StandardUser \nobjects each of which extend the User object. The \nAdminUser object may perform some extra tasks to \nensure access while the StandardUser may do the \nsame to limit access.\nSINGLETON\t\n\t\n                           Object Creational\nSingleton\n-static uniqueInstance\n-singletonData\n+static instance()\n+singletonOperation()\nPurpose\nEnsures that only one instance of a class is allowed \nwithin a system.\nUse \nWhen\n•\t\nExactly one instance of a class is required.\n•\t Controlled access to a single object is necessary.\nExample\nMost languages provide some sort of system or \nenvironment object that allows the language to \ninteract with the native operating system. Since the \napplication is physically running on only one operating \nsystem there is only ever a need for a single instance \nof this system object. The singleton pattern would \nbe implemented by the language runtime to ensure \nthat only a single copy of the system object is created \nand to ensure only appropriate processes are allowed \naccess to it.\nABOUT THE AUTHOR\nJASON MCDONALD is a career software engineer, manager, and \nexecutive with over 20 years of professional software experience. \nHe comes from a software family and started writing code at the \nage of seven. He spent his high school and college years writing \ncode, though he holds a Business Administration degree. He has \nowned multiple small consulting companies, worked for large and \nsmall companies, and has spent time in engineering, architecture, \nand management roles since 2000. He lives in Charleston, \nSC with his wife and three children. Website: mcdonaldland.\ninfo; Twitter: @jason_mc_donald; LinkedIn: linkedin.com/in/\njasonshaunmcdonald\nCapturing a wealth of experi­\nence about the design of \nobject-oriented software, four \ntop-notch designers present a \ncatalog of simpleand succinct \nsolutions to commonly occurring design \nproblems. Previously undocumented, these \n23 patterns allow designers to create more \nflexible, elegant, and ultimately reusable \ndesigns without having to rediscover the \ndesign solutions themselves. \nRECOMMENDED BOOK\nDZONE, INC.\n150 PRESTON EXECUTIVE DR.\nCARY, NC 27513\n888.678.0399\n919.678.0300\nCopyright © 2017 DZone, Inc. All rights reserved. No part of this publication may be reproduced, stored in a retrieval \nsystem, or transmitted, in any form or by means electronic, mechanical, photocopying, or otherwise, without prior \nwritten permission of the publisher. \nDZone communities deliver over 6 million pages each month to more than 3.3 million \nsoftware developers, architects and decision makers. DZone offers something for \neveryone, including news, tutorials, cheat sheets, research guides, feature articles, \nsource code and more. \n\"DZone is a developer's dream,\" says PC Magazine.\nREFCARDZ FEEDBACK\nWELCOME\nrefcardz@dzone.com \nSPONSORSHIP\nOPPORTUNITIES \nsales@dzone.com",
          "char_count": 3789,
          "ocr_used": false
        }
      ],
      "metadata": {
        "format": "PDF 1.4",
        "title": "",
        "author": "",
        "subject": "",
        "keywords": "",
        "creator": "",
        "producer": "",
        "creationDate": "",
        "modDate": "",
        "trapped": "",
        "encryption": null
      },
      "char_count": 28040,
      "word_count": 4128,
      "ocr_pages_count": 0,
      "error": null,
      "file_id": "141KLeSjIjHPKcnrSE5uAccytGLAQn8V5",
      "filename": "Design Patterns.pdf",
      "filepath": "downloads/Design Patterns.pdf",
      "download_link": "https://drive.google.com/uc?export=download&id=141KLeSjIjHPKcnrSE5uAccytGLAQn8V5"
    },
    {
      "success": true,
      "text": "DevOps Complete Package \nLAMP \n===== \nSo when it comes to LAMP we are talking about { Linux, Apache, MySql & PHP } \n \nLAMP is an open source Web development platform that uses Linux as the operating system, Apache as the Web \nserver, MySQL as the relational database management system and PHP as the object-oriented scripting language. \n \nBecause the platform has four layers, LAMP is sometimes referred to as a LAMP stack. Stacks can be built on different \noperating systems. \n \nThe same setup in windows is called WAMP \nThe same setup in mac is called MAMP \n \nAdvantages Of LAMP \n================== \n \n \nOpen Source \n \nEasy to code with PHP \n \nEasy to deploy an application \n \nDevelop locally \n \nCheap Hosting \n \nEasy to build CMS application \no \nWordpress, Drupal, Joomla, Moodle etc \n \nWeb Server \nA web server is a program which serves web pages to users in response to their requests, which are forwarded by their \ncomputers' HTTP clients(Browsers). \n \n \n \n \n \nAll computers that host websites must have web server program. \n \n \n \nPurpose of Web server \n=================== \nA web server’s main purpose is to store web site files and broadcast them over the internet for you site visitors to see. In \nessence, a web server is simply a powerful computer that stores and transmits data via the internet. \n \nWeb servers are the gateway between the average individual and the world wide web.\nLAMP Architecture \n \n \n \nLinux :: We already talked about linux \n \nApache \n======= \nAn open source web server used mostly for Unix and Linux platforms. \nIt is fast, secure and reliable. \n \nSince 1996 Apache has been the most popular web server, presently apache holds 49.5% of market share i.e, 49.5% of all \nthe websites followed by Nginx 34% and Microsoft IIS 11%. \n \nParameters for Apache (httpd) \nPackages \n \n \n \n- \n httpd  &  \nmod_ssl \nPort \n \n \n \n \n- \n 80  \n \n  \n 443 \nProtocol  \n \n \n \n-  \n http \n \n \n https \nServer Root \n \n \n \n-     \n /etc/httpd \nMain config file  \n \n- \n/etc/httpd/conf/httpd.conf  \nDocument root  \n \n-  \n/var/www/html    {all our web pages } \nLogs \n \n \n \n \n-  \n/var/log/httpd/error_log \n \n \n \n \n \n \n/var/log/httpd/access_log \n \n \nWhat version of OS you have \n======================== \n \n# cat /etc/redhat-release \n# cat /etc/os-release \n \nChecking httpd service is running or not \n================================= \n \n6.x - Whenever you have problem with httpd {troubleshooting httpd} \n \n# service httpd status \n \n# netstat -ntpl | grep 80 \n \n# ps -ef | grep httpd \n \n7.x - Whenever you have problem with httpd {troubleshooting httpd}\n# systemctl status httpd \n \n# netstat -ntpl | grep 80 \n \n# ps -ef | grep httpd \n \nNow when you are doing server configuration, we need to be very careful while doing server configuration, coz sometimes \nyou may do some typos and you are unaware why the server is not starting, so to confirm your config is correct use \nfollowing command: \n \nCheck configuration file (httpd.conf) is correct or not \n=========================================== \n \n \n# httpd -t \n \n{ 6.x and 7.x } \n \nInstalling httpd service \n=================== \n \n# rpm -qa | grep httpd \n# sudo yum -y install httpd \n \nLet’s check status of the web server: \n \n# systemctl status httpd \n \n# netstat -ntpl | grep 80 \n \n# ps -ef | grep httpd \n \nLet’s start the web server: \n# systemctl start httpd \n# systemctl status httpd \n \n# netstat -ntpl | grep 80 \n# ps -ef | grep httpd \n \n \nNow we have started the service \nIf you see # ls -l /var/www/html, you have no files in there, let’s create a index.html \n \n# cd /var/www/html \n \n# vi index.html {put some content} \n \n# vi /var/www/html/sample.php \n \n \n<?php \n \necho “Today is ” . date(“Y/m/d”) . <br>”; \necho \"Today is \" . date(\"l\"); \n \n?> \n \nhttp://ip-address/sample.php \n{ doesn’t show php rendering coz php engine was  not there } \n \n# sudo yum -y install php \n \nhttp://ip-address/sample.php \n{ php will be rendered } \n \nApache Virtual Hosting \n=================== \nAt the end of file add these lines \n \n<VirtualHost *:81> \nDocumentRoot /var/www/html/website2 \n</VirtualHost> \n \n<VirtualHost *:82> \nDocumentRoot /var/www/html/website3 \n</VirtualHost> \n \n \nNginx Server \n============\nNginx is a http server, which is used in many high traffic websites like GitHub, heroku etc. \n \nThe 3 imp features that nginx gives is: \n   Load balancing, Caching and Reverse proxying. \n    \nIn load balancing, say u have a high traffic website which  gets 1000 requests per second may be more then that, if u have \nonly one server all the 1000 requests will be severe by the single server where the response time will be decreased. \n \nSo we put more servers and distribute the load to all the servers equally. \nSo the 1000 requests will be distributed to 3 servers like 300 requests for each server assuming we got 3. \n \nReverse proxy, we are having multiple applications on the server, and u can only run one application on one port, say u \nrunning one app on port 80 now u can’t run another application on port 80 we got to use another port. { app1.com ==> \napp1.com:80 } \n \nNow we are running multiple applications we need give diff port right something like { app2.com:81 } \n \nNow I don’t want to specify port 81 in url, I want app2.com, to deal with this problem we can use nginx. \n \nNow nginx intercepts the requests and it will see that the following request is for app2.com now it routes this request to \nthat application which is running on port 81. This is what reverse proxy is. \n \nForward Proxy & Reverse Proxy \n========================== \n \nThe word \"proxy\" describes someone or something acting on behalf of someone else. \nIn the computer realm, we are talking about one server acting on the behalf of another computer. \n \nForward Proxy \n=============== \nForward Proxy: Acting on behalf of a requestor (or service consumer) \n \n\"forward proxy\" retrieves data from another web site on behalf of the original requestee. \n \nIn forward proxy we get to see 3 computers: \n \nX = your computer, or \"client\" computer on the internet \nY = the proxy web site, proxy.example.org \nZ = the web site you want to visit, www.example.net \nNormally, one would connect directly from     X    —>    \n Z. \nHowever, in some scenarios, it is better for Y —>  Z, on behalf of X, which chains as follows: X    —> Y—> Z \n \nReasons why X would want to use a forward proxy server \n==================================================== \nX is unable to access Z directly because \n \na) Someone with administration authority over X's internet connection has decided to block all access to site Z. \n \nEmployees at a large company have been wasting too much time on facebook.com, so management wants access blocked \nduring business hours. \n \nTorrent downloads can be blocked by our ISP’s so to view the torrent downloads we can use forward proxy. \n \nb) The administrator of Z has blocked X. \n \nExamples: \nThe administrator of Z has noticed hacking attempts coming from X, so the administrator has decided to block X's ip \naddress. \nZ is a forum web site.  X is spamming the forum. Z blocks X. \n \nReverse Proxy \n===============\nReverse Proxy: Acting on behalf of service/content producer \n \nIn reverse proxy we get to see 3 computers: \n \nX = your computer, or \"client\" computer on the internet \nY = the reverse proxy web site, proxy.example.com \nZ = the web site you want to visit, www.example.net \n \nNormally, one would connect directly from X --> Z \n \nHowever, in some scenarios, it is better for the administrator of Z to restrict or disallow direct access, and force visitors to \ngo through Y first. So, as before, we have data being retrieved by Y --> Z on behalf of X, which chains as follows: X --> Y --> \nZ. \n \nReasons why Z would want to setup a reverse proxy server \n============================================== \n1) Z wants to force all traffic to its web site to pass through Y first. \n \na) Z has a large web site that millions of people want to see, but a single web server cannot handle all the traffic. So Z sets \nup many servers, and puts a reverse proxy on the internet that will send users to the server closest to them when they try \nto visit Z. This is part of how the Content Distribution Network (CDN) concept works. \nExamples: \nApple Trailers uses Akamai \nJquery.com hosts its javascript files using CloudFront CDN (sample). \n \n2) The administrator of Z is worried about retaliation for content hosted on the server and does not want to expose the \nmain server directly to the public. \n \nLoad Balancing and Reverse Proxying with Nginx \n======================================== \n \nMake sure you have 3 machines setup with you: \n \nM1 IP-ADD: 104.154.22.74 \nM2 IP-ADD: 104.198.254.34 \nM3 IP-ADD: 104.198.70.78 \n \nMachine1 \n======== \n \n# yum -y install httpd \n# systemctl enable httpd \n# systemctl start httpd \n# allow port 80 in firewall \n# echo “MACHINE01” > /var/www/html/index.html \n# vi /var/www/html/index.html    { MACHINE —> 1 } \n \n \nMachine2 \n======== \n \n# yum -y install httpd \n# systemctl enable httpd \n# systemctl start httpd \n# allow port 80 in firewall \n# echo “MACHINE02” > /var/www/html/index.html \n# vi /var/www/html/index.html    { MACHINE —> 2 } \n \n \nMachine3 \n========\n# yum -y install nginx \n# systemctl enable nginx \n# systemctl start nginx \n# allow port 80 in firewall \n# vi /etc/nginx/nginx.conf    \n { delete everything under http section } \n \nI am going to define a group of web servers with directive upstream and going to give this group a name as lbmysite. \n \nUpstream defines a cluster that you can proxy requests to. It's commonly used for defining either a web server cluster for \nload balancing, or or an app server cluster for routing / load balancing. \n \nnginx.conf \n========= \nuser nginx; \nworker_processes auto; \nerror_log /var/log/nginx/error.log; \npid /run/nginx.pid; \n \n# Load dynamic modules. See /usr/share/nginx/README.dynamic. \ninclude /usr/share/nginx/modules/*.conf; \n \nevents { \n \nworker_connections 1024; \n} \n \n## remove and replace from http && remove old server block { } \n \nhttp { \n \n \nlog_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" ' \n                  \n'$status $body_bytes_sent \"$http_referer\" ' \n                  \n'\"$http_user_agent\" \"$http_x_forwarded_for\"'; \n \n \naccess_log  /var/log/nginx/access.log  main; \n \n \nsendfile         \non; \n \ntcp_nopush       on; \n \ntcp_nodelay      on; \n \nkeepalive_timeout   65; \n \ntypes_hash_max_size 2048; \n \n \ninclude          \n/etc/nginx/mime.types; \n \ndefault_type     \napplication/octet-stream; \n \n \ninclude /etc/nginx/conf.d/*.conf; \n \nupstream lbmysite { \n \nserver 104.197.127.92:90; \n \nserver 35.194.22.247; \n \n} \n \nserver { \n    \nlisten    80 default_server; \n    \nlisten    [::]:80 default_server; \n \nlocation / { \n    \nproxy_pass http://lbmysite; \n \n} \n} \n \n} \n \nDatabase \n========\nInstallation Version 5.6 \n================== \n# wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm \n \n# sudo rpm -ivh mysql-community-release-el7-5.noarch.rpm \n \n# sudo yum -y install mysql-server \n \nStart the mariadb server # sudo systemctl start mysqld \n \nSetting password # sudo mysql_secure_installation \n \nTo login to database # sudo mysql -u root -p \n \n# ps -ef | grep mysql \n# netstat -ntpl | grep 3306 \n \nHow to connect to Database? \n \n# mysql -u root -p \n \n \n \n{if db is same host} \n \n# mysql -u root -p -h <server_ip> \n \n{if db is on diff host \n \n \n \nCMS Application \n============= \n \nDownload wordpress # wget https://wordpress.org/latest.tar.gz \n \n# sudo tar xvf wordpress.tar -C /var/www/html \n \n# sudo yum -y install php php-mysql \n \nLogin to phpmyadmin, and create user a database and user called wordpress something like that. \n \nGive anything you want as username and password \nCopy and paste the code given by wordpress into wp-config.php \nChange the ownership of wordpress directory to apache:apache \n \n \n \n \n \n \n \n \n \n \nGIT\nWhy Version Control ?? \n================= \nHave you ever:  \n● Made a change to code, realised it was a mistake and wanted to revert back? \n● Lost code and didn’t have a backup of that code ? \n● Had to maintain multiple versions of a product ? \n● Wanted to see the difference between two (or more) versions of your code ? \n● Wanted to prove that a particular change in code broke application or fixed a application ? \n● Wanted to review the history of some code ? \n● Wanted to submit a change to someone else's code ? \n● Wanted to share your code, or let other people work on your code ? \n● Wanted to see how much work is being done, and where, when and by whom ? \n● Wanted to experiment with a new feature without interfering with working code ? \n  \nIn these cases, and no doubt others, a version control system should make your life easier. \n Key Points \n● Backup \n● Collaboration \n● Storing Versions \n● Restoring Previous Versions \n● Understanding What Happened \n \nVersion Control / Revision control / Source Control is is a software that helps software developers to work together \nand maintain a complete history of their work. \n  \nYou can think of a version control system (\"VCS\") as a kind of \"database\". \nIt lets you save a snapshot of your complete project at any time you want.  When you later take a look at an older snapshot \n(\"version\"), your VCS shows you exactly how it differed from the previous one. \nA version control system records the changes you make to your project’s files.  \nThis is what version control is about. It's really as simple as it sounds.\nPopular VCS \n      \n    \n           \n \n   \n \nTypes of VCS \n○ \nCentralized version control system (CVCS) \n○ \nEx: CVS, SVN \n○ \nDistributed version control system (DVCS) \n○ \nEx: Git, Mercurial\nCentralized Version Control System (CVCS) \nUses a central server to store all files and enables team collaboration. \nBut the major drawback of CVCS is its single point of failure, i.e., failure of the central server. Unfortunately, if the central \nserver goes down for an hour, then during that hour, no one can collaborate at all. \n  \n \nDistributed Version Control System (DVCS) \nDVCS does not rely on the central server and that is why you can perform many operations when you are offline. You can \ncommit changes, create branches, view logs, and perform other operations when you are offline. You require network \nconnection only to publish your changes and take the latest changes.\nDistributed Version Control\nMain Server Repository\nCollaborator #1 Collaborator #3\nLocal Repository ES eS\npull tt push\nCollaborator #2\nLocal Repository\n_ _\n‘CENTRAL REPOSITORY REMOTE REPOSITORY\nCOMMIT UPDATE ortiona. GED\nand automatically uploads data\n|\n| | S\nWORKING WORKING LOCAL\ncory copy REPOSITORY\nREVERT )( LOG )( STATUS\nSTATUS BRANCH )( MERGE )( BLAME\nEE ed\nHow the Typical VCS works \nA typical VCS uses something called Two tree architecture, this is what a lot of other VCS use apart from git. \nUsually, a VCS works by having two places to store things: \n1. Working Copy \n2. Repository \nThese are our two trees, we call them trees because they represent a file structure. \nWorking copy [CLIENT] is the place where you make your changes. \nWhenever you edit something, it is saved in working copy and it is a physically stored in a disk. \nRepository [SERVER] is the place where all the version of the files or  commits,  logs etc is stored. It is also saved in a disk \nand has its own set of files. \nYou cannot however change or get the files  in a repository directly, in able to retrieve a specific file from there, you have \nto checkout   \nChecking-out is the process of getting files from repository to your working copy.  This is because you can only edit files \nwhen it is on your working copy. When you are done editing the file, you will save it back to the repository by committing \nit, so that it can be used by other developers. \n  \nCommitting is the process of putting back the files from working copy to repository.\nHence, this architecture is called 2 Tree Architecture. \nBecause you have two tree in there Working Copy and Repository. \nThe famous VCS with this kind of architecture is Subversion or SVN. \n  \nHow the Distributed DVCS works \n=========================== \nUnusually, a DVCS works by having three places to store things: \n  \n1. Working Copy \n2. Staging \n3. Repository \nAs Git uses Distributed version control system, So let’s talk about Git which will give you an understanding of DVCS. \nGit was initially designed and developed by Linus Torvalds in 2005 for Linux kernel development. Git is an Open Source \ntool. \n \n \nHistory \nFor developing and maintaining Linux Kernel, Linus Torvalds used BitKeeper which is also one of the VCS, and is open \nsource till 2004. \nSo instead of depending on other tools, they developed their own VCS. \nJust see the wiki of Git.\nGit Architecture \nGit uses three tree architecture.  \nWell interestingly Git has the Working Copy and Repository as well but it has added an extra tree Staging in between: \n  \n \n  \nAs you can see above, there is a new tree called Staging.  \nWhat this is for ?   \nThis is one of the fundamental difference of Git that sets it apart from other VCS, this Staging tree (usually termed as \nStaging area) is a place where you prepare all the things that you are going to commit. \nIn Git, you don't move things directly from your working copy to the repository, you have to stage them first, one of the \nmain benefits of this is, to break up your working changes into smaller, self-contained pieces. \nTo stage a file is to prepare it for a commit. \nStaging allows you finer control over exactly how you want to approach version control. \n \n \nAdvantages Of Git \n===================\nGit works on most of OS: Linux, Windows, Solaris and MAC. \nInstalling Git \n● Download Git {git website} \nTo check if git is available or not use: \n# rpm -qa | grep git \n# sudo yum install git \n# git --version \n  \nSetting the Configuration \n======================= \n# git config --global user.name “Ravi Krishna” \n# git config --global user.email “info@gmail.com” \n# git config --list \n  \nNOTE :: The above info is not the authentication information. \n \nWhat is the need of git config \n===========================   \nWhen we setup git and before adding bunch of files, We need to fill up username & email and it’s basically git way of \ncreating an account. \n \nWorking with Git \n================= \nGetting a Git Repository \n# mkdir website  \n \nInitialising a repository into directory, to initialize git we use: \n# git init \n \nThe purpose of Git is to manage a project, or a set of files, as they change over time. Git stores this information in a data \nstructure called a repository. \n  \n# git init is only for when you create your own new repository from scratch. \nIt turns a directory into an empty git repository.\nLet’s have some configuration set: \n# git config --global user.name “Ravi” \n# git config --global user.email “ravi@digital-lync.com” \n \nTaking e-commerce sites as example. \n \nBasic Git Workflow \n============= \n1. You modify files in working directory \n2. You stage files, adding snapshots of them to your staging area. \n3. You do a commit, which takes the files as they are in the staging area and stores that snapshot to your git \nrepository. \n \n \n \n# mkdir website \n# git init \n# git status                    \n \n {Branches will talk later} \n# vi index.html    \n{put some tags <html><title><h1><body> just structure}                \n# git status \n# git add index.html             \n {staged the changes} \n# git commit -m “Message”    {moves file from staging area to local repo} \n# git status \nYou can skip the staging area by # git commit -a -m “New Changes”\nCommit History - How many Commits have happened ?? \nTo see what commits have been done so far we use a command: \n# git log \nIt gives commit history basically commit number, author info, date and commit message. \nWant to see what happend at this commit, zoom in info we use: \n# git show <commit number>  \nLet’s understand this commit number \nThis is sha1 value randomly generated number which is 40 character hexadecimal number which will be unique. \nLet’s change the title in index.html and go with \n# git add status commit \n# git log {gives the latest commit on top and old will get down} \n \n \nGit diff \n======== \nLet’s see, the diff command gives the difference b/w two commits. \n# git diff xxxxxx..xxxxxx  \nYou can get diff b/w any sha’s like sha1..sha20. \nNow our log started increasing, like this the changes keep on adding file is one but there are different versions of this file. \n# git log --since YYYY-MM-DD \n# git log --author ravi \n# git log --grep HTML  { commit message } \n# git log --oneline \n \n \n \nGit Branching \n================ \nIn a collaborative environment, it is common for several developers to share and work on the same source code. \nSome developers will be fixing bugs while others would be implementing new features.  \nTherefore, there has got to be a manageable way to maintain different versions of the same code base. \nThis is where the branch function comes to the rescue. Branch allows each developer to branch out from the original \ncode base and isolate their work from others. Another good thing about branch is that it helps Git to easily merge the \nversions later on. \n \nIt is a common practice to create a new branch for each task (eg. bug fixing, new features etc.)\nBranching means you diverge from the main line(master-working copy of application) of development and \ncontinue to do work without messing with that main line. \n \n \nBasically, you have your master branch and you don't want to mess anything up on that branch. \n \nIn many VCS tools, branching is an expensive process, often requiring you to create a new copy of your source code \ndirectory, which can take a long time for large projects. \nSome people refer to Git’s branching model as its “killer feature” and it certainly sets Git apart in the VCS community. \n \nWhy is it so special? \n \nThe way Git branches is incredibly lightweight, making branching operations nearly instantaneous, and switching back \nand forth between branches generally just as fast. \n \nWhen we make a commits, this is how git stores them. \n  \n \n \nA branch in Git is simply a lightweight movable pointer to one of these commits. The default branch name in Git is master. \nAs you start making commits, you’re given a master branch that points to the last commit you made. Every time you \ncommit, it moves forward automatically.\nWhat happens if you create a new branch? Well, doing so creates a new pointer for you to move around. \n \nLet’s say you create a new branch called testing. \n    # git branch testing \nThis creates a new pointer to the same commit you’re currently on.    \n \n \nTwo branches pointing into the same series of commits. \n \nHow does Git know what branch you’re currently on? \nIt keeps a special pointer called HEAD. \n \nHEAD is a pointer to the latest commit id and is always moving, not stable. \n    # git show HEAD \n \nIn Git, this is a pointer to the local branch you’re currently on. \n \nIn this case, you’re still on master. The git branch command only created a new branch — it didn’t switch to that branch.\nThis command shows you where the branch pointers are pointing: \n    # git log --oneline --decorate \nYou can see the “master” and “testing” branches that are right there next to the f30ab commit. \n \n \n \n \nTo switch to an existing branch, you run the git checkout command. \n    # git checkout testing \n       \nThis moves HEAD to point to the testing branch. \n \n \nWhat is the significance of that? \n \nWell, let’s do another commit: \n    # vim test.rb\n# git commit -a -m 'made a change' \n    # git log --oneline --decorate \n \nThe HEAD branch moves forward when a commit is made. \n \n \nThis is interesting, because now your testing branch has moved forward, but your master branch still points to the \ncommit you were on when you ran git checkout to switch branches. \n \nLet’s switch back to the master branch: \n    # git checkout master \n \n \n \nHEAD moves when you checkout. \n \nThat command(git checkout master) did two things. It moved the HEAD pointer back to point to the master branch, and \nit reverted the files in your working directory back to the snapshot that master points to. \n \nTo see all available branches \n    # git branch -a \n    # git reflog {short logs}\nMerging \n===========\n \n \n \n \n \n \n \n \n \nGit Merge Conflict \n================ \n \nA merge conflict happens when two branches both modify the same region of a file and are subsequently merged.  \n \nGit don’t know which of the changes to keep, and thus needs human intervention to resolve the conflict. \n \nShowing the example R&D, Training and Consulting. \n \nAutomatic merge failed\nExample showing Merge Conflict \n============================ \n# On master branch \n \n# vi services.html { Add Two Dummy services like Research & Development } \n# git branch training \n# git branch consulting \n \n# git checkout training \n \n# vi services.html { We provide training } \n# git add && git commit \n# git checkout master \n# git merge training \n \n# git checkout consulting \n# vi services.html { We provide Consulting add in same line} \n# git add && git commit \n# git checkout master \n# git merge consulting \n \nAutomatic merge failed \n  \n# we do get a merge conflict here, open the services.html in vi and resolve the conflicts that occurred. \n \n# git add . \n# git commit -m “Conflict resolved” \n \n \n \nGit ignore \n============ \nIt’s a list of files you want git to ignore in your working directory. \n \n \nIt's usually used to avoid committing transient files from your working directory that aren't useful to other collaborators \nsuch as temp files IDE’s create, Compilation files, OS files etc. \n \n \nA file should be ignored if any of the following is true: \n● The file is not used by your project \n● The file is not used by anyone else in your team \n● The file is generated by another process\n.gitignore \n============ \nhttps://www.gitignore.io/ \nIf you want some files to be ignored by git create a file .gitignore \n*.bk \n*.class \n# Ignore all php files \n*.php \n# but not index.php \n!index.php \n# Ignore all text files that start with aeiou \n[aeiou]*.txt \n \n \nStashing\n=============\n \n \n \n \n \n# git stash \n \n# git stash list \n \n \n# git stash apply {apply the top most stashed changes} \n \n# git stash apply stash@{2} {apply particular stashed changes} \n# git stash show <stash> \n# git stash pop {apply 2nd stash and remove it} \n# git stash pop stash@{2} {pop the stash at 2nd reference} \n# git stash drop stash@{3} {remove the stash} \n# git stash clear {Delete all stash entries} \n \n \nCherry Picking \n============== \nCheery picking in Git is designed to apply some commit from one branch to another branch.\nYou can just revert the commit and cherry-pick it on another branch. \n \n \n \n \nTagging \n============ \n  \nIn release management we are working as a team and I’m working on a module and whenever I’m changing some files I’m \npushing those files to remote master. \n  \nNow I have some 10 files which are perfect working copy, and I don’t want this files to be messed up by my other team \nmembers, these 10 files they can directly go for release. \n  \nBut if I keep them in the repository, as my team is working together, there is always a chance that, somebody or other can \nmess that file, so to avoid these we can do TAGGING.\nYou can tag till a particular commit id, imagine all the files till now are my working copies: \n   # git tag 1.0 -m “release 1.0” <commit_id> \n   # git show 1.0 \n   # git push --tags \n  \nGoto GitHub and see release click on it, you can download all the files till that commit. \n \n \nTAGGING helps you in release management.\nDetached Head\n© Basically we use checkout for moving from one\nbranch to another branch,\nme} But if i checkout into a commit id, then I go into a\nfo] state called DETACHED HEAD state.\noY\nIz Say i did # git checkout <commit-id>\nTo DETACHED HEAD: is a state where you are not in\n2 tree anymore, so we cannot track anymore we are\noO outside the tree. Any change we make in detached\nfe} head state are not saved.\n—\nri Now we doesn’t have a pointing branch, we are\na basically in a static commit, if we do\n# git branch\nReason for checking out into a commit id\nTo know what happened at that particular commit.\nre] Let’s say you have created a file and put some phone\nfe] no in there, and now person1 changed the file and\n(7) committed with a message “ph no changed”,\n= then again person2 changed the file and committed\nmo] with same message “ph no changed” later again\ns person3 changed the file and committed with same\n8 message “ph no changed”.\nF™)\nwo Now here we can't rely on commit message itself,\na this is were detached head is needed.\nMAVEN \nA VCS plays a vital role in any kind of organization, the entire software industry is built around code. \n \nWhat are we doing with this code ?? \n \n● Are we seeing the code when we open the application ?? NO \n● Are we seeing the code when we open the app in browser ?? NO \n \nSo we are seeing the executable format of the code, that is called build result of the code. \n \nWhat is build ?? \n============== \nBuild is the end result of your source code. \n \nBuild tool is nothing but, it takes your source code and converts it into human readable format (executable). \n \nBuild \n==== \nThe term build may refer to the process by which source code is converted into a stand-alone form that can be run on a \ncomputer. \n \nOne of the most important steps of a software build is the compilation process, where source code files are converted into \nexecutable code. \n \nThe process of building software is usually managed by a build tool i.e, maven. \n \nBuilds are created when a certain point in development has been reached or the code has been ready for implementation, \neither for testing or outright release. \n \nBuild: Developers write the code, compile it, compress the code and save it in a compressed folder. This is called Build. \n \nRelease: As a part of this, starting from System study, developing the software and testing it for multiple cycles and deploy the \nsame in the production server. In short, one release consists of multiple builds. \n \nMaven Objectives \n=============== \n● A comprehensive model for projects which is reusable, maintainable, and easier to comprehend(understand). \n● plugins  \n \nConvention over configuration \n========================== \nMaven uses Convention over Configuration which means developers are not required to create build process themselves. \nDevelopers do not have to mention each and every configuration detail. \n \nEarlier to maven we had ANT, which was pretty famous before maven.\nDisadvantages of ANT \n================== \n   ANT - Ant scripts need to be written for building  \n[build.xml need to tell src & classes ] \n   ANT - There is no dependency management \n   ANT - No project structure is defined \n \nAdvantages of Maven \n==================     \nNo script is required for building [automatically generated - pom.xml] \nDependencies are automatically downloaded \nProject structure is generated by maven \nDocumentation for project can be generated  \n \nMaven is called as project management tool also, the reason is earlier when we used to create projects and we used to \ncreate the directory structure and all by yourself, but now maven will take care of that process. \n \nMAVEN has the ability to create project structure. \nMaven can generate documentation for the project. \n \nWhenever i generate a project using maven i will get src and test all by default. \n \nMAVEN FEATURES \n================ \n \nDependency System \n================= \nInitially in any kind of a build, whenever a dependency is needed, if i’m using ANT i have to download the dependency \nthen keep it in a place where ANT can understood. \n \nIf i’m not giving the dependency manually my build will fail due to dependency issues. \n \nMaven handles dependency in a beautiful manner, there is place called MAVEN CENTRAL. Maven central is a centralized \nlocation where all the dependencies are stored over web/internet. \n \nFor ex im using a project and i’m having a dependency of junit, whenever my build reaches a phase where it needs junit \nthen it will download the dependencies automatically and it will store those dependencies in your machine. There is a \ndirectory called as .m2 created in your machine, where all the dependencies are going to be saved. Next time when it \ncomes across the same dependency, then it doesn’t download it coz its already available in .m2 directory. \n \nPlugin Oriented \n============= \nMaven has so many plugins that i can integrate, i can integrate junit, jmeter, sonarqube, tomcat, cobertura and so many \nother. \n \nIMPORTANT FILE IN MAVEN \n======================= \nProjects in maven is defined by POM (Project Object Model) pom.xml.\nMaven lifecycle phases \n=================== \n \nWhat is build life cycle? \nThe sequence of steps which is defined in order to execute the tasks and goals of any maven project is known as build \nlifecycle in maven. \n \nThe following are most common default lifecycle phases executed: \n● validate: validate the project is correct and all necessary information[dependencies] are available and keep it in \nlocal repo \n● compile: compile the source code of the project \n● test: Execution of unit tests, test the compiled source code using a suitable unit testing framework. These tests \nshould not require the code be packaged or deployed \n● package: take the compiled code and package it in its distributable format, such as a JAR. \n● verify: run any checks to verify the package is valid and meets quality criteria, keeps the HelloWorld.jar in .m2 \nlocal repo \n● install: Deploy to local repo [.m2], install the package into the local repository, for use as a dependency in other \nprojects locally \n● deploy: done in an integration or release environment, copies the final package to the remote repository for \nsharing with other developers and projects. This will push the libraries from .m2 to remote repo. \n \n \nThere are two other Maven lifecycles of note beyond the default list above. They are \n● clean: cleans up artifacts created by prior builds \n● site: generates site documentation for this project \n \nThese lifecycle phases are executed sequentially to complete the default life cycle. \n \n \nPOM { will be in XML format } \n==== \n \nGAV \n \nMaven uniquely identifies a project using: \n● groupID: Usually it will be the domain name used in reverse format (going to be given by the project manager). \n● artifactID: This should be the name of the artifact that is going to be generated \n● Version : Version of the project, Format {Major}.{Minor}.{Maintenance} and add “-SNAPSHOT” to identify in \ndevelopment \n \nVersion can be two things here, a SNAPSHOT and other is RELEASE. \nSnapshot - whenever your project is in working condition i mean we are still working on it that would be a snapshot \nversion, you can have multiple snapshots for one single project. \nBut there would be only one release for it, for example after my 20th snapshot we decided that 8th snapshot should goto \nrelease then will remove -SNAPSHOT for 8th. \nIn real time we will get the basic pom which is already written with groupid, artifactid and version, we can build up \nrequired plugins and dependencies.\nPackaging \n========= \nBuild type is identified by <packaging> element, this element will tell how to build the project. \n \nExample packaging types: jar, war etc. \n \nArchetype \n========= \nMaven archetypes are project templates which can be generated for you by Maven. \nIn other words, when you are starting a new project you can generate a template for that project with Maven. \nIn Maven a template is called an archetype. \nEach Maven archetype thus corresponds to a project template that Maven can generate. \nInstallation \n========== \n \nMaven is dependent on java as we are running java applications, so to have maven, we also need to have java in system. \n \nInstall java \n========= \nJava package : java \nprogram \n--- \njava-1.8.0-openjdk \nJava package : java \ncompiler \n--- \njava-1.8.0-openjdk-devel \n \n \n \n# sudo yum -y install java-1.8.0-openjdk \n# sudo yum -y install java-1.8.0-openjdk-devel \n \n# java -version { confirm java version} \n \nInstall Maven \n=========== \n \n \n# yum -y install maven \n \nMaven repository are of three types \n============================= \nFor maven to download the required artifacts of the build and dependencies (jar files) and other plugins which are \nconfigured as part of any project, there should be a common place. This common shared area is called as Repository in \nmaven. \n \nLocal \n===== \nThe repository which resides in our local machine which are cached from the remote/central repository downloads and \nready for the usage. \n \nRemote \n======= \nThis repository as the name suggests resides in the remote server. Remote repository will be used for both downloading \nand uploading the dependencies and artifacts.\nCentral \n====== \nThis is the repository provided by maven community. This repository contains large set of commonly used/required \nlibraries for any java project. Basically, internet connection is required if developers want to make use of this central \nrepository. But, no configuration is required for accessing this central repository. \n \nHow does Maven searches for Dependencies? \n====================================== \nBasically, when maven starts executing the build commands, maven starts for searching the dependencies as explained \nbelow : \n \n● It scans through the local repositories for all the configured dependencies. If found, then it continues with the \nfurther execution. If the configured dependencies are not found in the local repository, then it scans through the \ncentral repository. \n \n● If the specified dependencies are found in the central repository, then those dependencies are downloaded to the \nlocal repository for the future reference and usage. If not found, then maven starts scanning into the remote \nrepositories. \n \n● If no remote repository has been configured, then maven will throw an exception saying not able to find the \ndependencies & stops processing. If found, then those dependencies are downloaded to the local repository for \nthe future reference and usage. \n \n \n \nSetting up stand alone project \n========================= \n \n# cd ~ \n \n# mvn archetype:generate \n \n{ generates project structure } \n \n# we get some number like 1085 beside it we have 2xxx, which means maven \n currently supports 2xxx \nproject structures, 1085 is like default project \n \n# press enter \n \n# choose a number :: 6 which means latest, so press enter \n \n# groupId: com.digital.academy { unique in world, generally domain } \n \n# artifactId: project1 \n \n \n{ project name } \n \n# version: press enter \n \n{ snapshot - intermediate version } \n \n# package: enter { package name is java package } \n \n# Press: y \n \nNow maven has successfully created a project structure for you: \n# tree -a project1 \n \nWe have pom.xml which contains all the definitions for your project generated, this is the main file of the project. \n# ls -l ~/.m2/repository \n \n# mvn validate { whatever in the pom.xml is correct or not }\nLet’s make some mistakes and try to fail this phase,  \n# mv pom.xml pom.xml.bk \n \n# mvn validate { build failure } \n# mv pom.xml.bk pom.xml \n \n# vi App.java \n \n{ welcome to Devops } \n \n# mvn compile { after changing code we do compilation right } \n \n \n{ this generates a new structure - # tree -a . with class files} \n# mvn test \n \n{ test the application } \n# mvn package { generates the artifact - jar } \n# java -cp target/xxxx.jar  \ngroupid(com.digital.proj1).App \n     \n \nPlugins \n======= \nWe saw maven is only performing phases like validate, compile, test, package, install, deploy but if you remember there is \nno execution of a jar file, \n \nCan you see any of the phases running jar file, no right ?? \n \nExecuting jar file is not part not the part of life cycle, \napart from the above phases such as validate, compile, test, package, install, deploy all the other come under <build> \nunder <plugins>  </plugins> \n \nThese plugins will define, other then regular maven lifecycle phases, \n \nLet’s take example, i want to run my jar file, \nAfter </dependencies> in your pom.xml \n \nAfter </dependencies> in your pom.xml add the following \n \n<build> \n<plugins> \n<plugin> \n<groupId>org.codehaus.mojo</groupId> \n  <artifactId>exec-maven-plugin</artifactId> \n  <version>1.2.1</version> \n  <configuration> \n    <mainClass>com.digi.App</mainClass> \n        <arguments> \n              <argument>-jar</argument> \n              <argument>target/*.jar</argument> \n         </arguments> \n</configuration> \n</plugin> \n</plugins> \n</build> \n</project> \n \nIn pom.xml after </dependencies> add <build> <plugins> <plugin> \n \nWe need to run  \n# mvn exec:java\nWEB APP SETUP \n \nSetting up web project \n=================== \n# mvn archetype:generate | grep maven-archetype-webapp \n# type the number you get \n# tree -a project/ \n# mvn clean package \n# tree -a project \n \nYou can see the war generated under target directory \n \nTomcat Installation [Binaries] \n======================== \nGoogle tomcat 7 download \n# goto tomcat downloads page and get the binary tar file for the tomcat 7 by wget \n# wget <link> \n# wget http://www-us.apache.org/dist/tomcat/tomcat-7/v7.0.82/bin/apache-tomcat-7.0.82.tar.gz \n# tar xf apache-tomcat-7.0.82.tar.gz \n# cd apache-tomcat-7.0.82/bin \n# ./startup.sh \n# Check for port to be opened in firewall \n# netstat -ntpl { # sudo yum -y install net-tools } \n# ps -ef | grep tomcat \n \n# Goto http://ip-address/8080 {click cancel and change tomcat-users.xml file} \n<role rolename=\"manager-gui\"/> \n<user username=\"tomcat\" password=\"tomcat\" roles=\"manager-gui\"/> \n<user username=\"tomcat1\" password=\"tomcat1\" roles=\"manager-script\"/> \n# Change the port number in server.xml \n# cd apache-tomcat-7.0.81/bin \n# ./shutdown.sh \n# Copy the generated war file to webapps dir of tomcat \n# Refresh the tomcat page \n \nDeploy to tomcat maven tomcat plugin \n===================================== \n \nAdd Manager-Script Role \n===================== \nAdd new role under conf/tomcat-users.xml \n \n# vi conf/tomcat-users.xml \n<user username=\"tomcat1\" password=\"tomcat1\" roles=\"manager-script\"/>\nAdd Maven-Tomcat Authentication \n============================ \n# vi ~/.m2/settings.xml \n<settings> \n   <servers> \n   \n <server> \n   \n \n <id>TomcatServer</id> \n   \n \n <username>tomcat1</username> \n   \n \n <password>tomcat1</password> \n   \n </server> \n   </servers> \n</settings> \n \n \nAdd Tomcat 7 Maven Plugin \n======================= \n# vi pom.xml \n<plugin> \n   <groupId>org.apache.tomcat.maven</groupId> \n   <artifactId>tomcat7-maven-plugin</artifactId> \n   <version>2.2</version> \n   <configuration> \n   \n <url>http://localhost:8080/manager/text</url> \n   \n <server>TomcatServer</server> \n   \n <path>/WebApps</path> \n   </configuration> \n</plugin> \n \n# mvn tomcat7:deploy \n# mvn tomcat7:undeploy \n# mvn tomcat7:redeploy \n \n \n  \nProfiles \n======== \nSo in general, what is the meaning of profile, let’s take windows as example for each profile there would be some different \nsettings right. \n \nI mean in same machine we can have different different profiles. Similarly in pom.xml, the project is same, but you can \ncreate multiple profiles, for multiple purposes. \n  \nSo for my project i want to have different different profiles like dev, qa and prod env. \nHere the requirements are different for each and every env, like in dev env we don’t need any of the test to be run. \n  \nLet’s say there 4 people who have different different req, now i need to create 4 projects instead of 4 projects, within a \nsingle project i can have 4 profiles, that’s the profile concept.\n<profiles> will not be there under <build>, they will be below </dependencies>, \n So the pom.xml looks like this with <profiles> \n  \n</dependencies> \n<profiles> \n \n<profile> \n \n \n<id>DEV</id> \n \n \n<build> \n \n \n     <plugins> \n \n \n \n<plugin> \n \nI have keep the configuration here please go through it \n \nhttps://github.com/ravi2krishna/Maven-Build-Profiles.git \n \n \n \n \nNEXUS \n \n⇒ Nexus is a Binary Repository Manager \n \nNow the simple solution for this problem is, Dev B should ask Dev A to provide the HelloWorld.jar, so that DevB can keep \nthe Hello.jar in DevB machines .m2 directory, \n \nThis works coz the maven will look first in .m2 directory.[Local Repo] \nIf not then it maven central \n \n \n \n \n  [Apache Repo] \nNow imagine DevA, keeps on changing the Hello.jar code, let’s say DevA changed 100 times now DevB should ask DevA \n100 times which doesn’t make sense.\nTomorrow in your project, you have 100 Developers, now they have to exchange their libraries, now it won’t be that easy \nto exchange the libraries. \n \nIt’s really cumbersome process, Nobody understands which version is there with whom. \n \nThe solution is there, but this is a complex and time taking solution. \nThis may workout if there are only 2-3 developers, but not more then that. \n \nThis is where Binary Repository Concept comes into picture. \n \nNow we will introduce a new server within our organization. This server we call it as Remote Repository. \n \nJust like how apache is maintaining a MavenCentral, similarly we will maintain our own remote repository. \n \nNow DevA instead of sharing his Hello.jar with DevB, DevC etc \nHe will push it to Our Remote Repository and now DevB, DevC everyone who needs that Hello.jar will pull it from the \nRemote Repository. \n \nI mean they[DevB, DevC] will add that info in the pom.xml, now pom will take care of downloading it from remote repo. \now we got totally three kinds of Repo’s. \n1. Local \n2. Public Repo \n3. Private Repo \nNow even the libraries are secured, coz they are present within your organization. \n \nWe got different tools for that Artifactory/Nexus/Archiva. \n \nSonatype Nexus \n============== \n \nNow we are going to set up this within our server. \n \nInstall Nexus Server. \n \nSearch for Apache Maven Nexus Repository in google, \n \nSnapshot  \n⇒ \nDevelopment progress build [Partial Completed Jars] \n \n \n \n \nRelease  ⇒  \nReady to release build [Official proper release] \n \nInstallation of Nexus \n================= \n \n# type download nexus in google \n# go with version 2.x as 3.x is not yet supported with jenkins \n# copy the link address for 2.x and do wget \n# wget http://www.sonatype.org/downloads/nexus-latest-bundle.tar.gz \n# mkdir -p tools/nexus\n# tar xvzf nexus.tar.gz -C tools/nexus \n# Nexus by default starts on the port number 8081 and un & pw is admin & admin123 \n# cd nexus/nexus-2.x/bin \n# ./nexus start { if any problem change ownership to devops to both dirs of nexus} \n# netstat -ntpl | grep 8081 \n \n{ be patient it takes some time to start } \n# http://ip-addr:8081/nexus \n \n{ allow port 8081 through firewall } \n# login and give default creds admin & admin123 \n \nERROR: Change the permissions of two nexus directories with logged in username. \n \n \n \nhttp://ip-addr:8081/nexus \n \nOnce the nexus is up we will create two repositories, Snapshot & Release. \n \nSearch for distribution management tag in google for maven & nexus, and paste it after </dependencies> tag. \n \nIf your version contains a string SNAPSHOT, by default it goes to SNAPSHOT repo. \nIf your version contains only version 1.0, it goes to RELEASE repo. \n \nNow we will, deploy the artifacts to remote repo by deploy phase. \n \nLogin to nexus using admin & admin123, now we already we have some default repos, we have something Central, this is \nApache Maven Central. \n \nBut we will go with our own repo’s, using HOSTED repo’s. \n \nAdd → Hosted Repo → Repo Id: releaseRepo → Repo Name: releaseRepo → Repo Policy: Release → Deployment Policy: \nDisable Redeploy → Save \n \nAdd → Hosted Repo → Repo Id: snapshotRepo → Repo Name: snapshotRepo → Repo Policy: Snapshot → Deployment \nPolicy: Allow Redeploy → Save \n \n \nGoto the maven project and do # mvn deploy \n \nSearch for maven distributionmanagement nexus in google, \n \nGoto pom.xml and update <distributionManagement> below </dependencies> \n \n<distributionManagement> \n \n<repository> \n<id>releaseRepo</id> \n<name>releaseRepo</name> \n<url>http://192.168.56.101:8081/nexus/content/repositories/releaseRepo/</url> \n</repository> \n \n<snapshotRepository> \n<id>snapshotRepo</id>\n<name>snapshotRepo</name> \n<url>http://192.168.56.101:8081/nexus/content/repositories/snapshotRepo/</url> \n</snapshotRepository> \n \n</distributionManagement> \n \nGoto the maven project and do # mvn deploy again, now we get new error like 401. \n \nNexus is strictly authenticated, you cannot deploy until and unless you login, \n \nWe don’t provide usernames and passwords in pom.xml, coz pom.xml files are stored in VCS, which will be shared to \nother[most] developers as well. \n \nFor this maven provides solution in local repo, \n# cd ~/.m2 \n# vi settings.xml {search for maven settings.xml nexus username and pass } \n<settings> \n<servers> \n    <server> \n      <id>releaseRepo</id> \n      <username>admin</username> \n      <password>admin123</password> \n</server> \n<servers> \n    <server> \n      <id>snapshotRepo</id> \n      <username>admin</username> \n      <password>admin123</password> \n</server> \n</settings> \n \nNow change the pom.xml <version> to 1.0-SNAPSHOT and redeploy again. \n \n# mvn deploy \n[After one min, again run mvn deploy] \n \n# mvn deploy \n \nSnapshot is generally for other developers to get the dependency. \n \nNexus Assignment \n================ \n \nFollow this repo to get the code \n \nhttps://github.com/ravi2krishna/CalculatorTestCases.git \n \nPackage the application and implement the add(), substract() and multiply() using the JAR which will be generated from \nthe above repository.\nSONARQUBE \n \nPrerequisites : \n1. Java 1.7 + { recommended 1.8 } \n2. MySql 5.6+ \n \n \n \nInstall Java 1.8 \n============ \n# sudo yum -y install java-1.8.0-openjdk \n \n# sudo yum -y install java-1.8.0-openjdk-devel \n \n# java -version { confirm java version} \n \n \nInstall MySql 5.6+ { for production use } \n================================ \n \n# wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm \n \n# sudo rpm -ivh mysql-community-release-el7-5.noarch.rpm \n \n# sudo yum -y install mysql-server \n \n# sudo systemctl start mysqld \n \n# sudo mysql_secure_installation \n \n# sudo mysql -u root -p \n \n \n# CREATE DATABASE sonar; \n# CREATE USER 'sonar' IDENTIFIED BY 'sonar'; \n# GRANT ALL ON sonar.* TO 'sonar'@'localhost' IDENTIFIED BY 'sonar'; \n# FLUSH PRIVILEGES; \n \nSonarqube Installation \n=================== \nVisit https://www.sonarqube.org/downloads/ \n# wget the latest version [LTS] \n# unzip <file> \n# cd sonar/conf \n# vim sonar.properties \n \nChanges to make in sonar.properties \n \n============================== \n# sonar.jdbc.username=sonar\n# sonar.jdbc.password=sonar \n# uncomment sonar.jdbc.url of MySQL 5.6 \n# uncomment sonar.web.host=0.0.0.0 \n# uncomment sonar.web.port=9000 \n \n# cd sonarqube-6.7/bin \n# cd linux-x86-64 \n# ./sonar.sh start \n \nBrowse the sonarqube dashboard on  \nhttp://ip-addr:9000/ \n \nNow browse to one of the maven projects and do mvn sonar:sonar. \n \n# cd SampleApp \n# mvn compile sonar:sonar \n \n# git clone https://github.com/wakaleo/game-of-life \n# cd game-of-life \n# mvn compile sonar:sonar \n------------------------------------------------------------- \n \nJENKINS \n \nINTRODUCTION \n============= \nWhat is jenkins ?? \nJenkins is an application that monitors executions of repeated jobs, such as building a software project. \n \nNow jenkins can do a lot of things in an automated fashion and if a task is repeatable and it can be done in a same way \nover time, jenkins can do it not only doing it but it can automate the process, means jenkins can notify a team when a build \nfails, jenkins can do automatic testing(functional & performance) for builds. \n \nTraditionally, development makes software available in a repository, then they give a call to operations/submit a ticket to \nhelpdesk and then operations builds and deploys that software to one or more environments, one this is done, there is \nusually a QA team which loads and executes performance test on that build and makes it ready for production. \n \nSo what jenkins does is a lot of these are repeatable tasks, which can be automated by using the jenkins. \n \nJenkins has a large number of plugins which helps in this automation process.\nContinuous Integration \n=================== \nis a development practise that requires developers to integrate code into a shared repository several times per day (repos \nin subversion, CVS, mercurial or git). Each check-in is then verified by an automated build, allowing everyone to detect and \nbe notified of problems with the package immediately. \n \nBuild Pipeline \n============ \nis a process by which the software build is broken down in sections: \n•  Unit test \n•  Acceptance test \n•  Packaging \n•  Reporting \n \n•  Deployment \n•  Notification \n \nThe concepts of Continuous Integration, Build Pipeline and the new “DevOps” movement are revolutionizing how we \nbuild, deploy and use software.  \n \nTools that are effective in automating multiple phases of these processes (like Jenkins) become more valuable in \norganizations where resources, time or both are at a premium.  \n \nInstallation of Jenkins \n===================== \n● We need java to work with jenkins. \n○ \n# sudo yum -y install java-1.8.0-openjdk \n○ \n# sudo yum -y install java-1.8.0-openjdk-devel \n○ \n# java -version { confirm java version} \n● Generally jenkins runs on port 8080, so we need to make sure that there is no other service that is running and \nlistening on port 8080 \n \n \n● Now we need to add jenkins repo, to our repository list, so that we can pull down and install jenkins package. \n○ \n# sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo \n○ \nWe will run a key so that we can trust this repo and pull down jenkins package \n○ \n# sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key \n● Now our key has been imported we can do \n○ \n# sudo yum -y install jenkins \n● Now let’s enable the service \n○ \n# sudo systemctl enable jenkins \n● Now let’s enable the service \n \n○ \n# sudo systemctl start jenkins \n● Now if you do # ip-address:8080 you can see jenkins home \n \n \n \nNow if you do # cat /etc/passwd | grep jenkins\njenkins:x:996:994:Jenkins Automation Server:/var/lib/jenkins:/bin/false \n \nCreates a user called jenkins, and keeps this user away from login, this is the default \nbehaviour of jenkins coz we are right now on master and building everything here. \n \nBut in real time we have two things here master and build slaves \nMaster: where jenkins is installed and administration console is \nBuild slaves: these are servers that configured to off load jobs so that master is free \n \nCreating Users \n============ \nManage Jenkins → Manage Users → Create User Left Side → Fill details \nCreate Users : tester 1, tester 2, developer 1 and developer 2 \n \n \n \nCreate new jobs \n============= \ntestingJob ⇒ Execute Shell ⇒ echo \"Testing Team Jobs Info\" \n \ndevelopmentJob ⇒ Execute Shell ⇒ echo \"Development Team Jobs Info\" \n \nNow login with the tester1 and see the list of jobs. \nNow login with the developer1 and see the list of jobs. \n \nAs you can see, both the testing and development team jobs are visible across different teams, which would be a security \nconcern, but this is the default behavior of jenkins. \n \nManage Jenkins → Conϐigure Global Security → Authorization → Logged in users \n \nNow let’s see how we can secure the jenkins to make jobs only visible to testing and development teams. \n \n \nFor this we need to install new plugin called Role-based Authorization Strategy. \n \nLet’s see what are PLUGINS first \n \nPlugins: plugins enhances jenkins power and usability. \n \nInstalling Plugin \n============== \nManage Jenkins → Manage Plugins → Available → Search Role-based → Install without restart. \n \nManage Jenkins → Conϐigure Global Security → Authorization, now we can see the new option Role-Based Strategy. \n \nSelect the Role-Based Strategy → Apply → Save\nNow if i login with the tester or developer user i won’t have access, i can only access with the admin user. \n \nNow let’s see how we can go and create some roles and based on roles we should grant access to users: \nManage Jenkins → Manage and Assign Roles → Manage roles → \n \n \nGlobal Roles: check Role to add, give something like employee and click add \n \nand give overall read access and over all view access \n \n \n \n \n \nProject Roles: here we can create roles specific to a project, \n \nRole to add: developer && Pattern: dev.* and check everything, now developers will only have access to projects that \nstart with dev but nothing else. Click Apply and Save \n \nRole to add: tester && Pattern: test.* and check everything, now testers will only have access to projects that start with \ntest but nothing else. Click Apply and save \n● So we have created an employee role at global level and we created two roles developer and tester at project \nlevel.\nAssigning Roles To Users \n====================== \n● Manage Jenkins → Manage and assign roles → Assign roles → Global Roles → User/group to add → Add \nUsers tester1, tester2, develoepr1 and developer2 to Global roles as employee → Apply \n \nUnder Item Roles User/group to add → Add Users tester1, tester2, developer1 and developer2 to Item roles → \nNow add Users tester1 & tester2 to Tester Roles and Users developer1 & developer2 to Developer Roles → \nApply \n○ \nSo we created users, we created roles and we assigned roles \n \nNow if you login with tester users you can only see jobs related to testing, and similarly if you login with developer you \ncan see only development related jobs. \n \n \nJenkins [Master - Slave Config] \n============================ \n \nWe know we have user jenkins, who has no shell and this user is the owner of jenkins application but beyond that it’s a \nnormal user. So what we are going to do is manage the global credentials.\nNow we should make a decision that when we run jobs either locally or remotely we are going to run them with the \njenkins user using ssh so that we can control slaves. \n \nSo on our system(master) we are going to change jenkins user, so that we can login with jenkins user, let’s go and do it \n \n# vi /etc/passwd {jenkins change /bin/false to /bin/bash} \n# sudo su jenkins \n \nMake sure our jenkins user is a sudoer \n# usermod -aG google-sudoers jenkins \n# vi /etc/sudoers or # sudo visudo \nBelow root add the following \n \n# root ALL=(ALL)    \nALL \n \n# jenkins  ALL=(ALL) \nNOPASSWD:   ALL \n \nSwitch to jenkins user \n# sudo su jenkins   \n# cd  \n{ make sure you are in jenkins home /var/lib/jenkins} \n \n# ssh-keygen \n \n \nClick on Credentials on homepage of jenkins → Click on Global credentials → Adding some credentials → Kind (select SSH \nusername with private key) → Scope (global) → \nUsername (jenkins) → Private Key (From jenkins master ~/.ssh) → Ok \n \nNow this sets jenkins user account to be available for SSH key exchange with other servers. This is imp coz we want the \nability so that single jenkins user can be in control to run our jobs remotely so that master can off load its jobs to slaves. \n \nSlave \n===== \nNow create a new centos server(machine) where you will be getting new ip {1.2.3.4} \nCreate a user jenkins # useradd jenkins \n# usermod -aG google-sudoers jenkins \n \n# sudo su jenkins - \n# cd  \n{ make sure you are in jenkins home /home/jenkins} \n \n \nNow do password less authentication steps on both machines : \n \n# vi authorized_keys (add public key of other machine) \n# chmod 700 ~/.ssh \n# chmod 600 ~/.ssh/authorized_keys \nNow if everything is good then you should be able to login into the slave machine without password. \n \nDoing builds on slaves \n==================== \nSo we talked about this a lot, that we don’t want to do builds much on master.\nWe are going to create dumb slave nodes, slaves doesn’t need to know anything about implementation they should be able \nto just run jobs and be controlled by master. \n \nWe are going to use this slave nodes in order to off-load the build processing to other machines so that the master server \ndoesn’t get CPU, IO or N/W load etc in managing large number of jobs across multiple servers multiple times a day. \n \nSo we are going to create a slave node: \n# Now make sure there is key exchange b/w both the machines. \n \n# Manage Jenkins (scroll down) → Manage Nodes → Master (Now master is always going to be included by \ndefault  here if we click on master) →  Conϐigure → Usage : Only build jobs with label matching nodes(for the most \npart we want master only to use for controlling jobs we have setup) → Save  \n \n \n \n \n \n# Manage Jenkins (scroll down) → Manage Nodes → New Node → Node name : Remote slave 1 → # of executors : 3 (up to 3 \nconcurrent jobs) → Remote root Directory: (/home/jenkins) Labels: remote1 →  Usage: (as much as possible) → Launch \nmethod: Launch slave via SSH → Host: ip of slave → Credentials: Service acc(Give settings of Kind: SSH username with \nprivate key && Private key: From jenkins master ~/.ssh) → Availability: Keep online as much as possible → Save \nInstall java and javac on all slaves. \nThen click on node and see the log. \n \nNow if i goto jenkins home i can see both master and slave and their executors \n \nBuilding a job on slave \n=================== \nNew job → Under General (✅ Restrict where this project can run)(Label expression: remote1 or expression we gave while \ncreating slave) → Build → Execute Shell → in command give # pwd # uname -a # hostname → Save → Run build. \n \n \n \n \nVAGRANT \n \nVagrant \n======= \nVagrant is a tool for building and managing virtual machine environments. \nWith an easy-to-use workflow and focus on automation, Vagrant lowers development environment setup time, increases \nproduction parity, and makes the \"works on my machine\" excuse a relic of the past. \nVagrant is suitable for development environment. \n \nWhy Vagrant ?? \n============== \nVagrant provides easy to configure, reproducible, and portable work environments built on top of industry-standard \ntechnology and controlled by a single consistent workflow to help maximize the productivity and flexibility of you and \nyour team. \n \nTo achieve its magic, Vagrant stands on the shoulders of giants. Machines are provisioned on top of VirtualBox, VMware,\nAWS, or any other provider. Then, industry-standard provisioning tools such as shell scripts, Chef, or Puppet, can \nautomatically install and configure software on the virtual machine. \n \nFor Developers \nIf you are a developer, Vagrant will isolate dependencies and their configuration within a single disposable, consistent \nenvironment, without sacrificing any of the tools you are used to working with (editors, browsers, debuggers, etc.). Once \nyou or someone else creates a single Vagrantfile, you just need to vagrant up and everything is installed and configured for \nyou to work. Other members of your team create their development environments from the same configuration, so \nwhether you are working on Linux, Mac OS X, or Windows, all your team members are running code in the same \nenvironment, against the same dependencies, all configured the same way. Say goodbye to \"works on my machine\" bugs. \nFor Operators \nIf you are an operations engineer or DevOps engineer, Vagrant gives you a disposable environment and consistent \nworkflow for developing and testing infrastructure management scripts. You can quickly test things like shell scripts, Chef \ncookbooks, Puppet modules, and more using local virtualization such as VirtualBox or VMware. Then, with the same \nconfiguration, you can test these scripts on remote clouds such as AWS or RackSpace with the same workflow. Ditch your \ncustom scripts to recycle EC2 instances, stop juggling SSH prompts to various machines, and start using Vagrant to bring \nsanity to your life. \nFor Everyone \nVagrant is designed for everyone as the easiest and fastest way to create a virtualized environment! \n \nWhat is the meaning of setting up environment ?? \n================================================ \nLet’s say you need web server \nLet’s say you need db server \nLet’s say you need app server \n \nLet's say you need some machine for R & D purpose quickly, configure that machine quickly and able to use that machine \nvery quickly, now using vagrant we can reduce installation time. \n \nTypically when we setup OS, you may take around 30-45 min to go along with that. \nBut with vagrant you can spin up the development environment very very quickly. \n \nNow how to spin up the environment ?? \n======================================= \nlets see how it can be done \n \nGoto the official website of vagrant vagrantup.com \n \nWe see something like find boxes right, let’s understand some terminology: \nSo when we are going with Base Installation of Linux, we need couple of things \n●    \n You need a CD or ISO image \n●    \n You need a physical machine \n●    \n You define some CPU & RAM \n●    \n Storage \n●    \n Network\nSo whenever we are going with installation we need to go with all these steps always. \n   \nSo if we are working with cloud we have some images like CentOS, ubuntu etc \n●    \n AMI (Amazon Machine Images) \n●    \n Virtualization Layer \n●    \n CPU    {how much CPU i need} \n●    \n RAM {how much RAM i need} \n●    \n Storage {how much storage i need} \n●    \n Network \n \nI need to configure all the above \nSo in vagrant, we call these pre-installed Images/OS as BOXES \nNow you can easily download these boxes, and can spin up the virtual machines easily. \n \nAs i told to setup the OS it takes around 30-45 min, but using the vagrant it hardly takes 5-10 min depending on internet \nspeed. \n \n   \n Download and Install vagrant \n   \n Vagrant supports on following platforms: WIN/MAC/LINUX \n \n   But there is another dependency to vagrant, which is the virtualization layer. \n   So in our laptop/desktop, what is the virtualization layer we use: \n   \n Oracle Virtual Box \n \n   \n \n 1. Download and install {win - CMD} \n   \n \n # vagrant --version     \n \n \n   Now i don't have any box right, let’s download the box: \n   \n # goto vagrantup.com \n   \n # find BOXES {gives list of all boxes} \n \n   So once you go to find boxes, it shows what’s the virtualization layer is and the diff boxes available. \n \n   There is provider, lets understand the terminology \n   \n Here Virtualbox →    Provider \n   \n Provider: tool which is giving you the virtual layer \n   \n But we have diff providers available which provides virtualization layer. \n    \n \n \nGetting Started \n============= \nWe will use Vagrant with VirtualBox, since it is free, available on every major platform, and built-in to Vagrant. \nBut do not forget that Vagrant can work with many other providers.\nProviders \n======== \nWhile Vagrant ships out of the box with support for VirtualBox, Hyper-V, and Docker, Vagrant has the ability to manage \nother types of machines as well. This is done by using other providers with Vagrant. \n \nBefore you can use another provider, you must install it. Installation of other providers is done via the Vagrant plugin \nsystem. \n \nOnce the provider is installed, usage is straightforward and simple, as you would expect with Vagrant. \n \nYour project was always backed with VirtualBox. But Vagrant can work with a wide variety of backend providers, such as \nVMware, AWS, and more. \n \nOnce you have a provider installed, you do not need to make any modifications to your Vagrantfile, just vagrant up with \nthe proper provider and Vagrant will do the rest: \n \n# vagrant up --provider=vmware_fusion \n# vagrant up --provider=aws \n \n \n \nUp and Running \n=============== \n \n# vagrant init centos/7 \n \n# vagrant up \nAfter running the above two commands, you will have a fully running virtual machine in VirtualBox running. \nYou can SSH into this machine with # vagrant ssh, and when you are done playing around, you can terminate the virtual \nmachine with # vagrant destroy. \n \nProject Setup \n============ \nThe first step in configuring any Vagrant project is to create a Vagrantfile. The purpose of the Vagrantfile is: \n1. Mark the root directory of your project. Many of the configuration options in Vagrant are relative to this root directory. \n2. Describe the kind of machine and resources you need to run your project, as well as what software to install and how \nyou want to access it. \n \nVagrant has a built-in command for initializing a directory for usage with Vagrant: \n# vagrant init \nThis will place a Vagrantfile in your current directory. You can take a look at the Vagrantfile if you want, it is filled with \ncomments and examples. Do not be afraid if it looks intimidating, we will modify it soon enough. \n \nYou can also run vagrant init in a pre-existing directory to setup Vagrant for an existing project. \n \n# vagrant init centos/7 \n \nVagrantfile \n========= \nThe primary function of the Vagrantfile is to describe the type of machine required for a project, and how to configure \nand provision these machines.\nVagrant is meant to run with one Vagrantfile per project, and the Vagrantfile is supposed to be committed to version \ncontrol. \nThe syntax of Vagrantfiles is Ruby, but knowledge of the Ruby programming language is not necessary to make \nmodifications to the Vagrantfile, since it is mostly simple variable assignment. \n \n# vagrant up \n========== \nIn less than a minute, this command will finish and you will have a virtual machine running centos 7. You will not actually \nsee anything though, since Vagrant runs the virtual machine without a UI. \n# vagrant ssh \n=========== \nThis command will drop you into a full-fledged SSH session. Go ahead and interact with the machine and do whatever you \nwant. \n  \n  How to work with vagrant \n   \n 1. create a project directory \n   \n 2. create Vagrantfile (configuration file - # vagrant init) \n   \n 3. define the image name you want to use \n \n   Let’s go and do these steps \n   \n # mkdir project1 \n   \n # cd project1 \n   \n It's similar to git, whenever you start with git we say # git init \n   \n similarly for vagrant we use \n   \n # vagrant init \n \n   \n Open Vagrantfile \n   \n \n →  It's a ruby conϐiguration ϐile \n   \n \n →  I'll remove unwanted things like commented section, just keep \n   \n \n \n \n Vagrant.configure(\"2\") do |config| \n   \n \n \n \n \n config.vm.box = \"base\" \n   \n \n \n \n end    \n  \n \nI want centos-7 box so let's search for it in vagrantup.com \n   \n Change the vagrantfile # vi Vagrantfile \n   \n \n \n \n Vagrant.configure(\"2\") do |config| \n   \n \n \n \n \n config.vm.box = \"centos/7\" \n   \n \n \n \n end \n \n   \n \n # vagrant up   \n \n \n \n \n# vagrant up    {this command does the following} \n   \n \n \n 1. It will create a VM with name \"default\" \n   \n \n \n 2. It downloads the centos image to project dir\n3. Start the VM (by defualt it will be 1 CPU, 512MB RAM) \n   \n \n \n 4. Creates NAT n/w \n   \n \n \n 5. Setup SSH port forwarding \n   \n \n \n 6. Installs SSH keys \n   \n \n \n 7. Maps the storage \n   \n \n \n 8. Execute provision script \n   \n# vagrant up    {execute the command, this brings up the machine} \n \n   \n \n # vagrant ssh {logs you into the machine}     \n \n   \n \n # vagrant halt    {brings down the machine} \n \n# vagrant status {status} \n \n   \n \n # vagrant port {port forwarding} \n \n \n \n \n \nNetwork \n======= \n \nIn order to access the Vagrant environment created, Vagrant exposes some high-level networking options for things such \nas forwarded ports, connecting to a public network, or creating a private network. \n \nPort Forwarding \n============= \nPort forwarding allows you to specify ports on the guest machine to share via a port on the host machine. This allows you \nto access a port on your own machine, but actually have all the network traffic forwarded to a specific port on the guest \nmachine. \n \n \nLet us setup a forwarded port so we can access Apache in our guest. Doing so is a simple edit to the Vagrantfile, which now \nlooks like this: \n \nVagrant.configure(\"2\") do |config| \n  config.vm.box = \"hashicorp/precise64\" \n  config.vm.provision :shell, path: \"bootstrap.sh\" \n  config.vm.network :forwarded_port, guest: 80, host: 4567 \nend \n \n \nRun a # vagrant reload or # vagrant up. Once the machine is running again, load http://127.0.0.1:4567 in your browser. \nYou should see a web page that is being served from the virtual machine that was automatically setup by Vagrant. \n \nVagrant also has other forms of networking, allowing you to assign a static IP address to the guest machine, or to bridge \nthe guest machine onto an existing network.\nBy default vagrant uses NAT network provided by your provider aka Virtual Box. \nBut let say you want to use bridge network to get connected to router, just uncomment the “public_network”, and when \nyou do # vagrant up, it asks for the network to connect. \n \n# config.vm.network \"public_network\" \n \n# config.vm.network :public_network, bridge: \"en0: Wi-Fi (AirPort)\" \n \n \n \nConfigure Hostname \n================= \n \n# config.vm.hostname = “centos” \n \nChanging RAM \n============= \n# Customize the amount of memory on the VM: \nconfig.vm.provider \"virtualbox\" do |vb| \n \nvb.memory = \"1024\" \nend \n \nChanging CPU \n============= \n# Customize the number of CPU’s on the VM: \nconfig.vm.provider \"virtualbox\" do |vb| \n \nvb.cpus = \"2\" \nend \n \nProvisioning \n=========== \nProvisioners in Vagrant allow you to automatically install software, alter configurations, and more on the machine as part \nof the vagrant up process. \n \nOf course, if you want to just use vagrant ssh and install the software by hand, that works. But by using the provisioning \nsystems built-in to Vagrant, it automates the process so that it is repeatable. Most importantly, it requires no human \ninteraction. \n \nVagrant gives you multiple options for provisioning the machine, from simple shell scripts to more complex, industry-\nstandard configuration management systems. \n \nProvisioning happens at certain points during the lifetime of your Vagrant environment: \n● On the first vagrant up that creates the environment, provisioning is run. If the environment was already created \nand the up is just resuming a machine or booting it up, they will not run unless the --provision flag is explicitly \nprovided. \n● When vagrant provision is used on a running environment. \n● When vagrant reload --provision is called. The --provision flag must be present to force provisioning. \nYou can also bring up your environment and explicitly not run provisioners by specifying --no-provision.\nIf we want to install web server in our server. We could just SSH in and install a webserver and be on our way, but then \nevery person who used Vagrant would have to do the same thing. Instead, Vagrant has built-in support for automated \nprovisioning. Using this feature, Vagrant will automatically install software when you vagrant up so that the guest machine \ncan be repeatedly created and ready-to-use. \n \n \nWe will just setup Apache for our basic project, and we will do so using a shell script. Create the following shell script and \nsave it as bootstrap.sh in the same directory as your Vagrantfile. \n \n \nbootstrap.sh \n========== \n \nyum -y install git \n \nyum -y install httpd \nsystemctl start httpd \nsystemctl enable httpd \n \ngit clone https://github.com/devopsguy9/food.git /var/www/html/ \n \nsystemctl restart httpd \n \nNext, we configure Vagrant to run this shell script when setting up our machine. We do this by editing the Vagrantfile, \nwhich should now look like this: \n \n \n \nVagrant.configure(\"2\") do |config| \n   \n \nconfig.vm.box = \"centos/7\" \n   \n \nconfig.vm.provision :shell, path: \"bootstrap.sh\" \n \nend \n \nThe \"provision\" line is new, and tells Vagrant to use the shell provisioner to setup the machine, with the bootstrap.sh file. \nThe file path is relative to the location of the project root (where the Vagrantfile is). \n \nAfter everything is configured, just run vagrant up to create your machine and Vagrant will automatically provision it. You \nshould see the output from the shell script appear in your terminal. If the guest machine is already running from a \nprevious step, run vagrant reload --provision, which will quickly restart your virtual machine. \n \nAfter Vagrant completes running, the web server will be up and running. You can see the website from your own browser. \nThis works because in shell script above we installed Apache and setup the website. \n \nLoad Balancing \n============= \nIn this project we are going to work with three different machines, \nIn our previous Load balance example we took one nginx and two apache servers. \n \nBut in this example we will be using three nginx servers:\nVagrantfile \n========== \nVagrant.configure(\"2\") do |config| \n \nconfig.vm.define \"lb1\" do |lb1| \n    \nlb1.vm.box = \"ubuntu/trusty32\" \n    \nlb1.vm.network \"private_network\", ip: \"192.168.45.10\" \n   lb1.vm.network :forwarded_port, host: 7777, guest: 80 \n \n    \nlb1.vm.provision \"shell\", path: \"provision-nginx.sh\" \n \nend \n \nconfig.vm.define \"web1\" do |web1| \n    \nweb1.vm.box = \"ubuntu/trusty32\" \n    \nweb1.vm.network \"private_network\", ip: \"192.168.45.11\" \n   web1.vm.network :forwarded_port, host: 8888, guest: 80  \n    \nweb1.vm.provision \"shell\", path: \"provision-web1.sh\" \nend \n \nconfig.vm.define \"web2\" do |web2| \n    \nweb2.vm.box = \"ubuntu/trusty32\" \n    \nweb2.vm.network \"private_network\", ip: \"192.168.45.12\" \n   web2.vm.network :forwarded_port, host: 9999, guest: 80  \n    \nweb2.vm.provision \"shell\", path: \"provision-web2.sh\" \nend \n \nend \n \nprovision-nginx.sh \n================ \n#!/bin/bash \n \necho \"Starting Provision: Load balancer\" \nsudo apt-get -y install nginx \nsudo service nginx stop \nsudo rm -rf /etc/nginx/sites-enabled/default \nsudo touch /etc/nginx/sites-enabled/default \n \necho \"upstream testapp { \n \nserver 192.168.45.11; \n \nserver 192.168.45.12; \n} \n \nserver { \n \nlisten 80 default_server; \n \nlisten [::]:80 default_server ipv6only=on; \n \nserver_name  localhost;\nroot      /usr/share/nginx/html; \n \n#index index.html index.htm; \n \n \nlocation / { \n \nproxy_pass http://testapp; \n} \n \n}\" >> /etc/nginx/sites-enabled/default \nsudo service nginx start \necho \"MACHINE: LOAD BALANCER\" >> /usr/share/nginx/html/index.html \necho \"Provision LB1 complete\" \n \n \nprovision-web1.sh \n================== \necho \"Starting Provision on A\" \nsudo apt-get install -y nginx \necho \"<h1>MACHINE: A</h1>\" >> /usr/share/nginx/html/index.html \necho \"Provision A complete\" \nprovision-web2.sh \n================== \necho \"Starting Provision on B\" \nsudo apt-get install -y nginx \necho \"<h1>MACHINE: B</h1>\" >> /usr/share/nginx/html/index.html \necho \"Provision B complete\" \n \n# vagrant status \n# vagrant global-status \n# vagrant port lb1 \n# vagrant port web1 \n \nRunning Provisioners \n================== \nProvisioners are run in three cases: \n# vagrant up \n# vagrant provision \n# vagrant reload --provision\nDOCKER \n \nDocker \n======= \n \nDocker is a container management service. \n \nThe keywords of Docker are develop, ship and run anywhere. \n \nThe whole idea of Docker is for developers to easily develop applications, ship them into containers which can then be \ndeployed anywhere. \n \n \n \nRelease of Docker was in March 2013 and since then, it has become the buzzword for modern world development. \nFeatures of Docker \n================= \n● Docker has the ability to reduce the size of development by providing a smaller footprint of the operating \nsystem via containers. \n● With containers, it becomes easier for teams across different units, such as development, QA and Operations to \nwork seamlessly across applications. \n● You can deploy Docker containers anywhere, on any physical and virtual machines and even on the cloud. \n● Since Docker containers are pretty lightweight, they are easily scalable. \n \n \nWhy Virtualization ?? \n================== \n● Hardware Utilization \n● To reduce no of physical servers \n● Reduce Cost \n● More different OS\nYour whole design of virtualization, is to target the Applications. \n \nWe are focusing more on Hardware, Virtualization and OS. \nBut no one is focusing on Application side, \n \nOn Application side we need two fundamental characteristics: \n \nData Isolation & Data Protection \n \nLet say we are having 3 VM’s and minimum requirements for this system are: \n \n1 CPU & 1 GB RAM now like this i need 3 CPU and 3 GB RAM \n If the same things is needed for like 1000+ machines, it becomes more cumbersome. \n \nSo Docker took advantage of this, by using CONTAINERIZATION. \n \nUsing Docker we can build up entire application with OS. \n \nSometimes it happens like this application works fine on Linux OS, but doesn’t work on Unix and Win, these kind of \nproblems can be avoided by using Docker \n \nUsing docker we can create entire application with OS itslef(OS dependent files). \n \n \nDocker uses special file system, \n   Layered File system[COW - Copy On Write] \n \n   Box  \n→ \n VM's \n   AMI     → \n Instances \n   Images  \n→ \n Containers \n \nThree important things to check in docker: \n \nDocker Container, Docker Images & Docker Registry. \n \nDocker Container: is a running instance of an OS image. \n \n \n       Run time object \n \n \n \n \n Installation \n ============ \n    \n   # get.docker.com \n   # sudo usermod -aG docker <user-name> \n \n   Docker comes in two components SERVER & CLIENT \n   # systemctl start docker \n \n   # sudo docker <options>\nFirst thing we need to have is images, from those images will create containers. \n \n   \n # rpm -qa | grep docker \n   \n # sudo systemctl status docker \n \n   \n Let’s see do we have any images \n   \n # sudo docker images \n \nSo where do i get images from ?? \n========================== \nhub.docker.com {search for nexus, jenkins, tomcat and centos} \n \n# docker pull centos {this is how we get image from internet} \n \n# sudo docker images \n{ unique image id and size is also very less coz its limited OS } \n \n# docker info \n# docker images \n \nI want to see how many containers are running ?? \n# sudo docker ps \n \n \n# open two sessions of the same instance {we can do things simultaneously} \n# s1 - sudo docker ps {shows running containers} \n# s2 - sudo docker run -it centos /bin/bash \n{now we are inside container} \n \nit - terminal interactive \n \n \n# s1 - sudo docker ps {shows running containers} \nThis container is created from centos image \n \nIf we use container it will take at least 2 min but. here its 3 seconds \n# s1 - top {so many tasks} \n# s2 - top {literally 2 process} \n# s2 - ps -ef {same 2 process} \n# s2 - cat /etc/hosts {my hostname is container_id} \ns1 - sudo docker ps \n# s1 - sudo docker ps \n# s2 - exit {bash is finished - container is gone} \n# s1 - sudo docker ps \n \n# docker images\n# docker run -it { attached mode runs in foreground } \n \n# docker run -dt { detached mode runs in background } \n# docker exec -it <container-id> bash \n \nControlling the container \n===================== \n   \n \n \n \nnaming container \n================ \ns2 - # sudo docker run --rm -ti --name \"web-server01\" docker.io/centos /bin/bash \ns1 - sudo docker ps \ns2 - exit \n \nsetting hostname     \n================ \ns2 - sudo docker run --rm -ti --name \"web-server1\" --hostname \"web-server\" docker.io/centos /bin/bash \ns2 - cat /etc/hostname \ns2 - hostname \ns2 - exit \ns1 - sudo docker ps \n \nNow let’s do something interesting \n=========================== \n \n# docker run -it --name demo1 ubuntu /bin/bash \n \n# apt-get update \n \nMy image here is ubuntu, am i downloading the image again ?? \n \nI’m using the same image to build another container, \nso now my container is ubuntu + apt-get update. \n \nEverytime i work on an image, my container has an update. \nWhenever u start working on an image, that is when u start building a container. \n \nNow i’m doing new thing \n# apt-get install apache2 \nNow my container is  demo + apt update + install apache2 \n \nI’ll do exit \n# exit \n \nNow what is command to see the exit ed containers: \n \n# docker ps -a\nContainer lifetime and Persistent data \n================================ \nContainers are usually immutable and ephemeral, just fancy buzzwords for unchanging and temporary or \ndisposable, but the idea here is that we can just throw away the container and create a new one from an image right!!!!. \n Containers are Ephemeral and once a container is removed, it is gone. \n \nWhat about scenarios where you want the applications running inside the container to write to some files/data and then \nensure that the data is still present. For e.g. let’s say that you are running an application that is generating data and it \ncreates files or writes to a database and so on. Now, even if the container is removed and in the future you launch another \ncontainer, you would like that data to still be there. \n \nIn other words, the fundamental thing that we are trying to get over here is to separate out the container lifecycle from the \ndata. Ideally we want to keep these separate so that the data generated is not destroyed or tied to the container lifecycle \nand can thus be reused. This is done via Volumes, which we shall see via several examples. \n \nSo we are not talking about actual limitation of containers, but more of design goal or best practise, this is the idea of \nimmutable infrastructure \n \nSo docker has two solutions to this problem known as Volumes and Bind mounts.  \n \nWorking with volumes   \n=================== \nThere are three main use cases for Docker data volumes: \n1. To keep data around when a container is removed \n2. To share data between the host filesystem and the Docker container \n \nBy Docker Volumes, we are essentially going to look at how to manage data within your Docker containers. \n \n \nFew points about volumes \n====================== \n \n \n● A data volume is a specially designed directory in the container. \n● It is initialized when the container is created. By default, it is not deleted when the container is stopped. It is not \neven garbage collected when there is no container referencing the volume. \n● The data volumes are independently updated. Data volumes can be shared across containers too. \n \n \nMounting a Data volume \n==================== \nWe are going to use the -v [/VolumeName] as an option to mount a volume for our container. \n \n # docker run -it -v /data --name container1 ubuntu bash \n \n \nIn the container do # ls \n Notice that a volume named data is visible now.\nLet us a do a cd inside the data volume and create a file named file1.txt \n \n# cd data \n# touch file1.txt \n \nSo what we have done so far is to mount a volume /data in the container. \nNow, let us exit the container by typing exit \n \n# exit \n# docker ps \n # docker ps -a \n# docker inspect container1 \n# sudo ls var/lib/docker/volumes \n \nYou will see that it shows our file1.txt that we created. \nNow that the container is stopped i.e. exited, let us restart the container (container1) and see if our volume is still \navailable and that file1.txt exists. \n \n# docker start container1 \n# docker exec -it container1 bash \n \nNow, let’s do an interesting thing. Exit the container and remove the container. \n# docker rm container1 \n# sudo ls var/lib/docker/volumes \n \nThis shows to you that though you have removed the container1, the data volume is still present on the host. This is a \ndangling or ghost volume and could remain there on your machine consuming space. \n \nDo remember to clean up if you want. Alternatively, there is also a -v option while removing the container. \n \n# docker volume rm data { Removes the volume data } \n \nWe got three different implementations in volumes: \n1. Anonymous volumes \n2. Named Volumes \n3. Bind Mounts \n \nAnonymous volumes: don’t have a name it’s not so easy to work with Anonymous volumes if there are multiple \nAnonymous volumes. \n \n#  docker run -it -v /my-data --name container1 ubuntu bash \n \nNamed volumes: have a name to identify them so it’s easy to work with names if there are multiple volumes we can easily \nidentify them with names. \n \n#  docker run -it -v vol1:/my-data --name container1 ubuntu bash\nBind Mounts: same process of mounting a volume but this time we will mount an existing host folder in the Docker \ncontainer. This is an interesting concept and is very useful if you are looking to do some development where you \nregularly modify a file in a folder \n \n#  docker run -it -v /home/path:/my-data --name container1 ubuntu bash \n \nMySQL Volumes Example \n===================== \n \n# goto hub.docker.com \n# search for mysql and goto → Details → Click on latest Dockerϐile → Scroll down and you can see VOLUME \n/var/lib/mysql, this is the default location of MySQL Databases. \n \nThis mysql image is programmed in a way to tell docker, when we start a new container from it, it actually creates a new \nvolume location and assign it to this directory /var/lib/mysql, in the container, \n \n Which means any files we put in the container will outlive the container, until we manually delete the volume.   \n \nVolumes need manual deletion, you can’t clean them up just by removing the container that’s an extra setup with volumes, \nthe whole point of volume command is to say that this data is particularly important at least much more important than \ncontainer itself. \n \nNote: You might wanna do \n # docker volume prune \nto cleanup unused volumes and make it easier to see what you’re doing. \n \n# docker volume ls \n \nLet’s run a container from it: \n \n \n# docker container run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD=True mysql \n# docker container ls \n# docker volume ls \n# docker container inspect mysql { you can see Volumes /var/lib/mysql } \n \nAnd if you go up in the output, you can see Mounts, and this is actually the running container \nso actually the container actually thinks it’s getting data or writing data is from /var/lib/mysql, \n \nBut in this case, we can see the data is actually living in Source above line of /var/lib/mysql on the host. \n \nSo let’s do: \n # docker volume ls  \n \nIf you are doing this on a linux machine, You can actually navigate to the volume Source location { /var/lib/docker } and \ncan see the data, i.e some databases. \n \nAnd if i do just hit an up arrow and create a multiple mysql container:\n# docker container run -d --name mysql2 -e MYSQL_ALLOW_EMPTY_PASSWORD=True mysql \n# docker volume ls \n \n# docker container run -d --name mysql3 -e MYSQL_ALLOW_EMPTY_PASSWORD=True mysql \n# docker volume ls \n \n# docker container run -d --name mysql4 -e MYSQL_ALLOW_EMPTY_PASSWORD=True mysql \n# docker volume ls \n \n We can see two volumes but we can see the problem right ?? \nThere is no easy way to tell which volume belongs to which container. \n \n# docker container stop mysql \n# docker container stop mysql2 \n \n# docker container ls \n# docker volume ls \n # docker container rm -f mysql mysql2 mysql3 \n \n# docker volume ls \n{ my volumes are still there, my data is still safe, so we solved one prob } \n \nSo how we make little more user friendly ?? \n \nThat is where named volumes come in, the ability for us to specify names for docker volumes. \n \nNamed volume [ i can put a name in front of it with : that is known as named vol ] \n \n# docker container run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD=True -v mysql-db:/var/lib/mysql \nmysql   \n \n# docker volume ls \n{ you can see my new container is using a new volume and it’s using a friendly name } \n \n# docker volume inspect mysql-db { this is easier to use here} \n \nAnd if i removed my container: \n \n#  docker container rm -f mysql { -f coz it’s still running, if not stop & remove } \n \n \nAnd if i run another container with some other name and same volume: \n \n# docker container run -d --name mysql3 -e MYSQL_ALLOW_EMPTY_PASSWORD=True -v mysql-\ndb:/var/lib/mysql mysql \n \n# docker volume ls { only mysql-db is there }\nYou can see that we haven’t created a new volume, but still using the same mysql-db volume from earlier. \n \n# docker container inspect mysql3 \n \n{ and we can see Volumes, changed the Source location to be a little friendlier as well } \n \nMore on Mysql - Volumes \n===================== \nRunning mysql container with password: \n# docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=mypassword -d mysql \n \n \nThe following command line will give you a bash shell inside your mysql container: \n# docker exec -it some-mysql bash \n \n# mysql -u root -p \n \nThe MySQL Server log is available through Docker's container log: \n# docker logs some-mysql \n \nRemoving volumes along with container: \n \n# docker container rm -f -v some-mysql \n \nPersistent Data: Bind Mounting \nLook at the same process of mounting a volume but this time we will mount an existing host folder in the Docker \ncontainer. This is an interesting concept and is very useful if you are looking to do some development where you \nregularly modify a file in a folder outside and expect the container to take note or even sharing the volumes across \ndifferent containers. \n \nBind mounts are actually cool, this helps how to use docker for local development. \n \nSo really a bind mount is just a mapping of the host files or directories into a container file or directory. \nIn background, it’s just having two locations pointing to the same physical location(file) on the disk. \n \nFull path rather than just a name like volumes, the way actually docker can tell the difference between named volume \nand bind mount, is that bind mounts starts with a forward slash /  { root } \n \nNow where it really comes to shine is with development and running services inside your container, that are accessing the \nfiles you are using on your host which you are changing. So let’s do that with nginx: \n \ninstallation of Nginx \n================== \n \n# sudo docker run -d -P --name web-server nginx { -P random ports } \n \nNginx \n ===== \n# docker container run -d --name nginx1 -p 8080:80 nginx\n# docker container run -d --name nginx2 -p 80:80 nginx \n \nCheck the site in host machine http://ip-address:8080 \nThe site which we are seeing is default nginx html file, actually coming from default nginx image. \n \n \nNginx with Mountpoints example \n======================== \n \n# docker container run -d --name nginx -p 80:80 -v ~/website:/usr/share/nginx/html nginx \n \n \nNginx Example ( Volumes ) \n====================== \n# mkdir nginxlogs \n# sudo docker run -d -v ~/nginxlogs:/var/log/nginx -p 80:80 nginx \n \nI do this because every time i don’t want to go into the container and check the logs. \n \nNow i have all the logs in host machine itself rather than container. \n \nUsing multiple volumes on Single Container \n==================================== \nSaving both logs and website data \n \n# docker container run -v ~/nginx-logs:/var/log/nginx -v ~/website:/usr/share/nginx/html nginx \n \nWordpress example - Backup \n======================== \n \n## Creating DB \n# docker run -d --name=wp-mysql -e MYSQL_ROOT_PASSWORD=mypassword -v ~/mysql-data:/var/lib/mysql mysql \n# docker exec -it wp-mysql bash \n# mysql -u root -p \n # create database wordpress; \n \n \n## Creating WP \n# docker run --name my-wordpress --link wp-mysql:mysql -p 8080:80 -d -v ~/wp-data:/var/www/html wordpress \n \nFor Custom Docker Images go with following url \nhttps://github.com/ravi2krishna/Node-Js-Sample-App\nNAGIOS \nNagios \n======Nagios is a monitoring tool. \n \nIn your organization, you will be working on different different environments like dev, test, pre-prod, prod etc. \n \nNow let's say you have 1000 systems in your infrastructure, so daily it's a tedious task to go and understand what is the \nstatus of these 1000 devices. \n \nFor monitoring all these servers we use NAGIOS. \n    \nWhat we monitor ?? \n================ \n   1. Health    \n  \n{device is up/down} \n   2. Performance    {RAM & CPU utilization} \n   3. Capacity    \n {Watch HDD capacity} \n \nThreshold of Monitoring \n====================== \n   Warning    \n 85% \n   Critical    95% \n \nParameters to monitor \n===================== \n   CPU \n   RAM \n   Storage \n   Network etc \n \n \nNAGIOS SERVER SETUP \n====================== \n \n \nNAGIOS CORE \n============= \n \nGoto nagios.com and nagios.org \n   On nagios.org →  Downloads →  Nagios Core \n \nOn Host Machine \n   # mkdir nagios-software \n   # cd nagios-software \n \ncopy #link of nagios-core.tar.gz\n# wget <nagios-core-link> \n     # extract the tar \n    \n# sudo yum install -y wget httpd php gcc glibc glibc-common gd gd-devel make net-snmp unzip openssl-devel \n# sudo yum install httpd php php-cli gcc glibc glibc-common gd gd-devel net-snmp openssl-devel wget unzip -y \n \n# sudo useradd nagios \n# sudo groupadd nagcmd \n# sudo usermod -a -G nagcmd nagios \n# sudo usermod -a -G nagcmd apache \n# cd nagios-4.2.0 \n# ./configure \n# make all \n# sudo make install \n# sudo make install-init \n# sudo make install-config \n# sudo make install-commandmode \n# sudo make install-webconf      \n# sudo cp -R contrib/eventhandlers/ /usr/local/nagios/libexec/ \n# sudo chown -R nagios:nagios /usr/local/nagios/libexec/eventhandlers \n \n# sudo /usr/local/nagios/bin/nagios -v /usr/local/nagios/etc/nagios.cfg  [checking for syntax errors and to see if \neverything is working fine ] \n \nCreating nagiosadmin user account \n# sudo htpasswd -c /usr/local/nagios/etc/htpasswd.users nagiosadmin \n \n# sudo service nagios restart \n# sudo service httpd restart \n \nNAGIOS PLUGINS \n================ \n \nOn nagios.org →  Downloads →  Nagios Core Plugin \n    \n# wget <link-nagios-plugins> \n# extract the tar \n \n# cd nagios-plugin-x \n# sudo ./configure \n# make \n# sudo make install \n \n \nNRPE PLUGIN \n=============\nTo get monitor a system we are going to install NRPE plugin, \nNRPE - Nagios Remote Plugin Executor \n \nGoto nagios.org → Nagios core plugin → Find more plugins → General Addons → NRPE → Copy download URL \n \n \n# wget <link-nrpe> \n# cd nrpe \n# ./configure \n \n# sudo make all \n# sudo make install \n# ls -l /usr/local/nagios/libexec/check_nrpe    {installed successfully} \n \n# sudo service nagios restart \n# sudo service httpd restart \n \nTo view Nagios server Dashboard : # ip-add/nagios \n \n \n \n \n \nNAGIOS CLIENT/AGENT SETUP \n========================== \n \nUse another linux machine either on cloud or vm \n \n# sudo useradd nagios \n# sudo yum install -y wget php gcc glibc glibc-common gd gd-devel make net-snmp unzip openssl-devel net-tools xinetd \n \nNAGIOS PLUGINS \n================ \n \nDownloads →  Nagios Core Plugin \n    \n# wget <link-nagios-plugins> \n# extract the tar \n \n# cd nagios-plugin-x \n# ./configure \n# make all \n# sudo make install\nNRPE PLUGIN \n \nTo get monitor a system we are going to install NRPE plugin, \nNRPE - Nagios Remote Plugin Executor \n \nGoto nagios.org → Nagios core plugin → Find more plugins → General Addons → NRPE → Copy download URL \n \n# wget <link-nrpe> \n# cd nrpe \n \n# sudo chown -R nagios:nagios /usr/local/nagios/libexec \n# ./configure \n# make \n# make all \n# sudo make install \n \n# sudo mkdir -p /usr/local/nagios/etc \n# cd nrpe {dir - make sure you are in nrpe directory } \n# sudo cp sample-config/nrpe.cfg /usr/local/nagios/etc \n# cd sample-config \n# sudo vi sample-config/nrpe.xinetd \n \nservice nrpe \n{ \n       flags           = REUSE \n       port            = 5666 \n       socket_type     = stream \n       wait            = no \n       user            = nagios \n       group           = nagios \n       server          = /usr/local/nagios/bin/nrpe \n       server_args     = -c /usr/local/nagios/etc/nrpe.cfg --inetd \n       log_on_failure  += USERID \n       disable         = no \n      only_from       = 127.0.0.1 <ip-add-server> \n} \n \n# sudo cp sample-config/nrpe.xinetd /etc/xinetd.d/nrpe \n# vi /etc/xinetd.d/nrpe \nChange allow from to nagios server ip \n \n# vi /etc/services \n   Add →  nrpe            5666/tcp                # NRPE service \n# ls -l /usr/local/nagios    {i should be nagios:nagios} \n# chown -R nagios:nagios /usr/local/nagios \n# sudo service xinetd start \n# netstat -ntpl\nConfiguring Agent \nIn Server machine \n# cd /usr/local/nagios/etc \n# sudo touch hosts.cfg \n# sudo touch services.cfg \n# sudo vi /usr/local/nagios/etc/nagios.cfg [ goto OBJECT CONFIGURATION FILE(S) ] \n Add the following lines below templates.cfg \n# This config is to add agents \ncfg_file=/usr/local/nagios/etc/hosts.cfg \ncfg_file=/usr/local/nagios/etc/services.cfg \n \n## Default \n \ndefine host{ \nuse generic-host ; Inherit default values from a template \nhost_name c1 ; The name we're giving to this server \nalias CentOS 7 ; A longer name for the server \naddress 192.168.44.11; IP address of Remote Linux host \nmax_check_attempts 5; \n} \n \n# sudo vi /usr/local/nagios/etc/services.cfg \n \ndefine service{ \nuse generic-service \nhost_name c1 \nservice_description CPU Load \ncheck_command check_nrpe!check_load \n} \n \ndefine service{ \nuse generic-service \nhost_name c1 \nservice_description Total Processes \ncheck_command check_nrpe!check_total_procs \n} \n# sudo vi /usr/local/nagios/etc/objects/commands.cfg \n \nAdd the following to end of the file \n \n# Command to use NRPE to check remote host systems \ndefine command{ \ncommand_name check_nrpe \ncommand_line $USER1$/check_nrpe -H $HOSTADDRESS$ -c $ARG1$  }\nAnd check for any syntax errors \n# sudo /usr/local/nagios/bin/nagios -v /usr/local/nagios/etc/nagios.cfg \nYou should have Zero warnings and errors \nYou can see Checked hosts \n \n# sudo /etc/init.d/nagios restart \n# sudo systemctl restart httpd \n \n \n \nCHEF \n \nWhen it comes to learning chef \n● \nYou bring your business and problems \n● \nYou know about your infrastructure, you knew the challenges you face within your infrastructure and \nyou know how your infrastructure works \n● \nChef will provide a framework to solve those problems  \n \n \nThe Best way to learn chef is to use chef. \n \nIn chef terminology is very important \n \nCOMPLEXITY \n===========\nSystem administrators will have a lot of complexity to manage. \nThis complexity can be from many items (Resources) across the infrastructure. \nWe call these items as resources. \n \n \nResources \n========= \nThe resources in your infrastructure can be files, directories, users that you need to manage, packages that should be \ninstalled, services that should be running and list goes on. \n \n \n \nLet’s look at typical application \n========================== \nYou usually start configuring and installing that application on a single server(node). \n \n \n \nTo set up this node we may need to install packages, manage their configurations, installing a database, installing web \nserver, installing application server and lots of things to make this application up and running. \n \nNow overtime you wanna take this application and make it available to the public/client. \n \nSo you are going to have a database server, so maybe now we are going to have multiple environments like staging, dev, \nqa or even production i.e multi tier environment.\nWe have one server that handles all the Application requests and a separate server for Database. \n \nOf course once we have a database server, we want to make sure we don’t lose data, so we decide to make database \nserver redundant. We want to prevent failure and loss of the data. \n \n \n \nWe may as well make the Application server redundant as well. \n \n \n \nSo now we are up to 4 Servers. \n \n========\nBut of course as time goes on, load increases on our application, and we need to scale out the no of application servers that \nwe have and in order to do that we need to put a load balancer in front of our app servers, so that requests can be evenly \ndistributed. \n \n \n \n============ \nEventually we gonna reach web scale, this application has grown and grown and getting bigger and bigger and we are now \nat web scale. Look at all these servers now we have to manage. \n \n   \n \n======== \n \nNow we ran into another problem, our database just isn’t keeping up with the demands from our users, so we decided to \nadd a caching layer. \nNow i got even more servers and more complexity to infrastructure.\n========== \n \nAs if this no of servers are not complex enough, and of course each infrastructure has its own topology. There are \nconnections between the load balancer and Application servers. \n \nThe load balancers need to know which application servers to talk to, which application server they are sending requests \nto. \n \nThe Application server in turn needs to know about the database server. \n \nThe Database cache servers need to know what are the backend database servers which they need to do caching. \n \nAll of this just adds up more and more complexity to your infrastructure.\n========= \nAnd your complexity is only going to increase and its increasing quickly. \n \n \n \n \n============ \n \nSo chef solves this problem for you. \n \n \n \n========\nSo how can we manage complexity with chef ?? \n \nChef gives us a no of items/tools with which we can manage this complexity, we are going to look at this \n \n \n \n \nWe are going to look at organizations, and how organizations can help you manage complexity. \n \nSo let’s start with top level, let’s see Organizations: \n \n \n \nIf you think about organizations, let’s take digital lync itself it has its own infrastructure and TCS has its own infrastructure \netc etc \n \n \n=====\nOrganizations are independent tenants on enterprise chef. \n \nNothing is shared across the organizations, your organizations may represent different companies take Tata group as \nexample. \n \n======= \n \nThe next layer down is environments. \n \n======== \n \nNext comes Roles \n \nRoles is a way of identifying or classifying different types of servers that you have within your infrastructure.\nWithin your infrastructure you have multiple instances of servers that are in each one of these roles, you may have \nmultiple application servers, multiple database servers etc \nYou will specify this as roles in chef. \n \n=========== \n \nRoles allow you to define policy, they may include list of chef configuration files that should be applied to the servers or \nnodes that are within that role. \n \nWe call this list of chef configuration files that should be applied we call this a Run list.\n================ \n \nStepping down from roles then we look at nodes\nBelong to Environment : Server can be either in dev or testing or staging or production \n \nRoles : Each node may have zero or more roles, so a node maybe a database server or an application server or it can have \nboth roles the same node can have both database and the application servers. \n \n===========\nChef client will do all of the heavy lifting, it will make updates, it will configure the node, such that it adheres to the policy \nthat is specified on the chef server. \n \n======== \n \n \n \nBy capturing your infrastructure as code, you can reconstruct all of your business applications from three things a code \nrepository, a backup of your data and the bare metal resources or compute resources that are required to run your \napplications. \nThis puts you in a really really nice state and it’s a great way to manage your complexity of your infrastructure, with these \nthree simple things i can rebuild my applications. \n \n================\nStore the configuration of infrastructure in version control: \nSo gone are the days when you have a server that was hand crafted lovingly by a system administrator and that system \nadministrator is the only person in your organization that knows all of the knobs, dials, tweaks, tricks, packages etc that \nhave been placed onto that server, now we can take the knowledge of that system administrator has and move it into a \nframework that can be stored in a Source code repository. \n \nFramework that allows for abstraction so that you can build up bits and pieces and transform the way you manage your \ninfrastructure. \n \n============ \n \n \nWith chef you can define policies that your infrastructure should follow, \n \nYour policy states what each resource should be in, but not how to get there \nFor example we will in our chef configuration file, we will say that a package should be installed but we will not need to \nspecify how to install that particular package, chef is smart enough to sort that out. \n \nSo for example if you want to install a package on a debian based system you may use apt package manager to install that \npackage, # apt-get install <pkg-name> and \nIf you are running on a redhat based distribution such as centos then apt will not work here we would rather use yum \npackage manager # yum install <pkg-name>\nChef abstracts all of that away from you so that you can write configuration files that will work across the various \nplatforms. \n \n======== \n \n======== \n \n \n \n \nYou will take resources and gather them together into recipes. \n \n=========\nRecipes are the real work forces within chef. \n \nSo resources are the building blocks and will take those building blocks and we would gather them together into recipes \nthat would help bring our systems in line with policy. \n \n \n \nLet’s see some example recipe code \n \n \n \n \nI will walk through you what happens when the chef client encounters this recipe code \n \nSo Chef-client is an application that runs on the nodes within your infrastructure. \nIt will gather it’s policy, where its run list from the chef server and it will inspect current system configuration and it will \nthen execute through this recipes or walk through these recipes and ensure the node is in desired state or the node \ncomplies with policy.\nThe first resource this recipe includes is a package resource and this package is named apache2, when the chef client sees \nthis, it knows that the package named apache2 should be installed on this particular server/node, if it is not installed the \nchef-client will go ahead and installs that for you. \n \nAgain we are not telling the chef-client, how to install apache2, it’s smart enough to figure that out on its own. \n \nSo our policy states that the package apache2 should be installed, if this is the case already, then the chef-client will  move \non to the next resource in our recipe. \n \nIf that package is not yet installed, chef-client will take care to install it and then move  on to next resource in our recipe. \n \nWith chef we are going to take this recipes and package them up into cookbooks. \n \n========= \n \nSo a cookbook is a container that we will use to describe, configuration data and configuration policies about our \ninfrastructure. \nA cookbook may certainly contain recipes, but it can also include templates, files etc \n \n \n \nSo the recipe we just looked at had template resource in it, the template resource itself had a source file \napache2.conf.erb, that source file is stored as part of the same apache2 cookbook. \nThis cookbooks allow code reuse and modularity. \n \n======== \n \nLet’s look what happens at very very high level, when the chef-client runs on the node.\nSo node is a server in our infrastructure, on the node we have an application called chef-client, this is typically configured \nto execute on a regular interval maybe every 15-30 min using cron job. \n \nWhen chef-client executes it will ask the enterprise-chef(chef-server), what policy should i follow, all of our policy is \ndescribed on the chef-server, our policy includes things like our environments, our roles and our cookbooks. \n \nSo the joining of a node, to a set of policies, we call that as a run-list. \n \n \n \nSo the chef-client will download all of the necessary components, that make up the runlist and will move those down to \nthe node, so you’ll see here the run-list includes the recipe ntp-client, recipe to manage users, and the role to make this \nnode a web server. \n \n==========\nOnce the run-list has been downloaded to the node,the chef-clients job is to look at each of the recipes within that run-list \nand ensure that policy is enforced on that particular node, so it brings the node inline with policy. \n \n \n \n========= \n \nSo run-list is, how we specify policy for each node within our infrastructure. \nThe run-list is a collection of policies that node should follow. \nChef-client will obtain this run-list from the chef server. \nThen chef-client ensures the node complies with the policy in the run-list. \n \n \n \n=====\nSo with Chef this is how you manage complexity: \n \n \n \nThe first thing you will do, is to determine the desired state of your infrastructure. \nOne we done that, you will identify the resources required to meet that state, what resources are required to meet that \nstate, what resources are required users, services, packages etc. \nYou gather up each of these resources into recipe. \nAgain the run-list is the thing that gets applied to the node, you apply a run-list to each node within your environment and \nas the chef client runs on those nodes, your infrastructure will adhere to the policy modeled in the chef. \n \nThis is a great way to launch new servers to add the capacity to your infrastructure, this is how you add new capacity to an \nexisting infrastructure. \n \nAdding new capacity to an existing infrastructure is not, the only challenge that we face, the other challenge we face is \nConfiguration Drift. \n \n======= \n \n \n \n \n \n \nConfiguration drift happens when your infrastructure requirements change. \nNo infrastructure has static list of requirements, the requirements are certainly going to change over time.\nAdditionally a server within your infrastructure may fall out of line with policy, perhaps a system administrator logged \ninto a server and changed the port number that an application was listening to, maybe that shouldn’t have happened it's \noutside the policy, how do we address that, well chef makes it very easy to manage this, we are going to model the new \nrequirements that we have in chef configuration files and then re-run the chef-client, so when chef-client runs it will \nenforce that each node within your infrastructure is following the current and accurate policy as stored on chef server, so \novertime you can manage change across the infrastructure and you can enforce this policies across the infrastructure. \n \nIn open source we can manage up to 25 nodes freely. \n \nOn workstation side we use chef development kit (chefdk) \nChef-server we can work directly on cloud and we use chef-manage \nOn nodes we use chef-client \n \nSo we totally need three systems to workout. \n \nWorkstation \n \n \nChef-Server  \n \n \n \n  Node \n   MACH1 \n \n \n    MACH2 \n \n \n \nMACH3 \n \nInstall Chef-Server \n=============== \nLet’s get the Chef-server from https://downloads.chef.io/chef-server \n \n# mkdir chef-sw \n# wget <link-of-rhel-7-distro-rpm> \n# sudo rpm -ivh chef-server.rpm \n \nOnce we installed it, we need to configure it by using command: \n \n# sudo chef-server-ctl reconfigure\nAbove command will set up all of the required components, RabbitMQ, PostgreSQL DB, SSL Certificates etc. \n \nOnce it is done successfully, it means server has been set successfully. \n \nstatus of chef server we can use: # sudo chef-server-ctl status which is going to give all the services which are running in \nthe background. \n \nNow let’s access the chef-server by ip add https://192.168.33.10/ \n \nNow let’s setup the web interface to manage chef-server, to get the GUI we need to install another package which is \nchef-manage. \n \n# sudo chef-server-ctl install chef-manage \n \nThis installation of chef-manage will glue the chef-manage to chef-core means it will integrate chef-manage with your \nchef-core. \n \n# sudo chef-manage-ctl reconfigure \n \nNext accept the license agreement and go on with further steps say q for quit and yes. \nOnce the installation is successful use the ip of machine to see the web interface: \n \n \nhttps://ip_address_machine \n \n \nCreate admin user and organization \n============================= \n \nCreating User \n============ \n \nWe need to create an admin user. This user will have access to make changes to the infrastructure components in the \norganization we will be creating. \nBelow command will generate the RSA private key automatically and should be saved to a safe location. \n \n# sudo chef-server-ctl user-create --help \n \n# chef-server-ctl user-create <USER_NAME> <FIRST_NAME> <LAST_NAME> <EMAIL> 'PASSWORD' -f \nPATH_FILE_NAME \n \n# sudo chef-server-ctl user-create admin admin admin admin admin@digital.com password -f \n/etc/chef/admin.pem \n \npem file is your private key which will authenticate you while logging into the server and the corresponding public key \nwill be stored in the server. We should not share this pem file coz now anyone with this pem file can login into the server. \n \nNow you have created the user successfully.\nCreating Organization \nIt is the time for us to create an organization to hold the chef configurations. \n \n# sudo chef-server-ctl org-create --help \n \n# chef-server-ctl org-create short_name 'full_organization_name' --association_user user_name --filename \nORGANIZATION-validator.pem \n \n# sudo chef-server-ctl org-create dl \"Digital-lync-academy\" --association_user admin -f /etc/chef/dl-\nvalidator.pem \n \nThat’s it this is how we will be creating organization name. \n \nLet’s access the web interface now. \n \n \nhttps://ip_address_machine \n \nOnce we logged in we can see Nodes, but we didn’t setup any node so will see this in later part. \n \nWe can see the organization, on the top right corner. \n \n \nSetting Up Workstation \n=================== \n \nSetting up workstation is very important coz even if you don’t know how to set up chef-server it’s okay coz in real time \nchef-server is already set-up but work station we need to set up yourself. \n \nSteps involved \n============ \n● Create a new centos machine for workstation \n● Login to the workstation machine \n● Setup hostname and add all three in /etc/hosts \n● Download chef-dk on workstation (laptop/desktop/vm) \n○ \nhttps://downloads.chef.io/chefdk \n○ \n# wget <link-of-chef-dk> \n○ \n# sudo rpm -ivh chefdk-1.3.43-1.el7.x86_64.rpm \n○ \n# sudo chef-client -v \n● Download chef-dk on workstation (laptop/desktop/vm) \n● Install the chef-dk through rpm \n● # cd ~ { workstation } \n● Generate Chef-Repo using “chef generate repo” command. \n○ \n# chef generate repo chef-repo \n○ \nThis command places the basic chef repo structure into a directory called “chef-repo” in your home \ndirectory. \n● In chef server we created two pem files one is user and another is organization, now we need to bring this two \npem files into the workstation\n● Now we need to create a directory .chef in chef-repo where we put RSA keys, \n○ \n# cd ~/chef-repo; # mkdir .chef; cd .chef; \n● Copy the RSA keys to the workstation ~/chef-repo/.chef \n○ \nMake the handshake b/w chef-server & workstation \n○ \n# scp /etc/chef/*.pem vagrant@workstation:~/chef-repo/.chef/ \nሁ Run the above in chef-server \n● Knife is a command line interface for between a local chef-repo and the Chef server. To make the knife to work \nwith your chef environment, we need to configure it by creating knife.rb in the “~/chef-repo/.chef/” directory. \n○ \nGoto Web interface of chef-server → Click Administration → Select Organization (dl) → Click on \nsettings wheel(extreme right) → Click on the Generate Knife Config \n○ \nCopy the file contents and paste it in ~/chef-repo/.chef/knife.rb \n● Testing knife: test the configuration by running knife client list command. Make sure you are in ~/chef-repo/ \ndirectory. \n○ \n# cd ~/chef-repo/ \n○ \n# knife client list \n○ \n# knife ssl check \n○ \nTo resolve this issue, we need to fetch the Chef server’s SSL certificate on our workstation \n○ \n# knife ssl fetch \nሁ The above command will add the Chef server’s certificate file to trusted certificate directory. ( # \ntree .chef) \n● Once the SSL certificate has been fetched, run the previous command to test the knife configuration. \n# knife client list { output then verification completed successfully } \n \n \nSetting Up Node / Bootstrapping new node with knife \n============================================ \n \nBootstrapping a node is a process of installing chef-client on a target machine so that it can run as a chef-client node and \ncommunicate with the chef server. \n \nFrom the workstation, you can bootstrap the node with elevated user privileges. \n \n# setup new machine \n# Setup hostname and add all three in /etc/hosts \n# Install wget \n# Make handshake between workstation to node1 using vagrant user in both machines \n# knife node list {nothing is seen run this command in workstation} \n \nShow the web interface of chef-server, there are no nodes as of now. \n \n# knife bootstrap --help {shows options} \n \n# knife bootstrap 192.168.33.92 -x vagrant -P vagrant -N node1 --sudo \n \nThis command runs the following \n○ \nIt runs ohai process and collects the node info \n○ \nUploads the info to chef server\n○ \nRegisters the node with  chef server \n○ \nRuns any cookbooks/recipes if defined \n \n \nLet’s write first recipe [ Recipes are in Workstation under cookbooks dir ] \n============================================================ \n# chef generate cookbook <cook-book> \n \n→ \nNew style (Chef 12+) \n \n \nEverything will do it on workstation. \n \n1. Create cookbook on workstation \n2. Then will upload this cookbook on server \n3. Then will associate the cookbook with node \n4. Then will execute that cookbook on that particular node \n \n \n(last step is automatic in real time, but for now will do it manually) \n \n# chef generate cookbook file-test [ makes sure under cookbooks dir ] \n \nknife cookbook create is the legacy way to create a cookbook, it builds the structure of a cookbook with all possible \ndirectories. \n \nchef generate cookbook is part of Chef-DK and aims at generating a testable cookbook structure with minimal \ndirectories to use in test-kitchen. \n \nBoth can be tweaked, the chef generate is easiest to tweak as the command has been written in this way to allow all to \nbuild the cookbook structure that better fits their needs. \n \n \nOnce we create this cook we need to understand the structure of this cookbook, so let’s see the tree structure of our \ncookbook # tree file-test \n \nThe no.1 directory you need to check is recipes under which we have default.rb where you will write your code, write \nyour work \n \n1 default.rb \n========= \nSo what will you write in recipe?? What is recipe ?? \nRecipe is collection of resources with desired state. \nSo your resource can be a file, directory, package etc. \n \n2 metadata.rb \n=========== \nMetadata.rb has the information about version no of cookbook, dependencies, organization info and so on all the \ninformation related with cookbook will be here. \nIf this cookbook is dependent on other bookbook then this info is also stored here. \n \n# sudo vim cookbooks/file-test/metadata.rb \n(change Maintainer etc)\nLet’s start creating our resources : \n \n# vim cookbooks/file-test/recipes/default.rb \nSo here we will be defining resources in ruby language, now it’s not necessary that you should be master in ruby lang to \nunderstand this, coz structure is very easy to understand. \n \ndefault.rb \n======== \nfile '/tmp/sample.txt' do \n    \ncontent 'This file is created with CHEF' \n    \nowner 'root' \n    \ngroup 'root' \n    \nmode '644' \n \n#action :create \nend \n \nEvery resource has got a default action, the action here is to create. \n \n# knife cookbook test file-test \n \nNow we created the cookbook but we need to upload this cookbook to server. \n \n# knife cookbook list \n{when i execute this command, it fetches list of cookbooks from server} \n \nNow if we check the same in web interface [Policy are cookbooks], i don’t have file-test cookbook, so will upload the \ncookbook. \n \n# knife cookbook upload file-test \n \nUploaded successfully, let’s confirm it. \n \n# knife cookbook list \n \nNow let’s check the same in web interface \n \nNow we need to associate this cookbook with a node. \n \nFirst let’s see what nodes are available \n \n# knife node list \n \nMore info about node \n \n# knife node show <node-name> \n \n# knife node show node1 \n \n \nOur focus will be on run list: \n \n# knife node show node1\nThere is nothing in Run List \n \nGoto web interface → Select node1 → Click on Edit run list { nothing }, let’s add one \n \nAdding node to run list \n \n# knife node run_list add <node-name> <cookbook-name> \n \n# knife node run_list add node1 file-test \n \nAgain i’ll go with show, previously we had empty run list \n# knife node show node1  { we can see file-test in run-list } \n \nNow let’s move to node machine \n \nRun the command # sudo chef-client \n \nThis command makes node to talk to server and asks server, do you have any runlist for me ?? Then it will run all the \ncookbooks mentioned in run_list, then it starts executing them. \n \n \n \n# sudo chef-client \n1. Runs the ohai process \n2. It will upload the latest host info to server \n3. It will get the list of runlist and its dependencies \n4. Downloads the cookbooks as mentioned in run_list \n5. Execute the run_list \n \nRESULT : you will get resource with desired state \n \nIf we run the command for the second time # sudo chef-client \n \nNothing is updated coz the resource is already in its desired state. \n \nLet’s do one thing, forcefully damage the system (someone made changes to sample file content) \n \n# vi /tmp/sample {made changes which corrupted the system} \n \nNow let’s run the # sudo chef-client again \n \nNow you can see that the file was not in desired state, bring it back to desired state. \n \nNow if you open and see the file content # vim /tmp/sample.txt it’s back to original all the old tampered data is gone away. \n \nThat’s the desired state, if you define the desired state, it will make sure that it has the desired state. \n \nLet’s tamper the permissions # chmod 777 sample.txt\nNow if i run # sudo chef-client then the file permissions will be back to normal desired state. \n \nNow that’s what chef is trying to do, whatever you define, it will try to control and make sure it stays that way, but \nif it’s already there in desired state then it will not make any changes and this is called IDEMPOTENCY. \n \nThat means if the state is already achieved then it will not disturb the state. \nNow imagine that you have 10000 systems, so you don’t have to worry about any of the system. You will write the \nrecipe and will execute them that’s it. \n \nNow let’s say you want to edit some content in sample.txt then you will be doing that on recipe, then it will automatically \nreflect on your node machines. \n \nThis is the basic resource we worked on, but in the upcoming session we would see some more recipes to work. \n \n \nToday let’s look deeper into cookbook and recipe configurations \n1. Will create the cookbook/recipe \n2. Will upload the recipe to server \n3. Will associate that particular cookbook with particular node \n4. Will login to node and will execute \n \n \nToday will install web server that is httpd, and will make sure that httpd package is up and running then will create one \nindex.html and will see everything is working fine as expected. Let’s do it one by one. \n \nSo we will create cookbook in workstation (under cookbooks directory) \n# chef generate cookbook webtest \n# vi webtest/metadata.rb (change maintainer and maintainer email) \n \nNow lets update the recipes \n \n# vi webtest/recipes/default.rb \n \n \nOn the command line we generally do \n \n# yum -y install httpd \n \n# systemctl start httpd \n \n# systemctl enable httpd \n \n# echo “WELCOME” > /var/www/html/index.html \n \nNow let’s login to node machine and disable firewall and set selinux to permissive \n \nNow we need to install the package, here in chef we have a resource with name package: \n \n# package ‘httpd’ do \n \n \naction :install \n {by default} \n \n# end\nBy default the action is install, but we want more information like what we can do more with package, so chef has \nexcellent documentation, lets go and check the document and see what more we can do with the package. \n \n# docs.chef.io/resource_package.html \n \nWe can install the package like this, \npackage ‘httpd’ do \n \n \naction :install  \n \nend \nOr simply you can say \npackage ‘httpd’  \n \ncoz the default action is install \n \n \nNow if you want to remove the package you can figure it out from document (actions) \n \npackage ‘httpd’ do \n \n \naction :remove  \n \nend \n \n \nNow i need to start the service, so i will use the second resource module, \nNow let’s go to document section again and see resource_service \nhttps://docs.chef.io/resources.html { select service} \n \n \n \n \nservice ‘httpd’ do \n \n \naction  [:start, :enable] now i can specify two actions also \nend \n \nOr \n \nservice ‘httpd’ do \n \n \naction  :start \nend \n \nservice ‘httpd’ do \n \n \naction  :enable \nend \n \nNow the third thing is we need to go with index.html, so i’ll use the same resource file \n \nfile ‘/var/www/html/index.html’ do \n \n \ncontent “<h1>Welcome to APACHE - By CHEF</h1>” \n \n \nowner ‘root’ \n \n \ngroup ‘root’ \n \n \nmode ‘644’ \n \nend\nNow need to save the changes then test it and upload it. \n \n# knife cookbook test webtest { good no syntax error } \n \n \nOr \n \n# cookstyle webtest \n \n# knife cookbook upload webtest { we can check this in web interface } \n \n \n# knife node show node-name \n{show what’s already attached to this node} \n# knife node show node1 \n \n# knife node run_list add <node-name> <cookbook-name> \n \n# knife node run_list add node1 webtest \n \nNow if i say show node then i should see two recipes \n \n# knife node show node1 \n \n \nLet’s go and confirm it on server side as well \n \n# go to Nodes section → Select Node → Click Settings (gear) → Edit run list \n \n \nWe can see two run_lists \n \nNow we have done the association, now our task is to execute the chef-client on the node, that will install httpd service \nand starts the service and it will set index page. \n \nLogin back to node machine \n \n# sudo chef-client \n \nNow i’m always running the chef-client manually, i want to do it automatically, for that to happen we need to setup a cron \njob. \n \nWhat is cron ?? \nCron is a linux scheduler with which you can schedule a task at some regular frequency or at regular interval of times. \n \n# crontab -e \n \n{ create crontab } \n* * * * sudo chef-client \n \n# crontab -l \n \nTemplates and Cross-platform \n========================= \n \nNow let’s see some dynamic things with our node, maybe we can use some of our node attributes to update the home page \non our web server.\nHow do i make this particular recipe platform, of course there is no apache2 package in centos or RHEL, it’s called httpd \nin centos/rhel. \n \nSo how do we model this, within the cookbook, let’s take a look: \n \nChef - Attributes \n=============== \n \nIt is a specific detail about a given node. \nI can define this attributes in chef cookbook, \n \nChef gets the attributes by ohai, \n \n# ohai | wc -l { almost gets 4000 parameters } \n \nHow to access these attributes inside the recipe ?? \n \nnode[‘platform’] \n \nnode[‘hostname’] \nLet’s write something that works across different platforms \n \n \nNow we know that in centos the package name, service name and document root is also different, from ubuntu. \n \nNow i’ll create some variables(attributes), that chef will be able to use, and i’ll call this variable's/attributes with \npackage_name, service_name \n \nCreate attributes directory under web-test where we put default.rb \n \n# mkdir cookbooks/webtest/attributes \n \n# vi cookbooks/webtest/attributes/default.rb \n# Creating attributes for multi os \n \ncase node[‘platform’] \n \nwhen “centos”,”rhel” \n \n \ndefault[“package_name”]=”httpd” \n \n \ndefault[“service_name”]=”httpd” \n \n \ndefault[“document_root”]=”/var/www/html” \nwhen “ubuntu”,”debain” \n \n \ndefault[“package_name”]=”apache2” \n \n \ndefault[“service_name”]=”apache2” \n \n \ndefault[“document_root”]=”/var/www” \nend \n \n \n \n \n \nNow change the default.rb in recipes to, \n \npackage node[“package_name”] do\naction :install \n \n \nend \n \nservice node[“service_name”] do \n \n \naction  :start \nend \nservice node[“service_name”] do \n \n \naction  :enable \nend \n \nfile ‘/var/www/html/index.html’ do \n \n \ncontent “<h1>Welcome to APACHE - By CHEF</h1>” \n \n \nowner ‘root’ \n \n \ngroup ‘root’ \n \n \nmode ‘644’ \n \nend \n \n \nWhat is template ?? \n \nIt is a script with which you will create static files. \n \nindex.html.erb → \nindex.html \nhttpd.conf.erb \n→ \nhttpd.conf \n \n# cd webtest \n# mkdir templates \n# vi webtest/templates/default/index.html.erb \n \n<html> \n \n<h1>Welcome to Chef <%= node[“nodename”] %></h1> \n \n<br> \n \n<h3>The host has total memory <%= node[“memory”][“total”] %></h3> \n \n</html> \n \n# knife cookbook test webtest \n# knife cookbook upload webtest \n \nSUPER MARKET \n=============== \nWith chef there is a beautiful feature, which is called SUPER MARKET. \n \nSuper market is place where we have huge no of readymade recipes, so just we can download them, install and \nstart working also its FREE. \n \n# knife supermarket \n \nWe can download recipe, install recipe, list etc\n# supermarket.chef.io \n \nSearch for java, \nLet’s see java example \n \n# login to node machine and do: \n \n# java  \n{ not installed } \n \n# javac { not installed } \n \n# knife supermarket download java_se \n \n# untar the java_se \n# cookstyle java_se \n# knife cookbook upload java_se \n# knife node show node1 \n# knife node run_list add node1 java_se \n \nRoles \nRoles help you configure multiple cookbooks to node at once. \nNormally we are going and running one cookbook at a time which would be cumbersome task to avoid this and \nadd multiple cookbooks at once we use roles. \n \nSo far we have been just adding recipes directly to a single node. \nIn practise, Roles make it easy to configure many nodes identically without repeating yourself each time. \n \nLet’s create our first role, \nThe node we are working on so far is a web server, so let’s go and create a web server role, login into the workstation  \nmachine \n# cd chef-repo/roles \n# vi webserver.rb \nname ‘web-server’ \n \n \n \n \n{ each role will have a name } \ndescription ‘Web Servers’ \n \n \n \n{ each role will have a description } \nrun_list ‘recipe[apachetest]’, ‘recipe[php-file]’ \n{ each role may have a run list } \n# Where apachetest is apache & new1 is php \n#your roles may have attributes as well, \n# vim php-file/recipes/default.rb \n                package 'php' do \n  notifies :restart, 'service[httpd]', :immediately \nend \n \ncookbook_file '/var/www/html/index.php' do \n \nsource 'index.php' \nend \n# knife role from file webserver.rb \n# knife role list \n# knife role show web-server \n# knife node run_list add NODE_NAME 'role[ROLE_NAME]'\nGo to chef server and check roles, now we got new role. \nAnd click on node, say edit run list, and you can see available roles. \n \nNow let’s do one thing, remove the apachetest from Current run list and save run list. \n \nNow goto the node1 and change the index.html file, and do # sudo chef-client, nothing is changed right, coz we have \nremoved the apachetest from the run list. \n \nLet’s apply the role now, Drag the webserver role from Available roles to Current Run List and save run list. \n \nEnvironments \n============= \nUsing environments combined with roles we can execute cookbook on multiple nodes. I can add multiple nodes to \nenvironment and can execute cookbooks on those multiple nodes. \n \nLet’s see what are environments and where they can be used. \nWill see the simple apache cookbook, \n # knife cookbook show apache \n# knife environment list \n \nThe _default environment is read-only and sets no policy at all, you can’t modify any configuration under this \nenvironment. \n \nSimilar to roles \n# vim environments/dev.rb \nname ‘dev’ \ndescription ‘My Dev Env’ \n \n# knife environment from file dev.rb \n# knife environment show dev \n# knife node show node1 \n \nBy default the nodes are part of _default environment, now will put our server in dev env \n# knife node environment_set node1 dev \n# knife node show node1 \n \nTo group all the nodes with one command: \n# knife exec -E 'nodes.find(\"chef_environment:dev\") {|n| puts n.run_list << \"role[web-server]\" unless \nn.run_list.include?(\"role[web-server]\"); n.save }' \n \n# sudo chef-client \n \nDatabags \n========= \nLet’ see a scenario where Data Bags can be used, let’s say we created a cookbook for creating users, \nSo what we do is we will create a new redhat user with some password. \n \n# useradd redhat\n# password: redhat \n \n# openssl passwd -1 -salt bacon lync123 \n \n# perl -e 'print crypt(\"password*\",\"\\$6\\$salt\\$\") . \"\\n\"' \n \n# perl -e 'print crypt(\"redhat\",\"\\$6\\$cx56hui\\$\") . \"\\n\"' \n \n# cd cookbooks \n# chef generate cookbook user-test \n# vim user-test/recipes/default.rb \ngroup 'redhat' do \n action :create \nend \n \nuser 'redhat' do \n uid '2000' \n password \n'$1$hPx7gY1X$pDm6ir7zJ2jirr4rriJTZ0$1$aISz6I6w$Q1X/dWJ7F2DBWzKZdABon.$1$oviO3aG4$DdUI5iFi6kSwX\nwWDo5qhc.' \n group 'redhat' \n shell ‘/bin/bash’ \n action :create \nend \n \n# knife cookbook test user-test \n# knife cookbook upload user-test \n# knife node show node1 \n# knife node run_list add node1 user-test \n# knife node show node1 \n \nGo back to node1 and check whether user redhat is created or not. \nIn node1 machine \n# id redhat \n# sudo chef-client \n# id redhat \n# sudo grep redhat /etc/shadow { same password which we have given in enc format } \n \nNow we have copied and pasted password in recipe, but never do it, coz recipes are easily accessible. \n \n \nThe otherway you have to assign the passwords is through Databags. \nWe store passwords in databags and databags are safely kept on chef server but not kept on this machine i.e workstation. \n \nLet’s see how to work with databags: \n \nDatabags are a global variables that are stored as JSON data format. \n \nCreating databag and upload it via file \n===============================\n# vi chef-repo/data_bags/redhat.json { any name } \n \n{ \n \n  “Id” : “redhat”, \n \n  “password” : \n“$1$hPx7gY1X$pDm6ir7zJ2jirr4rriJTZ0$1$aISz6I6w$Q1X/dWJ7F2DBWzKZdABon.$1$oviO3aG4$DdUI5iFi6kSwXwWDo5\nqhc.” \n \n} \n \n \nDatabags commands: \n# knife data bag list \n# knife data bag create redhat_password \n# knife data bag from file redhat_password redhat.json \n# knife data bag list \n# knife data bag show redhat_password \n# knife data bag show redhat_password redhat \n \n# vi cookbooks/user-test/recipes/default.rb \n \n# redhat_password = data_bag_item(‘dbname’,’key’) \n          redhat_password = data_bag_item(‘redhat_password’,’redhat’) \ngroup 'redhat' do \n action :create \nend \n \nuser 'redhat' do \n uid '2000' \n home '/home/redhat' \n password redhat_password[‘password’] \n group 'redhat' \n shell ‘/bin/bash’ \n action :create \nend \n \nIn node machine: \n# sudo cat /var/chef/cache/cookbooks/user-test/recipes/default.rb \nEarlier i was able to see the password.\nConfiguring JENKIN \nJenkins \n1) Download and install JDK. \n2) Define JAVA_HOME to <C:\\Program Files\\Java\\jdk1.8.0_**>. Version Used – 8u73 \n3) Define JRE_HOME to <C:\\Program Files\\Java\\jre1.8.0_**>. \n4) Point PATH to <C:\\Program Files\\Java\\jdk1.8.0_**>\\bin. \n5) Download and install Git from https://git-scm.com \nVersion Used– 2.8.3 \n6) Download and install Eclipse IDE for Java Developers from https://eclipse.org \n7) Download Jenkins **.*.zip (for Windows) from https://jenkins.io \nVersion Used – 2.7.1 \n8) Download ANT from http://ant.apache.org, define ANT_HOME as an environment variable (if \nyou would be using ANT). Version Used – 1.9.7 \n9) Download MAVEN from http://maven.apache.org. Install by entering in command prompt \n“mvn –v”. Define MAVEN_HOME as an environment variable. Version Used – 3.3.9 \n10) Install Jenkins in C:\\. Don’t install in C:\\Program Files (x86) as it may cause a permissions \nissue. \n11) Go to localhost:8080 (Default URL of Jenkins). Enter the secret key. \n12) Create a new user. This user will be the “Administrator” of the CI Server. \n13) After login using admin credentials, create a new user like this –\nConfiguring proxy in Jenkins \n1) Click “Manage Jenkins->Manage Plugins->Advanced”. \n2) Enter “proxy.wdf.sap.corp” in “Server”. \n3)  \n4) Enter “8080” in “Port Number”. \n5) Don’t enter your SAP ID in “Username”, just enter your SAP password in “Password”.\n6) Click “Submit” and then click “Check Now”. \n \nNote – This configuration is only needed if you are connected to SAP-Corporate. If you are \nconnected to SAP-Internet or any other network, clear these settings. Assuming you have done \neverything correctly in the project and you have properly defined your POM, run your project \nfirst, on SAP Internet, as SAP-Corporate blocks POM from downloading plugins, JARs and \ndependencies. Also, do not select the option “Delete workspace before build starts”, as it will \ndelete all the plugins, JARs and dependencies and download them again. When on SAP-Internet, \ndon’t use SonarQube as it will be configured for use on SAP-Corporate. \n \nPlugin to be installed \nClick “Manage Jenkins -> Manage Plugins”. Check which are installed by default and which \nneed to be installed. The required plugins are listed below –\nSheu Res Bur ; ize L nases\nus °\n| 0)\nina Sy F\n- . wy 5\n: 7 the Mago cC .\n° °\na =a\n; °\nthe QWWASP Java HTM Santizes e\n> =a *\nad Allows Hucson Jenin te integrate with Pastrce SCM repositories, ue =\nrs ts the BMD “\nat ips can b e\nJenkins Global Configuration \n1) Configure these only after all the tools are installed! \n2) Click “Manage Jenkins”. \n3) To configure JDK, Git, SonarQube Scanner, Ant, Maven & Node.js, click “Global Tool \nConfiguration”.\n4) Click “Global Settings” to manage other settings. Please note that you need to have “http://” \nbefore the IPAddress in both Jenkins URL and SonarQube Server URL because the job will fail \nsince Jenkins would not be able to call SonarQube Server.\n\n\nESLint with Jenkins \n1) Download and install Node.js from https://nodejs.org/en/download/ \nVersion Used – 4.4.5 \n2) Add <Install Directory of NodeJS> and <Install Directory of NodeJS>\\node_modules\\npm\\bin \nto PATH. C:\\User\\<SAP User ID>\\AppData\\Roaming\\npm should also be in PATH. \n \nInstall ESLint by using commanXd “npm install –g eslint” on command prompt (admin). \n3) Use the command “eslint –c eslintrc.js –f checkstyle **.js > eslint.xml” in the Build Step \n“Execute Windows Batch Command” where -> \nI) \n–c points to the file whose config you have to use. \nII) \n“eslintrc.js” is the file which you get after running “eslint --init” at the location \n“C:\\User\\<SAP User ID>\\AppData\\Roaming\\npm\\node_modules\\eslint” (though the \nname of the file generated is “.eslintrc.js”, remove the leading “.”). This file decides the \nstyling format of the js file. \nIII) \n-f points to the ruleset which you are supposed to use. \nIV) \nCheckstyle is the ruleset which I am using. \nV) \n**.js is the name of the file(s) which you want to be analyzed. \nVI) \n“>” is used for pipelining the results to a file, here I have used “eslint.xml”. \n4) Now, you have to publish the contents of the “eslint.xml” file, so I used “Publish Checkstyle \nResults” in the Post-Build step and mentioned file name as “eslint.xml”. \n5) Now, when you make a build, you will see that execution of build fails, here is the solution-> \nI) \nGo to the “Services” app. \nII) \nFind the “Jenkins” service and right-click on it. \nIII) \nSelect “Properties” and go-to the “Log On” tab. \nIV) \nSelect “This Account” and enter your Account ID like this -> “GLOBAL\\<Your SAP \nID>”. \nV) \nEnter and reconfirm your SAP password. \nVI) \nClick “Apply” and “OK”. \nVII) \nStop the Jenkins service and then start the service. \n6) Now, click “Build Now” and you will see that “Batch command” executes successfully. \n7) In case, the above command doesn’t run, run this command “eslint –f checkstyle > eslint.xml”. \n \nCheckstyle, PMD, Cobertura \n1) Download and install Checkstyle, PMD and Code Coverage (Cobertura) plugins. \n2) Now in your project’s pom.xml, include plugins and dependencies required according to the \nformat mentioned on Maven Central repository http://search.maven.org \nor just search for it on Stack OverFlow or just Google it. You may find it on GitHub. \n3) Now, add Build Step “Invoke Top-Level Maven Targets” in your Freestyle Project and add the \nGoal for (separate, individual goals)-> \nI) \nCheckstyle -> “checkstyle:checkstyle –e” \nII) \nPMD -> “pmd:pmd –e” \nIII) \nCPD -> “pmd:cpd –e” \nIV) \nCobertura -> “cobertura:cobertura -Dcobertura.report.format=xml -e” \nHere I am using “–e“ flag to print a complete stack trace in-case any error occurs and its \nhighly recommended. Also, in advanced section of each, mention “pom.xml”. \n4) Publish Checkstyle, PMD, Cobertura reports in their respective Post-Build Step. \n5) Click “Build Now” and see the results.\n6) Below is the pom.xml needed for the above plugins and also include JUnit. \n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" \nxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \n \nxsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-\nv4_0_0.xsd\"> \n \n<modelVersion>4.0.0</modelVersion> \n \n<groupId>com.mycompany.app</groupId> \n \n<artifactId>junitmavenexample</artifactId> \n \n<packaging>jar</packaging> \n \n<version>1.0-SNAPSHOT</version> \n \n<name>junitmavenexample</name> \n \n<url>http://maven.apache.org</url> \n \n \n \n<build> \n \n \n<plugins> \n \n \n \n<plugin> \n \n \n \n \n<groupId>org.apache.maven.plugins</groupId> \n \n \n \n \n<artifactId>maven-site-plugin</artifactId> \n \n \n \n \n<version>3.5.1</version> \n \n \n \n \n<configuration> \n \n \n \n \n \n<reportPlugins> \n \n \n \n \n \n \n<plugin> \n \n \n \n \n \n \n \n<groupId>org.apache.maven.plugins</groupId> \n \n \n \n \n \n \n \n<artifactId>maven-checkstyle-plugin</artifactId> \n \n \n \n \n \n \n \n<version>2.17</version> \n \n \n \n \n \n \n</plugin> \n \n \n \n \n \n</reportPlugins> \n \n \n \n \n</configuration> \n \n \n \n</plugin> \n \n \n \n<plugin> \n \n \n \n \n<groupId>org.apache.maven.plugins</groupId> \n \n \n \n \n<artifactId>maven-site-plugin</artifactId> \n \n \n \n \n<version>3.5.1</version> \n \n \n \n \n<configuration> \n \n \n \n \n \n<reportPlugins> \n \n \n \n \n \n \n<plugin> \n \n \n \n \n \n \n \n<groupId>org.apache.maven.plugins</groupId> \n \n \n \n \n \n \n \n<artifactId>maven-pmd-plugin</artifactId> \n \n \n \n \n \n \n \n<version>3.6</version> \n \n \n \n \n \n \n \n<configuration> \n \n \n \n \n \n \n \n \n<!-- PMD options --> \n \n \n \n \n \n \n \n \n<targetJdk>1.8</targetJdk> \n \n \n \n \n \n \n \n \n<aggregate>true</aggregate> \n \n \n \n \n \n \n \n \n<format>xml</format> \n \n \n \n \n \n \n \n \n<rulesets> \n \n \n \n \n \n \n \n \n \n<ruleset>/pmd-rules.xml</ruleset> \n \n \n \n \n \n \n \n \n</rulesets>\n<!-- CPD options --> \n \n \n \n \n \n \n \n \n<minimumTokens>50</minimumTokens> \n \n \n \n \n \n \n \n \n<ignoreIdentifiers>true</ignoreIdentifiers> \n \n \n \n \n \n \n \n</configuration> \n \n \n \n \n \n \n</plugin> \n \n \n \n \n \n</reportPlugins> \n  \n \n \n \n</configuration> \n \n \n \n</plugin> \n \n \n \n<plugin> \n \n \n \n \n<groupId>org.codehaus.mojo</groupId> \n \n \n \n \n<artifactId>cobertura-maven-plugin</artifactId> \n \n \n \n \n<version>2.4</version> \n \n \n \n \n<configuration> \n \n \n \n \n \n<instrumentation> \n \n \n \n \n \n \n<includes> \n \n \n \n \n \n \n \n<include>**/*.class</include> \n \n \n \n \n \n \n</includes> \n \n \n \n \n \n</instrumentation> \n \n \n \n \n</configuration> \n \n \n \n \n<executions> \n \n \n \n \n \n<execution> \n \n \n \n \n \n<id>clean</id> \n \n \n \n \n \n<phase>pre-site</phase> \n \n \n \n \n \n<goals> \n \n \n \n \n \n \n<goal>clean</goal> \n \n \n \n \n \n</goals> \n \n \n \n \n \n</execution> \n \n \n \n \n \n<execution> \n \n \n \n \n \n \n<id>instrument</id> \n \n \n \n \n \n \n<phase>site</phase> \n \n \n \n \n \n \n<goals> \n \n \n \n \n \n \n \n<goal>instrument</goal> \n \n \n \n \n \n \n \n<goal>cobertura</goal> \n \n \n \n \n \n \n</goals> \n \n \n \n \n \n</execution> \n \n \n \n \n</executions> \n \n \n \n</plugin> \n \n \n</plugins> \n \n</build> \n \n \n<reporting> \n \n \n<plugins> \n \n \n \n<plugin> \n \n \n \n<!-- use mvn cobertura:cobertura to generate cobertura reports --> \n \n \n \n \n<groupId>org.codehaus.mojo</groupId> \n \n \n \n \n<artifactId>cobertura-maven-plugin</artifactId> \n \n \n \n \n<version>2.4</version>\n<configuration> \n \n \n \n \n \n<formats> \n \n \n \n \n \n \n<format>html</format> \n \n \n \n \n \n \n<format>xml</format> \n \n \n \n \n \n</formats> \n \n \n \n \n</configuration> \n \n \n \n</plugin> \n \n \n</plugins> \n \n</reporting> \n \n \n<dependencies> \n \n \n<dependency> \n \n \n \n<groupId>junit</groupId> \n \n \n \n<artifactId>junit</artifactId> \n \n \n \n<version>4.11</version> \n \n \n \n<scope>test</scope> \n \n \n</dependency> \n \n</dependencies> \n</project>\npom.xml\n \n \n \nSonarQube with Jenkins \n1) Download SonarQube Server from www.sonarqube.org/downloads and unzip. Version 5.6 \n2) Download SonarQube Scanner from www.sonarqube.org/downloads and unzip. Version – \n2.6.1 \n3) Download and install SonarQube plugin. \n4) Create environment variable SCANNER_RUNNER_HOME which points to <Directory of \nScanner>. \n5) In PATH environment variable, make an entry which will point to <Directory of Scanner>\\bin. \n6) Download MySQL Community from https://dev.mysql.com/downloads/ \nVersion 5.7.13 \n7) Install MySQL as Server Machine. \n8) Select Custom Installation. \n9) Install Server and Workbench only. \n10) Also select “Show Advanced Options”, which will give logging options. Select all logging \noptions. \n11) Also install X Protocol/MySQL as a Document Store (NoSQL). \n12) Add <Install Directory of MySQL>\\MySQL Server 5.x\\bin to PATH for easy access. \n13) Login to the Local Instance with the password provided during installation. \n14) Create an empty DB schema named “sonar”.\nFie eit View Gary Crane Sener Teds Soy Hp\n886 O88a ae ®\nMANAGEMEN * GBF FA B| into icon + |%/G Qf ih hy | sor :\n© see sats i\nA en antes\nlS Sen as\n& oastot\n& datainpoainetre\nasta 9\n0 sti htoun\nA seve\nA ontenste\nsR\n@ outoows\nPerformance Reports !\nA eta Sema Sg\ncas e\nMANAGEMENT : hone: ET\n© server status Se\nRename References\nB cent connections\n© Users ana Prvteges cane GSE =\nstatus ana system Variables\n& data Export\n& ate inportRestore\nINSTANCE\nB startup /Snutsown\nA servertogt\nF options Fie\n@ oamndoars\nHF Pestormance Reports\nGS Pertormance Schema Setup\n>\nrows\nCc >| hoot Revert\nApply SQL Script to Database\nee ee Review the SQL Script to be Applied on the Database\n‘Online DOL\n1 CREATE SCHEMA ‘sonarl” ;\n2\n< >\naa oral\n\"Apply SOL Script to Database\na Applying SQL script to the database\nApply SQL Script\n‘The folowing tasks will now be executed. Please monitor the execution.\nPress Show Logs to see the execution logs.\n@& Execute SQ Statements\n‘SQL script was successfully applied to the database.\n| Shon tons | tek | [prc | [Gane\n15) Create a new new user with username – root and password – **** from Users and Privileges. \nChange “localhost” to “IPAddress”. \n \n \n \n16) Now, set Administrative privileges of User “root” to DBA. \n \n \n \n17) Now, set Schema privileges of User “sonar” to access only the DB “sonar” and select all \nprivileges, except “GRANT OPTION”.\n18) Now create a new server instance. Give it a name and IP Address. Clear the password and enter \nthe password you entered before. Select the default schema later.\n19) Go to <Directory of SonarQube>\\conf – \n \nI) \nUncomment the following lines in “sonar-properties.xml”and set them – \nsonar.jdbc.url (MySQL) (Server Instance) – Change localhost to <IPAddress>  \nsonar.jdbc.username=root (Server Instance) \nsonar.jdbc.password=_____ (Server Instance) \n \n \n \nII) \nAdd the following line to “wrapper.conf” – \nset.TMPDIR=<Path of SonarQube>\\temp \n \n \n \n20) Install the SonarQube Server (Run as Admin) at <Directory of SonarQube>\\bin\\Windows-x86-\n64\\InstallNTService. \n21) Check if the service is running from the task manager. If not, start it. \n22) Run the SonarQube Server service (Run as Admin) at<Directory of SonarQube>\\bin\\Windows-\nx86-64\\StartNTService. \n23) Use mysql –u root –p –h <IPAddress> to check DB from cmd. Use “SHOW DATABASES;” to \ncheck the list of DBs, use “USE DBNAME;” to select the DB and “SHOW TABLES;” to see its \ntables.\n24) Go to Jenkins and install SonarQube plugin and set build step of SonarQube in project. \n25) Make sonar-project.properties according to language in root directory of project. \n26) Start the Build and get the results. Location of SonarQube is <IPAddress>:9000.\nSelenium Webdriver \n1) Download and install Selenium plugin. \n2) Selenium Hub has been setup at localhost:4444. \n3) Also download and unzip selenium-server-standalone-*.**.*.jar from \nhttp://docs.seleniumhq.org/download/ \n4) Download ChromeDriver from here - http://chromedriver.storage.googleapis.com/index.html \n5) Now, create a simple Java project which invokes the webdriver and the browser (in this case, \nChrome). \n6) Add the location of Chrome WebDriver to PATH. \n7) Install ChromeDriver plugin. \n8) There is a line in LaunchBrowser code which may be like this “http:localhost:4444/wd/hub”. If \nso, provide your laptop’s “IPaddress” there in place of “localhost. \n9) Start the node using this command “java –jar selenium-server-standalone-*.**.*.jar  \n-role node –hub <nodeIPaddress>:4444/grid/register -port <port no>” in the location where \nthis JAR is located. By default, the node runs on the port number 5555, so you don’t really need \nthe part “-port <port no>”. Don’t close the command prompt window. \n10) Check Selenium Grid at “localhost:4444/grid/console” to see if a node has been created. \n11) Invoke “Top-Level Maven Targets” in the Build Step in your Freestyle Project and enter Goal as \n“test –e”. \n12) Click “Build Now” and see that the browser page you wished to see in the browser window \nstart and close automatically. \n \nPostman \n1) Install “postman” by using command “npm install –g newman” on command prompt (admin). \n2) Get a public postman collections JSON file. \n3) Use the command “newman -c *.json -H *.html” in the Build Step “Execute Windows Batch \nCommand” where –H points to the HTML file where test results should be written or else, use \n“-o” instead of “-H” to use other file format. \n4) Click “Build Now” and see the report published in the file. \n \nQTP/UFT \n1) Ensure that your laptop is connected to SAP-Corporate. \n2) Download and install QTP/UFT. \n3) Create a QTP script and save it. \n4) Launch QTP. Select Tools-> Options->Run Sessions-> Configure the results to be saved as .html. \n5) Create a simple VB script which launches QTP, run the test and save the results in a specified \nlocation. Ensure that it works while running the script on command prompt. \n6) Create a new job in Jenkins. \n7) Use the command “CScript “<Path of VBScript>.vbs” “, which will launch QTP, run the test and \nclose it after execution. \n8) If you are making more than 1 build, use “Execute Windows Batch Command” to move the \nexisting .html files to another folder and save the latest report to original folder so that while \npublishing the HTML Report, there are no issues. The command is “move “<Location of .html \nfiles>\" \"<New Location>\" “. Now the “CScript” command should follow after the “move” \ncommand.\nCreating .zip using Hudson Post-Build Task Plugin \n1) Install Post-Build task “Add Hudson Post-Build Task” plugin. \n2) Select Logical Operation “OR”. \n3) In the Script box, enter the command “jar –cMf *.zip .” \nHere, * is the name of the zip we want to create. “.” Is used to address the parent of the \nfolder/location where the job is. So, whenever this command is run, it will create a ZIP of the \nworkspace. “jar” is used because we are using Java to run this command. Use “jar –cMf” on \ncommand prompt first for information on the flags.  \n \n \n \nNagios: \nNagios is an open source software that can be used for network and \ninfrastructure monitoring. Nagios will monitor servers, switches, applications \nand services. It alerts the System Administrator when something went wrong and \nalso alerts back when the issues has been rectified. \nNagios is useful for keeping an inventory of your servers, and making sure your \ncritical services are up and running. Using a monitoring system, like Nagios, is \nan essential tool for any production server environment. \n \nWith Nagios you can: \n– Monitor your entire IT infrastructure. \n– Identify problems before they occur. \n– Know immediately when problems arise. \n– Share availability data with stakeholders \n– Detect security breaches. \n– Plan and budget for IT upgrades. \n– Reduce downtime and business losses.",
      "page_count": 127,
      "pages": [
        {
          "page": 1,
          "text": "DevOps Complete Package \nLAMP \n===== \nSo when it comes to LAMP we are talking about { Linux, Apache, MySql & PHP } \n \nLAMP is an open source Web development platform that uses Linux as the operating system, Apache as the Web \nserver, MySQL as the relational database management system and PHP as the object-oriented scripting language. \n \nBecause the platform has four layers, LAMP is sometimes referred to as a LAMP stack. Stacks can be built on different \noperating systems. \n \nThe same setup in windows is called WAMP \nThe same setup in mac is called MAMP \n \nAdvantages Of LAMP \n================== \n \n \nOpen Source \n \nEasy to code with PHP \n \nEasy to deploy an application \n \nDevelop locally \n \nCheap Hosting \n \nEasy to build CMS application \no \nWordpress, Drupal, Joomla, Moodle etc \n \nWeb Server \nA web server is a program which serves web pages to users in response to their requests, which are forwarded by their \ncomputers' HTTP clients(Browsers). \n \n \n \n \n \nAll computers that host websites must have web server program. \n \n \n \nPurpose of Web server \n=================== \nA web server’s main purpose is to store web site files and broadcast them over the internet for you site visitors to see. In \nessence, a web server is simply a powerful computer that stores and transmits data via the internet. \n \nWeb servers are the gateway between the average individual and the world wide web.",
          "char_count": 1407,
          "ocr_used": false
        },
        {
          "page": 2,
          "text": "LAMP Architecture \n \n \n \nLinux :: We already talked about linux \n \nApache \n======= \nAn open source web server used mostly for Unix and Linux platforms. \nIt is fast, secure and reliable. \n \nSince 1996 Apache has been the most popular web server, presently apache holds 49.5% of market share i.e, 49.5% of all \nthe websites followed by Nginx 34% and Microsoft IIS 11%. \n \nParameters for Apache (httpd) \nPackages \n \n \n \n- \n httpd  &  \nmod_ssl \nPort \n \n \n \n \n- \n 80  \n \n  \n 443 \nProtocol  \n \n \n \n-  \n http \n \n \n https \nServer Root \n \n \n \n-     \n /etc/httpd \nMain config file  \n \n- \n/etc/httpd/conf/httpd.conf  \nDocument root  \n \n-  \n/var/www/html    {all our web pages } \nLogs \n \n \n \n \n-  \n/var/log/httpd/error_log \n \n \n \n \n \n \n/var/log/httpd/access_log \n \n \nWhat version of OS you have \n======================== \n \n# cat /etc/redhat-release \n# cat /etc/os-release \n \nChecking httpd service is running or not \n================================= \n \n6.x - Whenever you have problem with httpd {troubleshooting httpd} \n \n# service httpd status \n \n# netstat -ntpl | grep 80 \n \n# ps -ef | grep httpd \n \n7.x - Whenever you have problem with httpd {troubleshooting httpd}",
          "char_count": 1167,
          "ocr_used": false
        },
        {
          "page": 3,
          "text": "# systemctl status httpd \n \n# netstat -ntpl | grep 80 \n \n# ps -ef | grep httpd \n \nNow when you are doing server configuration, we need to be very careful while doing server configuration, coz sometimes \nyou may do some typos and you are unaware why the server is not starting, so to confirm your config is correct use \nfollowing command: \n \nCheck configuration file (httpd.conf) is correct or not \n=========================================== \n \n \n# httpd -t \n \n{ 6.x and 7.x } \n \nInstalling httpd service \n=================== \n \n# rpm -qa | grep httpd \n# sudo yum -y install httpd \n \nLet’s check status of the web server: \n \n# systemctl status httpd \n \n# netstat -ntpl | grep 80 \n \n# ps -ef | grep httpd \n \nLet’s start the web server: \n# systemctl start httpd \n# systemctl status httpd \n \n# netstat -ntpl | grep 80 \n# ps -ef | grep httpd \n \n \nNow we have started the service \nIf you see # ls -l /var/www/html, you have no files in there, let’s create a index.html \n \n# cd /var/www/html \n \n# vi index.html {put some content} \n \n# vi /var/www/html/sample.php \n \n \n<?php \n \necho “Today is ” . date(“Y/m/d”) . <br>”; \necho \"Today is \" . date(\"l\"); \n \n?> \n \nhttp://ip-address/sample.php \n{ doesn’t show php rendering coz php engine was  not there } \n \n# sudo yum -y install php \n \nhttp://ip-address/sample.php \n{ php will be rendered } \n \nApache Virtual Hosting \n=================== \nAt the end of file add these lines \n \n<VirtualHost *:81> \nDocumentRoot /var/www/html/website2 \n</VirtualHost> \n \n<VirtualHost *:82> \nDocumentRoot /var/www/html/website3 \n</VirtualHost> \n \n \nNginx Server \n============",
          "char_count": 1605,
          "ocr_used": false
        },
        {
          "page": 4,
          "text": "Nginx is a http server, which is used in many high traffic websites like GitHub, heroku etc. \n \nThe 3 imp features that nginx gives is: \n   Load balancing, Caching and Reverse proxying. \n    \nIn load balancing, say u have a high traffic website which  gets 1000 requests per second may be more then that, if u have \nonly one server all the 1000 requests will be severe by the single server where the response time will be decreased. \n \nSo we put more servers and distribute the load to all the servers equally. \nSo the 1000 requests will be distributed to 3 servers like 300 requests for each server assuming we got 3. \n \nReverse proxy, we are having multiple applications on the server, and u can only run one application on one port, say u \nrunning one app on port 80 now u can’t run another application on port 80 we got to use another port. { app1.com ==> \napp1.com:80 } \n \nNow we are running multiple applications we need give diff port right something like { app2.com:81 } \n \nNow I don’t want to specify port 81 in url, I want app2.com, to deal with this problem we can use nginx. \n \nNow nginx intercepts the requests and it will see that the following request is for app2.com now it routes this request to \nthat application which is running on port 81. This is what reverse proxy is. \n \nForward Proxy & Reverse Proxy \n========================== \n \nThe word \"proxy\" describes someone or something acting on behalf of someone else. \nIn the computer realm, we are talking about one server acting on the behalf of another computer. \n \nForward Proxy \n=============== \nForward Proxy: Acting on behalf of a requestor (or service consumer) \n \n\"forward proxy\" retrieves data from another web site on behalf of the original requestee. \n \nIn forward proxy we get to see 3 computers: \n \nX = your computer, or \"client\" computer on the internet \nY = the proxy web site, proxy.example.org \nZ = the web site you want to visit, www.example.net \nNormally, one would connect directly from     X    —>    \n Z. \nHowever, in some scenarios, it is better for Y —>  Z, on behalf of X, which chains as follows: X    —> Y—> Z \n \nReasons why X would want to use a forward proxy server \n==================================================== \nX is unable to access Z directly because \n \na) Someone with administration authority over X's internet connection has decided to block all access to site Z. \n \nEmployees at a large company have been wasting too much time on facebook.com, so management wants access blocked \nduring business hours. \n \nTorrent downloads can be blocked by our ISP’s so to view the torrent downloads we can use forward proxy. \n \nb) The administrator of Z has blocked X. \n \nExamples: \nThe administrator of Z has noticed hacking attempts coming from X, so the administrator has decided to block X's ip \naddress. \nZ is a forum web site.  X is spamming the forum. Z blocks X. \n \nReverse Proxy \n===============",
          "char_count": 2914,
          "ocr_used": false
        },
        {
          "page": 5,
          "text": "Reverse Proxy: Acting on behalf of service/content producer \n \nIn reverse proxy we get to see 3 computers: \n \nX = your computer, or \"client\" computer on the internet \nY = the reverse proxy web site, proxy.example.com \nZ = the web site you want to visit, www.example.net \n \nNormally, one would connect directly from X --> Z \n \nHowever, in some scenarios, it is better for the administrator of Z to restrict or disallow direct access, and force visitors to \ngo through Y first. So, as before, we have data being retrieved by Y --> Z on behalf of X, which chains as follows: X --> Y --> \nZ. \n \nReasons why Z would want to setup a reverse proxy server \n============================================== \n1) Z wants to force all traffic to its web site to pass through Y first. \n \na) Z has a large web site that millions of people want to see, but a single web server cannot handle all the traffic. So Z sets \nup many servers, and puts a reverse proxy on the internet that will send users to the server closest to them when they try \nto visit Z. This is part of how the Content Distribution Network (CDN) concept works. \nExamples: \nApple Trailers uses Akamai \nJquery.com hosts its javascript files using CloudFront CDN (sample). \n \n2) The administrator of Z is worried about retaliation for content hosted on the server and does not want to expose the \nmain server directly to the public. \n \nLoad Balancing and Reverse Proxying with Nginx \n======================================== \n \nMake sure you have 3 machines setup with you: \n \nM1 IP-ADD: 104.154.22.74 \nM2 IP-ADD: 104.198.254.34 \nM3 IP-ADD: 104.198.70.78 \n \nMachine1 \n======== \n \n# yum -y install httpd \n# systemctl enable httpd \n# systemctl start httpd \n# allow port 80 in firewall \n# echo “MACHINE01” > /var/www/html/index.html \n# vi /var/www/html/index.html    { MACHINE —> 1 } \n \n \nMachine2 \n======== \n \n# yum -y install httpd \n# systemctl enable httpd \n# systemctl start httpd \n# allow port 80 in firewall \n# echo “MACHINE02” > /var/www/html/index.html \n# vi /var/www/html/index.html    { MACHINE —> 2 } \n \n \nMachine3 \n========",
          "char_count": 2088,
          "ocr_used": false
        },
        {
          "page": 6,
          "text": "# yum -y install nginx \n# systemctl enable nginx \n# systemctl start nginx \n# allow port 80 in firewall \n# vi /etc/nginx/nginx.conf    \n { delete everything under http section } \n \nI am going to define a group of web servers with directive upstream and going to give this group a name as lbmysite. \n \nUpstream defines a cluster that you can proxy requests to. It's commonly used for defining either a web server cluster for \nload balancing, or or an app server cluster for routing / load balancing. \n \nnginx.conf \n========= \nuser nginx; \nworker_processes auto; \nerror_log /var/log/nginx/error.log; \npid /run/nginx.pid; \n \n# Load dynamic modules. See /usr/share/nginx/README.dynamic. \ninclude /usr/share/nginx/modules/*.conf; \n \nevents { \n \nworker_connections 1024; \n} \n \n## remove and replace from http && remove old server block { } \n \nhttp { \n \n \nlog_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" ' \n                  \n'$status $body_bytes_sent \"$http_referer\" ' \n                  \n'\"$http_user_agent\" \"$http_x_forwarded_for\"'; \n \n \naccess_log  /var/log/nginx/access.log  main; \n \n \nsendfile         \non; \n \ntcp_nopush       on; \n \ntcp_nodelay      on; \n \nkeepalive_timeout   65; \n \ntypes_hash_max_size 2048; \n \n \ninclude          \n/etc/nginx/mime.types; \n \ndefault_type     \napplication/octet-stream; \n \n \ninclude /etc/nginx/conf.d/*.conf; \n \nupstream lbmysite { \n \nserver 104.197.127.92:90; \n \nserver 35.194.22.247; \n \n} \n \nserver { \n    \nlisten    80 default_server; \n    \nlisten    [::]:80 default_server; \n \nlocation / { \n    \nproxy_pass http://lbmysite; \n \n} \n} \n \n} \n \nDatabase \n========",
          "char_count": 1626,
          "ocr_used": false
        },
        {
          "page": 7,
          "text": "Installation Version 5.6 \n================== \n# wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm \n \n# sudo rpm -ivh mysql-community-release-el7-5.noarch.rpm \n \n# sudo yum -y install mysql-server \n \nStart the mariadb server # sudo systemctl start mysqld \n \nSetting password # sudo mysql_secure_installation \n \nTo login to database # sudo mysql -u root -p \n \n# ps -ef | grep mysql \n# netstat -ntpl | grep 3306 \n \nHow to connect to Database? \n \n# mysql -u root -p \n \n \n \n{if db is same host} \n \n# mysql -u root -p -h <server_ip> \n \n{if db is on diff host \n \n \n \nCMS Application \n============= \n \nDownload wordpress # wget https://wordpress.org/latest.tar.gz \n \n# sudo tar xvf wordpress.tar -C /var/www/html \n \n# sudo yum -y install php php-mysql \n \nLogin to phpmyadmin, and create user a database and user called wordpress something like that. \n \nGive anything you want as username and password \nCopy and paste the code given by wordpress into wp-config.php \nChange the ownership of wordpress directory to apache:apache \n \n \n \n \n \n \n \n \n \n \nGIT",
          "char_count": 1070,
          "ocr_used": false
        },
        {
          "page": 8,
          "text": "Why Version Control ?? \n================= \nHave you ever:  \n● Made a change to code, realised it was a mistake and wanted to revert back? \n● Lost code and didn’t have a backup of that code ? \n● Had to maintain multiple versions of a product ? \n● Wanted to see the difference between two (or more) versions of your code ? \n● Wanted to prove that a particular change in code broke application or fixed a application ? \n● Wanted to review the history of some code ? \n● Wanted to submit a change to someone else's code ? \n● Wanted to share your code, or let other people work on your code ? \n● Wanted to see how much work is being done, and where, when and by whom ? \n● Wanted to experiment with a new feature without interfering with working code ? \n  \nIn these cases, and no doubt others, a version control system should make your life easier. \n Key Points \n● Backup \n● Collaboration \n● Storing Versions \n● Restoring Previous Versions \n● Understanding What Happened \n \nVersion Control / Revision control / Source Control is is a software that helps software developers to work together \nand maintain a complete history of their work. \n  \nYou can think of a version control system (\"VCS\") as a kind of \"database\". \nIt lets you save a snapshot of your complete project at any time you want.  When you later take a look at an older snapshot \n(\"version\"), your VCS shows you exactly how it differed from the previous one. \nA version control system records the changes you make to your project’s files.  \nThis is what version control is about. It's really as simple as it sounds.",
          "char_count": 1587,
          "ocr_used": false
        },
        {
          "page": 9,
          "text": "Popular VCS \n      \n    \n           \n \n   \n \nTypes of VCS \n○ \nCentralized version control system (CVCS) \n○ \nEx: CVS, SVN \n○ \nDistributed version control system (DVCS) \n○ \nEx: Git, Mercurial",
          "char_count": 205,
          "ocr_used": false
        },
        {
          "page": 10,
          "text": "Centralized Version Control System (CVCS) \nUses a central server to store all files and enables team collaboration. \nBut the major drawback of CVCS is its single point of failure, i.e., failure of the central server. Unfortunately, if the central \nserver goes down for an hour, then during that hour, no one can collaborate at all. \n  \n \nDistributed Version Control System (DVCS) \nDVCS does not rely on the central server and that is why you can perform many operations when you are offline. You can \ncommit changes, create branches, view logs, and perform other operations when you are offline. You require network \nconnection only to publish your changes and take the latest changes.",
          "char_count": 696,
          "ocr_used": false
        },
        {
          "page": 11,
          "text": "Distributed Version Control\nMain Server Repository\nCollaborator #1 Collaborator #3\nLocal Repository ES eS\npull tt push\nCollaborator #2\nLocal Repository\n_ _\n‘CENTRAL REPOSITORY REMOTE REPOSITORY\nCOMMIT UPDATE ortiona. GED\nand automatically uploads data\n|\n| | S\nWORKING WORKING LOCAL\ncory copy REPOSITORY\nREVERT )( LOG )( STATUS\nSTATUS BRANCH )( MERGE )( BLAME\nEE ed",
          "char_count": 365,
          "ocr_used": true,
          "original_char_count": 16
        },
        {
          "page": 12,
          "text": "How the Typical VCS works \nA typical VCS uses something called Two tree architecture, this is what a lot of other VCS use apart from git. \nUsually, a VCS works by having two places to store things: \n1. Working Copy \n2. Repository \nThese are our two trees, we call them trees because they represent a file structure. \nWorking copy [CLIENT] is the place where you make your changes. \nWhenever you edit something, it is saved in working copy and it is a physically stored in a disk. \nRepository [SERVER] is the place where all the version of the files or  commits,  logs etc is stored. It is also saved in a disk \nand has its own set of files. \nYou cannot however change or get the files  in a repository directly, in able to retrieve a specific file from there, you have \nto checkout   \nChecking-out is the process of getting files from repository to your working copy.  This is because you can only edit files \nwhen it is on your working copy. When you are done editing the file, you will save it back to the repository by committing \nit, so that it can be used by other developers. \n  \nCommitting is the process of putting back the files from working copy to repository.",
          "char_count": 1185,
          "ocr_used": false
        },
        {
          "page": 13,
          "text": "Hence, this architecture is called 2 Tree Architecture. \nBecause you have two tree in there Working Copy and Repository. \nThe famous VCS with this kind of architecture is Subversion or SVN. \n  \nHow the Distributed DVCS works \n=========================== \nUnusually, a DVCS works by having three places to store things: \n  \n1. Working Copy \n2. Staging \n3. Repository \nAs Git uses Distributed version control system, So let’s talk about Git which will give you an understanding of DVCS. \nGit was initially designed and developed by Linus Torvalds in 2005 for Linux kernel development. Git is an Open Source \ntool. \n \n \nHistory \nFor developing and maintaining Linux Kernel, Linus Torvalds used BitKeeper which is also one of the VCS, and is open \nsource till 2004. \nSo instead of depending on other tools, they developed their own VCS. \nJust see the wiki of Git.",
          "char_count": 877,
          "ocr_used": false
        },
        {
          "page": 14,
          "text": "Git Architecture \nGit uses three tree architecture.  \nWell interestingly Git has the Working Copy and Repository as well but it has added an extra tree Staging in between: \n  \n \n  \nAs you can see above, there is a new tree called Staging.  \nWhat this is for ?   \nThis is one of the fundamental difference of Git that sets it apart from other VCS, this Staging tree (usually termed as \nStaging area) is a place where you prepare all the things that you are going to commit. \nIn Git, you don't move things directly from your working copy to the repository, you have to stage them first, one of the \nmain benefits of this is, to break up your working changes into smaller, self-contained pieces. \nTo stage a file is to prepare it for a commit. \nStaging allows you finer control over exactly how you want to approach version control. \n \n \nAdvantages Of Git \n===================",
          "char_count": 883,
          "ocr_used": false
        },
        {
          "page": 15,
          "text": "Git works on most of OS: Linux, Windows, Solaris and MAC. \nInstalling Git \n● Download Git {git website} \nTo check if git is available or not use: \n# rpm -qa | grep git \n# sudo yum install git \n# git --version \n  \nSetting the Configuration \n======================= \n# git config --global user.name “Ravi Krishna” \n# git config --global user.email “info@gmail.com” \n# git config --list \n  \nNOTE :: The above info is not the authentication information. \n \nWhat is the need of git config \n===========================   \nWhen we setup git and before adding bunch of files, We need to fill up username & email and it’s basically git way of \ncreating an account. \n \nWorking with Git \n================= \nGetting a Git Repository \n# mkdir website  \n \nInitialising a repository into directory, to initialize git we use: \n# git init \n \nThe purpose of Git is to manage a project, or a set of files, as they change over time. Git stores this information in a data \nstructure called a repository. \n  \n# git init is only for when you create your own new repository from scratch. \nIt turns a directory into an empty git repository.",
          "char_count": 1125,
          "ocr_used": false
        },
        {
          "page": 16,
          "text": "Let’s have some configuration set: \n# git config --global user.name “Ravi” \n# git config --global user.email “ravi@digital-lync.com” \n \nTaking e-commerce sites as example. \n \nBasic Git Workflow \n============= \n1. You modify files in working directory \n2. You stage files, adding snapshots of them to your staging area. \n3. You do a commit, which takes the files as they are in the staging area and stores that snapshot to your git \nrepository. \n \n \n \n# mkdir website \n# git init \n# git status                    \n \n {Branches will talk later} \n# vi index.html    \n{put some tags <html><title><h1><body> just structure}                \n# git status \n# git add index.html             \n {staged the changes} \n# git commit -m “Message”    {moves file from staging area to local repo} \n# git status \nYou can skip the staging area by # git commit -a -m “New Changes”",
          "char_count": 870,
          "ocr_used": false
        },
        {
          "page": 17,
          "text": "Commit History - How many Commits have happened ?? \nTo see what commits have been done so far we use a command: \n# git log \nIt gives commit history basically commit number, author info, date and commit message. \nWant to see what happend at this commit, zoom in info we use: \n# git show <commit number>  \nLet’s understand this commit number \nThis is sha1 value randomly generated number which is 40 character hexadecimal number which will be unique. \nLet’s change the title in index.html and go with \n# git add status commit \n# git log {gives the latest commit on top and old will get down} \n \n \nGit diff \n======== \nLet’s see, the diff command gives the difference b/w two commits. \n# git diff xxxxxx..xxxxxx  \nYou can get diff b/w any sha’s like sha1..sha20. \nNow our log started increasing, like this the changes keep on adding file is one but there are different versions of this file. \n# git log --since YYYY-MM-DD \n# git log --author ravi \n# git log --grep HTML  { commit message } \n# git log --oneline \n \n \n \nGit Branching \n================ \nIn a collaborative environment, it is common for several developers to share and work on the same source code. \nSome developers will be fixing bugs while others would be implementing new features.  \nTherefore, there has got to be a manageable way to maintain different versions of the same code base. \nThis is where the branch function comes to the rescue. Branch allows each developer to branch out from the original \ncode base and isolate their work from others. Another good thing about branch is that it helps Git to easily merge the \nversions later on. \n \nIt is a common practice to create a new branch for each task (eg. bug fixing, new features etc.)",
          "char_count": 1712,
          "ocr_used": false
        },
        {
          "page": 18,
          "text": "Branching means you diverge from the main line(master-working copy of application) of development and \ncontinue to do work without messing with that main line. \n \n \nBasically, you have your master branch and you don't want to mess anything up on that branch. \n \nIn many VCS tools, branching is an expensive process, often requiring you to create a new copy of your source code \ndirectory, which can take a long time for large projects. \nSome people refer to Git’s branching model as its “killer feature” and it certainly sets Git apart in the VCS community. \n \nWhy is it so special? \n \nThe way Git branches is incredibly lightweight, making branching operations nearly instantaneous, and switching back \nand forth between branches generally just as fast. \n \nWhen we make a commits, this is how git stores them. \n  \n \n \nA branch in Git is simply a lightweight movable pointer to one of these commits. The default branch name in Git is master. \nAs you start making commits, you’re given a master branch that points to the last commit you made. Every time you \ncommit, it moves forward automatically.",
          "char_count": 1113,
          "ocr_used": false
        },
        {
          "page": 19,
          "text": "What happens if you create a new branch? Well, doing so creates a new pointer for you to move around. \n \nLet’s say you create a new branch called testing. \n    # git branch testing \nThis creates a new pointer to the same commit you’re currently on.    \n \n \nTwo branches pointing into the same series of commits. \n \nHow does Git know what branch you’re currently on? \nIt keeps a special pointer called HEAD. \n \nHEAD is a pointer to the latest commit id and is always moving, not stable. \n    # git show HEAD \n \nIn Git, this is a pointer to the local branch you’re currently on. \n \nIn this case, you’re still on master. The git branch command only created a new branch — it didn’t switch to that branch.",
          "char_count": 713,
          "ocr_used": false
        },
        {
          "page": 20,
          "text": "This command shows you where the branch pointers are pointing: \n    # git log --oneline --decorate \nYou can see the “master” and “testing” branches that are right there next to the f30ab commit. \n \n \n \n \nTo switch to an existing branch, you run the git checkout command. \n    # git checkout testing \n       \nThis moves HEAD to point to the testing branch. \n \n \nWhat is the significance of that? \n \nWell, let’s do another commit: \n    # vim test.rb",
          "char_count": 459,
          "ocr_used": false
        },
        {
          "page": 21,
          "text": "# git commit -a -m 'made a change' \n    # git log --oneline --decorate \n \nThe HEAD branch moves forward when a commit is made. \n \n \nThis is interesting, because now your testing branch has moved forward, but your master branch still points to the \ncommit you were on when you ran git checkout to switch branches. \n \nLet’s switch back to the master branch: \n    # git checkout master \n \n \n \nHEAD moves when you checkout. \n \nThat command(git checkout master) did two things. It moved the HEAD pointer back to point to the master branch, and \nit reverted the files in your working directory back to the snapshot that master points to. \n \nTo see all available branches \n    # git branch -a \n    # git reflog {short logs}",
          "char_count": 734,
          "ocr_used": false
        },
        {
          "page": 22,
          "text": "Merging \n===========\n \n \n \n \n \n \n \n \n \nGit Merge Conflict \n================ \n \nA merge conflict happens when two branches both modify the same region of a file and are subsequently merged.  \n \nGit don’t know which of the changes to keep, and thus needs human intervention to resolve the conflict. \n \nShowing the example R&D, Training and Consulting. \n \nAutomatic merge failed",
          "char_count": 395,
          "ocr_used": false
        },
        {
          "page": 23,
          "text": "Example showing Merge Conflict \n============================ \n# On master branch \n \n# vi services.html { Add Two Dummy services like Research & Development } \n# git branch training \n# git branch consulting \n \n# git checkout training \n \n# vi services.html { We provide training } \n# git add && git commit \n# git checkout master \n# git merge training \n \n# git checkout consulting \n# vi services.html { We provide Consulting add in same line} \n# git add && git commit \n# git checkout master \n# git merge consulting \n \nAutomatic merge failed \n  \n# we do get a merge conflict here, open the services.html in vi and resolve the conflicts that occurred. \n \n# git add . \n# git commit -m “Conflict resolved” \n \n \n \nGit ignore \n============ \nIt’s a list of files you want git to ignore in your working directory. \n \n \nIt's usually used to avoid committing transient files from your working directory that aren't useful to other collaborators \nsuch as temp files IDE’s create, Compilation files, OS files etc. \n \n \nA file should be ignored if any of the following is true: \n● The file is not used by your project \n● The file is not used by anyone else in your team \n● The file is generated by another process",
          "char_count": 1211,
          "ocr_used": false
        },
        {
          "page": 24,
          "text": ".gitignore \n============ \nhttps://www.gitignore.io/ \nIf you want some files to be ignored by git create a file .gitignore \n*.bk \n*.class \n# Ignore all php files \n*.php \n# but not index.php \n!index.php \n# Ignore all text files that start with aeiou \n[aeiou]*.txt \n \n \nStashing",
          "char_count": 291,
          "ocr_used": false
        },
        {
          "page": 25,
          "text": "=============\n \n \n \n \n \n# git stash \n \n# git stash list \n \n \n# git stash apply {apply the top most stashed changes} \n \n# git stash apply stash@{2} {apply particular stashed changes} \n# git stash show <stash> \n# git stash pop {apply 2nd stash and remove it} \n# git stash pop stash@{2} {pop the stash at 2nd reference} \n# git stash drop stash@{3} {remove the stash} \n# git stash clear {Delete all stash entries} \n \n \nCherry Picking \n============== \nCheery picking in Git is designed to apply some commit from one branch to another branch.",
          "char_count": 546,
          "ocr_used": false
        },
        {
          "page": 26,
          "text": "You can just revert the commit and cherry-pick it on another branch. \n \n \n \n \nTagging \n============ \n  \nIn release management we are working as a team and I’m working on a module and whenever I’m changing some files I’m \npushing those files to remote master. \n  \nNow I have some 10 files which are perfect working copy, and I don’t want this files to be messed up by my other team \nmembers, these 10 files they can directly go for release. \n  \nBut if I keep them in the repository, as my team is working together, there is always a chance that, somebody or other can \nmess that file, so to avoid these we can do TAGGING.",
          "char_count": 638,
          "ocr_used": false
        },
        {
          "page": 27,
          "text": "You can tag till a particular commit id, imagine all the files till now are my working copies: \n   # git tag 1.0 -m “release 1.0” <commit_id> \n   # git show 1.0 \n   # git push --tags \n  \nGoto GitHub and see release click on it, you can download all the files till that commit. \n \n \nTAGGING helps you in release management.",
          "char_count": 346,
          "ocr_used": false
        },
        {
          "page": 28,
          "text": "Detached Head\n© Basically we use checkout for moving from one\nbranch to another branch,\nme} But if i checkout into a commit id, then I go into a\nfo] state called DETACHED HEAD state.\noY\nIz Say i did # git checkout <commit-id>\nTo DETACHED HEAD: is a state where you are not in\n2 tree anymore, so we cannot track anymore we are\noO outside the tree. Any change we make in detached\nfe} head state are not saved.\n—\nri Now we doesn’t have a pointing branch, we are\na basically in a static commit, if we do\n# git branch\nReason for checking out into a commit id\nTo know what happened at that particular commit.\nre] Let’s say you have created a file and put some phone\nfe] no in there, and now person1 changed the file and\n(7) committed with a message “ph no changed”,\n= then again person2 changed the file and committed\nmo] with same message “ph no changed” later again\ns person3 changed the file and committed with same\n8 message “ph no changed”.\nF™)\nwo Now here we can't rely on commit message itself,\na this is were detached head is needed.",
          "char_count": 1036,
          "ocr_used": true,
          "original_char_count": 34
        },
        {
          "page": 29,
          "text": "MAVEN \nA VCS plays a vital role in any kind of organization, the entire software industry is built around code. \n \nWhat are we doing with this code ?? \n \n● Are we seeing the code when we open the application ?? NO \n● Are we seeing the code when we open the app in browser ?? NO \n \nSo we are seeing the executable format of the code, that is called build result of the code. \n \nWhat is build ?? \n============== \nBuild is the end result of your source code. \n \nBuild tool is nothing but, it takes your source code and converts it into human readable format (executable). \n \nBuild \n==== \nThe term build may refer to the process by which source code is converted into a stand-alone form that can be run on a \ncomputer. \n \nOne of the most important steps of a software build is the compilation process, where source code files are converted into \nexecutable code. \n \nThe process of building software is usually managed by a build tool i.e, maven. \n \nBuilds are created when a certain point in development has been reached or the code has been ready for implementation, \neither for testing or outright release. \n \nBuild: Developers write the code, compile it, compress the code and save it in a compressed folder. This is called Build. \n \nRelease: As a part of this, starting from System study, developing the software and testing it for multiple cycles and deploy the \nsame in the production server. In short, one release consists of multiple builds. \n \nMaven Objectives \n=============== \n● A comprehensive model for projects which is reusable, maintainable, and easier to comprehend(understand). \n● plugins  \n \nConvention over configuration \n========================== \nMaven uses Convention over Configuration which means developers are not required to create build process themselves. \nDevelopers do not have to mention each and every configuration detail. \n \nEarlier to maven we had ANT, which was pretty famous before maven.",
          "char_count": 1936,
          "ocr_used": false
        },
        {
          "page": 30,
          "text": "Disadvantages of ANT \n================== \n   ANT - Ant scripts need to be written for building  \n[build.xml need to tell src & classes ] \n   ANT - There is no dependency management \n   ANT - No project structure is defined \n \nAdvantages of Maven \n==================     \nNo script is required for building [automatically generated - pom.xml] \nDependencies are automatically downloaded \nProject structure is generated by maven \nDocumentation for project can be generated  \n \nMaven is called as project management tool also, the reason is earlier when we used to create projects and we used to \ncreate the directory structure and all by yourself, but now maven will take care of that process. \n \nMAVEN has the ability to create project structure. \nMaven can generate documentation for the project. \n \nWhenever i generate a project using maven i will get src and test all by default. \n \nMAVEN FEATURES \n================ \n \nDependency System \n================= \nInitially in any kind of a build, whenever a dependency is needed, if i’m using ANT i have to download the dependency \nthen keep it in a place where ANT can understood. \n \nIf i’m not giving the dependency manually my build will fail due to dependency issues. \n \nMaven handles dependency in a beautiful manner, there is place called MAVEN CENTRAL. Maven central is a centralized \nlocation where all the dependencies are stored over web/internet. \n \nFor ex im using a project and i’m having a dependency of junit, whenever my build reaches a phase where it needs junit \nthen it will download the dependencies automatically and it will store those dependencies in your machine. There is a \ndirectory called as .m2 created in your machine, where all the dependencies are going to be saved. Next time when it \ncomes across the same dependency, then it doesn’t download it coz its already available in .m2 directory. \n \nPlugin Oriented \n============= \nMaven has so many plugins that i can integrate, i can integrate junit, jmeter, sonarqube, tomcat, cobertura and so many \nother. \n \nIMPORTANT FILE IN MAVEN \n======================= \nProjects in maven is defined by POM (Project Object Model) pom.xml.",
          "char_count": 2162,
          "ocr_used": false
        },
        {
          "page": 31,
          "text": "Maven lifecycle phases \n=================== \n \nWhat is build life cycle? \nThe sequence of steps which is defined in order to execute the tasks and goals of any maven project is known as build \nlifecycle in maven. \n \nThe following are most common default lifecycle phases executed: \n● validate: validate the project is correct and all necessary information[dependencies] are available and keep it in \nlocal repo \n● compile: compile the source code of the project \n● test: Execution of unit tests, test the compiled source code using a suitable unit testing framework. These tests \nshould not require the code be packaged or deployed \n● package: take the compiled code and package it in its distributable format, such as a JAR. \n● verify: run any checks to verify the package is valid and meets quality criteria, keeps the HelloWorld.jar in .m2 \nlocal repo \n● install: Deploy to local repo [.m2], install the package into the local repository, for use as a dependency in other \nprojects locally \n● deploy: done in an integration or release environment, copies the final package to the remote repository for \nsharing with other developers and projects. This will push the libraries from .m2 to remote repo. \n \n \nThere are two other Maven lifecycles of note beyond the default list above. They are \n● clean: cleans up artifacts created by prior builds \n● site: generates site documentation for this project \n \nThese lifecycle phases are executed sequentially to complete the default life cycle. \n \n \nPOM { will be in XML format } \n==== \n \nGAV \n \nMaven uniquely identifies a project using: \n● groupID: Usually it will be the domain name used in reverse format (going to be given by the project manager). \n● artifactID: This should be the name of the artifact that is going to be generated \n● Version : Version of the project, Format {Major}.{Minor}.{Maintenance} and add “-SNAPSHOT” to identify in \ndevelopment \n \nVersion can be two things here, a SNAPSHOT and other is RELEASE. \nSnapshot - whenever your project is in working condition i mean we are still working on it that would be a snapshot \nversion, you can have multiple snapshots for one single project. \nBut there would be only one release for it, for example after my 20th snapshot we decided that 8th snapshot should goto \nrelease then will remove -SNAPSHOT for 8th. \nIn real time we will get the basic pom which is already written with groupid, artifactid and version, we can build up \nrequired plugins and dependencies.",
          "char_count": 2489,
          "ocr_used": false
        },
        {
          "page": 32,
          "text": "Packaging \n========= \nBuild type is identified by <packaging> element, this element will tell how to build the project. \n \nExample packaging types: jar, war etc. \n \nArchetype \n========= \nMaven archetypes are project templates which can be generated for you by Maven. \nIn other words, when you are starting a new project you can generate a template for that project with Maven. \nIn Maven a template is called an archetype. \nEach Maven archetype thus corresponds to a project template that Maven can generate. \nInstallation \n========== \n \nMaven is dependent on java as we are running java applications, so to have maven, we also need to have java in system. \n \nInstall java \n========= \nJava package : java \nprogram \n--- \njava-1.8.0-openjdk \nJava package : java \ncompiler \n--- \njava-1.8.0-openjdk-devel \n \n \n \n# sudo yum -y install java-1.8.0-openjdk \n# sudo yum -y install java-1.8.0-openjdk-devel \n \n# java -version { confirm java version} \n \nInstall Maven \n=========== \n \n \n# yum -y install maven \n \nMaven repository are of three types \n============================= \nFor maven to download the required artifacts of the build and dependencies (jar files) and other plugins which are \nconfigured as part of any project, there should be a common place. This common shared area is called as Repository in \nmaven. \n \nLocal \n===== \nThe repository which resides in our local machine which are cached from the remote/central repository downloads and \nready for the usage. \n \nRemote \n======= \nThis repository as the name suggests resides in the remote server. Remote repository will be used for both downloading \nand uploading the dependencies and artifacts.",
          "char_count": 1660,
          "ocr_used": false
        },
        {
          "page": 33,
          "text": "Central \n====== \nThis is the repository provided by maven community. This repository contains large set of commonly used/required \nlibraries for any java project. Basically, internet connection is required if developers want to make use of this central \nrepository. But, no configuration is required for accessing this central repository. \n \nHow does Maven searches for Dependencies? \n====================================== \nBasically, when maven starts executing the build commands, maven starts for searching the dependencies as explained \nbelow : \n \n● It scans through the local repositories for all the configured dependencies. If found, then it continues with the \nfurther execution. If the configured dependencies are not found in the local repository, then it scans through the \ncentral repository. \n \n● If the specified dependencies are found in the central repository, then those dependencies are downloaded to the \nlocal repository for the future reference and usage. If not found, then maven starts scanning into the remote \nrepositories. \n \n● If no remote repository has been configured, then maven will throw an exception saying not able to find the \ndependencies & stops processing. If found, then those dependencies are downloaded to the local repository for \nthe future reference and usage. \n \n \n \nSetting up stand alone project \n========================= \n \n# cd ~ \n \n# mvn archetype:generate \n \n{ generates project structure } \n \n# we get some number like 1085 beside it we have 2xxx, which means maven \n currently supports 2xxx \nproject structures, 1085 is like default project \n \n# press enter \n \n# choose a number :: 6 which means latest, so press enter \n \n# groupId: com.digital.academy { unique in world, generally domain } \n \n# artifactId: project1 \n \n \n{ project name } \n \n# version: press enter \n \n{ snapshot - intermediate version } \n \n# package: enter { package name is java package } \n \n# Press: y \n \nNow maven has successfully created a project structure for you: \n# tree -a project1 \n \nWe have pom.xml which contains all the definitions for your project generated, this is the main file of the project. \n# ls -l ~/.m2/repository \n \n# mvn validate { whatever in the pom.xml is correct or not }",
          "char_count": 2235,
          "ocr_used": false
        },
        {
          "page": 34,
          "text": "Let’s make some mistakes and try to fail this phase,  \n# mv pom.xml pom.xml.bk \n \n# mvn validate { build failure } \n# mv pom.xml.bk pom.xml \n \n# vi App.java \n \n{ welcome to Devops } \n \n# mvn compile { after changing code we do compilation right } \n \n \n{ this generates a new structure - # tree -a . with class files} \n# mvn test \n \n{ test the application } \n# mvn package { generates the artifact - jar } \n# java -cp target/xxxx.jar  \ngroupid(com.digital.proj1).App \n     \n \nPlugins \n======= \nWe saw maven is only performing phases like validate, compile, test, package, install, deploy but if you remember there is \nno execution of a jar file, \n \nCan you see any of the phases running jar file, no right ?? \n \nExecuting jar file is not part not the part of life cycle, \napart from the above phases such as validate, compile, test, package, install, deploy all the other come under <build> \nunder <plugins>  </plugins> \n \nThese plugins will define, other then regular maven lifecycle phases, \n \nLet’s take example, i want to run my jar file, \nAfter </dependencies> in your pom.xml \n \nAfter </dependencies> in your pom.xml add the following \n \n<build> \n<plugins> \n<plugin> \n<groupId>org.codehaus.mojo</groupId> \n  <artifactId>exec-maven-plugin</artifactId> \n  <version>1.2.1</version> \n  <configuration> \n    <mainClass>com.digi.App</mainClass> \n        <arguments> \n              <argument>-jar</argument> \n              <argument>target/*.jar</argument> \n         </arguments> \n</configuration> \n</plugin> \n</plugins> \n</build> \n</project> \n \nIn pom.xml after </dependencies> add <build> <plugins> <plugin> \n \nWe need to run  \n# mvn exec:java",
          "char_count": 1653,
          "ocr_used": false
        },
        {
          "page": 35,
          "text": "WEB APP SETUP \n \nSetting up web project \n=================== \n# mvn archetype:generate | grep maven-archetype-webapp \n# type the number you get \n# tree -a project/ \n# mvn clean package \n# tree -a project \n \nYou can see the war generated under target directory \n \nTomcat Installation [Binaries] \n======================== \nGoogle tomcat 7 download \n# goto tomcat downloads page and get the binary tar file for the tomcat 7 by wget \n# wget <link> \n# wget http://www-us.apache.org/dist/tomcat/tomcat-7/v7.0.82/bin/apache-tomcat-7.0.82.tar.gz \n# tar xf apache-tomcat-7.0.82.tar.gz \n# cd apache-tomcat-7.0.82/bin \n# ./startup.sh \n# Check for port to be opened in firewall \n# netstat -ntpl { # sudo yum -y install net-tools } \n# ps -ef | grep tomcat \n \n# Goto http://ip-address/8080 {click cancel and change tomcat-users.xml file} \n<role rolename=\"manager-gui\"/> \n<user username=\"tomcat\" password=\"tomcat\" roles=\"manager-gui\"/> \n<user username=\"tomcat1\" password=\"tomcat1\" roles=\"manager-script\"/> \n# Change the port number in server.xml \n# cd apache-tomcat-7.0.81/bin \n# ./shutdown.sh \n# Copy the generated war file to webapps dir of tomcat \n# Refresh the tomcat page \n \nDeploy to tomcat maven tomcat plugin \n===================================== \n \nAdd Manager-Script Role \n===================== \nAdd new role under conf/tomcat-users.xml \n \n# vi conf/tomcat-users.xml \n<user username=\"tomcat1\" password=\"tomcat1\" roles=\"manager-script\"/>",
          "char_count": 1448,
          "ocr_used": false
        },
        {
          "page": 36,
          "text": "Add Maven-Tomcat Authentication \n============================ \n# vi ~/.m2/settings.xml \n<settings> \n   <servers> \n   \n <server> \n   \n \n <id>TomcatServer</id> \n   \n \n <username>tomcat1</username> \n   \n \n <password>tomcat1</password> \n   \n </server> \n   </servers> \n</settings> \n \n \nAdd Tomcat 7 Maven Plugin \n======================= \n# vi pom.xml \n<plugin> \n   <groupId>org.apache.tomcat.maven</groupId> \n   <artifactId>tomcat7-maven-plugin</artifactId> \n   <version>2.2</version> \n   <configuration> \n   \n <url>http://localhost:8080/manager/text</url> \n   \n <server>TomcatServer</server> \n   \n <path>/WebApps</path> \n   </configuration> \n</plugin> \n \n# mvn tomcat7:deploy \n# mvn tomcat7:undeploy \n# mvn tomcat7:redeploy \n \n \n  \nProfiles \n======== \nSo in general, what is the meaning of profile, let’s take windows as example for each profile there would be some different \nsettings right. \n \nI mean in same machine we can have different different profiles. Similarly in pom.xml, the project is same, but you can \ncreate multiple profiles, for multiple purposes. \n  \nSo for my project i want to have different different profiles like dev, qa and prod env. \nHere the requirements are different for each and every env, like in dev env we don’t need any of the test to be run. \n  \nLet’s say there 4 people who have different different req, now i need to create 4 projects instead of 4 projects, within a \nsingle project i can have 4 profiles, that’s the profile concept.",
          "char_count": 1478,
          "ocr_used": false
        },
        {
          "page": 37,
          "text": "<profiles> will not be there under <build>, they will be below </dependencies>, \n So the pom.xml looks like this with <profiles> \n  \n</dependencies> \n<profiles> \n \n<profile> \n \n \n<id>DEV</id> \n \n \n<build> \n \n \n     <plugins> \n \n \n \n<plugin> \n \nI have keep the configuration here please go through it \n \nhttps://github.com/ravi2krishna/Maven-Build-Profiles.git \n \n \n \n \nNEXUS \n \n⇒ Nexus is a Binary Repository Manager \n \nNow the simple solution for this problem is, Dev B should ask Dev A to provide the HelloWorld.jar, so that DevB can keep \nthe Hello.jar in DevB machines .m2 directory, \n \nThis works coz the maven will look first in .m2 directory.[Local Repo] \nIf not then it maven central \n \n \n \n \n  [Apache Repo] \nNow imagine DevA, keeps on changing the Hello.jar code, let’s say DevA changed 100 times now DevB should ask DevA \n100 times which doesn’t make sense.",
          "char_count": 879,
          "ocr_used": false
        },
        {
          "page": 38,
          "text": "Tomorrow in your project, you have 100 Developers, now they have to exchange their libraries, now it won’t be that easy \nto exchange the libraries. \n \nIt’s really cumbersome process, Nobody understands which version is there with whom. \n \nThe solution is there, but this is a complex and time taking solution. \nThis may workout if there are only 2-3 developers, but not more then that. \n \nThis is where Binary Repository Concept comes into picture. \n \nNow we will introduce a new server within our organization. This server we call it as Remote Repository. \n \nJust like how apache is maintaining a MavenCentral, similarly we will maintain our own remote repository. \n \nNow DevA instead of sharing his Hello.jar with DevB, DevC etc \nHe will push it to Our Remote Repository and now DevB, DevC everyone who needs that Hello.jar will pull it from the \nRemote Repository. \n \nI mean they[DevB, DevC] will add that info in the pom.xml, now pom will take care of downloading it from remote repo. \now we got totally three kinds of Repo’s. \n1. Local \n2. Public Repo \n3. Private Repo \nNow even the libraries are secured, coz they are present within your organization. \n \nWe got different tools for that Artifactory/Nexus/Archiva. \n \nSonatype Nexus \n============== \n \nNow we are going to set up this within our server. \n \nInstall Nexus Server. \n \nSearch for Apache Maven Nexus Repository in google, \n \nSnapshot  \n⇒ \nDevelopment progress build [Partial Completed Jars] \n \n \n \n \nRelease  ⇒  \nReady to release build [Official proper release] \n \nInstallation of Nexus \n================= \n \n# type download nexus in google \n# go with version 2.x as 3.x is not yet supported with jenkins \n# copy the link address for 2.x and do wget \n# wget http://www.sonatype.org/downloads/nexus-latest-bundle.tar.gz \n# mkdir -p tools/nexus",
          "char_count": 1818,
          "ocr_used": false
        },
        {
          "page": 39,
          "text": "# tar xvzf nexus.tar.gz -C tools/nexus \n# Nexus by default starts on the port number 8081 and un & pw is admin & admin123 \n# cd nexus/nexus-2.x/bin \n# ./nexus start { if any problem change ownership to devops to both dirs of nexus} \n# netstat -ntpl | grep 8081 \n \n{ be patient it takes some time to start } \n# http://ip-addr:8081/nexus \n \n{ allow port 8081 through firewall } \n# login and give default creds admin & admin123 \n \nERROR: Change the permissions of two nexus directories with logged in username. \n \n \n \nhttp://ip-addr:8081/nexus \n \nOnce the nexus is up we will create two repositories, Snapshot & Release. \n \nSearch for distribution management tag in google for maven & nexus, and paste it after </dependencies> tag. \n \nIf your version contains a string SNAPSHOT, by default it goes to SNAPSHOT repo. \nIf your version contains only version 1.0, it goes to RELEASE repo. \n \nNow we will, deploy the artifacts to remote repo by deploy phase. \n \nLogin to nexus using admin & admin123, now we already we have some default repos, we have something Central, this is \nApache Maven Central. \n \nBut we will go with our own repo’s, using HOSTED repo’s. \n \nAdd → Hosted Repo → Repo Id: releaseRepo → Repo Name: releaseRepo → Repo Policy: Release → Deployment Policy: \nDisable Redeploy → Save \n \nAdd → Hosted Repo → Repo Id: snapshotRepo → Repo Name: snapshotRepo → Repo Policy: Snapshot → Deployment \nPolicy: Allow Redeploy → Save \n \n \nGoto the maven project and do # mvn deploy \n \nSearch for maven distributionmanagement nexus in google, \n \nGoto pom.xml and update <distributionManagement> below </dependencies> \n \n<distributionManagement> \n \n<repository> \n<id>releaseRepo</id> \n<name>releaseRepo</name> \n<url>http://192.168.56.101:8081/nexus/content/repositories/releaseRepo/</url> \n</repository> \n \n<snapshotRepository> \n<id>snapshotRepo</id>",
          "char_count": 1853,
          "ocr_used": false
        },
        {
          "page": 40,
          "text": "<name>snapshotRepo</name> \n<url>http://192.168.56.101:8081/nexus/content/repositories/snapshotRepo/</url> \n</snapshotRepository> \n \n</distributionManagement> \n \nGoto the maven project and do # mvn deploy again, now we get new error like 401. \n \nNexus is strictly authenticated, you cannot deploy until and unless you login, \n \nWe don’t provide usernames and passwords in pom.xml, coz pom.xml files are stored in VCS, which will be shared to \nother[most] developers as well. \n \nFor this maven provides solution in local repo, \n# cd ~/.m2 \n# vi settings.xml {search for maven settings.xml nexus username and pass } \n<settings> \n<servers> \n    <server> \n      <id>releaseRepo</id> \n      <username>admin</username> \n      <password>admin123</password> \n</server> \n<servers> \n    <server> \n      <id>snapshotRepo</id> \n      <username>admin</username> \n      <password>admin123</password> \n</server> \n</settings> \n \nNow change the pom.xml <version> to 1.0-SNAPSHOT and redeploy again. \n \n# mvn deploy \n[After one min, again run mvn deploy] \n \n# mvn deploy \n \nSnapshot is generally for other developers to get the dependency. \n \nNexus Assignment \n================ \n \nFollow this repo to get the code \n \nhttps://github.com/ravi2krishna/CalculatorTestCases.git \n \nPackage the application and implement the add(), substract() and multiply() using the JAR which will be generated from \nthe above repository.",
          "char_count": 1414,
          "ocr_used": false
        },
        {
          "page": 41,
          "text": "SONARQUBE \n \nPrerequisites : \n1. Java 1.7 + { recommended 1.8 } \n2. MySql 5.6+ \n \n \n \nInstall Java 1.8 \n============ \n# sudo yum -y install java-1.8.0-openjdk \n \n# sudo yum -y install java-1.8.0-openjdk-devel \n \n# java -version { confirm java version} \n \n \nInstall MySql 5.6+ { for production use } \n================================ \n \n# wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm \n \n# sudo rpm -ivh mysql-community-release-el7-5.noarch.rpm \n \n# sudo yum -y install mysql-server \n \n# sudo systemctl start mysqld \n \n# sudo mysql_secure_installation \n \n# sudo mysql -u root -p \n \n \n# CREATE DATABASE sonar; \n# CREATE USER 'sonar' IDENTIFIED BY 'sonar'; \n# GRANT ALL ON sonar.* TO 'sonar'@'localhost' IDENTIFIED BY 'sonar'; \n# FLUSH PRIVILEGES; \n \nSonarqube Installation \n=================== \nVisit https://www.sonarqube.org/downloads/ \n# wget the latest version [LTS] \n# unzip <file> \n# cd sonar/conf \n# vim sonar.properties \n \nChanges to make in sonar.properties \n \n============================== \n# sonar.jdbc.username=sonar",
          "char_count": 1057,
          "ocr_used": false
        },
        {
          "page": 42,
          "text": "# sonar.jdbc.password=sonar \n# uncomment sonar.jdbc.url of MySQL 5.6 \n# uncomment sonar.web.host=0.0.0.0 \n# uncomment sonar.web.port=9000 \n \n# cd sonarqube-6.7/bin \n# cd linux-x86-64 \n# ./sonar.sh start \n \nBrowse the sonarqube dashboard on  \nhttp://ip-addr:9000/ \n \nNow browse to one of the maven projects and do mvn sonar:sonar. \n \n# cd SampleApp \n# mvn compile sonar:sonar \n \n# git clone https://github.com/wakaleo/game-of-life \n# cd game-of-life \n# mvn compile sonar:sonar \n------------------------------------------------------------- \n \nJENKINS \n \nINTRODUCTION \n============= \nWhat is jenkins ?? \nJenkins is an application that monitors executions of repeated jobs, such as building a software project. \n \nNow jenkins can do a lot of things in an automated fashion and if a task is repeatable and it can be done in a same way \nover time, jenkins can do it not only doing it but it can automate the process, means jenkins can notify a team when a build \nfails, jenkins can do automatic testing(functional & performance) for builds. \n \nTraditionally, development makes software available in a repository, then they give a call to operations/submit a ticket to \nhelpdesk and then operations builds and deploys that software to one or more environments, one this is done, there is \nusually a QA team which loads and executes performance test on that build and makes it ready for production. \n \nSo what jenkins does is a lot of these are repeatable tasks, which can be automated by using the jenkins. \n \nJenkins has a large number of plugins which helps in this automation process.",
          "char_count": 1601,
          "ocr_used": false
        },
        {
          "page": 43,
          "text": "Continuous Integration \n=================== \nis a development practise that requires developers to integrate code into a shared repository several times per day (repos \nin subversion, CVS, mercurial or git). Each check-in is then verified by an automated build, allowing everyone to detect and \nbe notified of problems with the package immediately. \n \nBuild Pipeline \n============ \nis a process by which the software build is broken down in sections: \n•  Unit test \n•  Acceptance test \n•  Packaging \n•  Reporting \n \n•  Deployment \n•  Notification \n \nThe concepts of Continuous Integration, Build Pipeline and the new “DevOps” movement are revolutionizing how we \nbuild, deploy and use software.  \n \nTools that are effective in automating multiple phases of these processes (like Jenkins) become more valuable in \norganizations where resources, time or both are at a premium.  \n \nInstallation of Jenkins \n===================== \n● We need java to work with jenkins. \n○ \n# sudo yum -y install java-1.8.0-openjdk \n○ \n# sudo yum -y install java-1.8.0-openjdk-devel \n○ \n# java -version { confirm java version} \n● Generally jenkins runs on port 8080, so we need to make sure that there is no other service that is running and \nlistening on port 8080 \n \n \n● Now we need to add jenkins repo, to our repository list, so that we can pull down and install jenkins package. \n○ \n# sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo \n○ \nWe will run a key so that we can trust this repo and pull down jenkins package \n○ \n# sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key \n● Now our key has been imported we can do \n○ \n# sudo yum -y install jenkins \n● Now let’s enable the service \n○ \n# sudo systemctl enable jenkins \n● Now let’s enable the service \n \n○ \n# sudo systemctl start jenkins \n● Now if you do # ip-address:8080 you can see jenkins home \n \n \n \nNow if you do # cat /etc/passwd | grep jenkins",
          "char_count": 1954,
          "ocr_used": false
        },
        {
          "page": 44,
          "text": "jenkins:x:996:994:Jenkins Automation Server:/var/lib/jenkins:/bin/false \n \nCreates a user called jenkins, and keeps this user away from login, this is the default \nbehaviour of jenkins coz we are right now on master and building everything here. \n \nBut in real time we have two things here master and build slaves \nMaster: where jenkins is installed and administration console is \nBuild slaves: these are servers that configured to off load jobs so that master is free \n \nCreating Users \n============ \nManage Jenkins → Manage Users → Create User Left Side → Fill details \nCreate Users : tester 1, tester 2, developer 1 and developer 2 \n \n \n \nCreate new jobs \n============= \ntestingJob ⇒ Execute Shell ⇒ echo \"Testing Team Jobs Info\" \n \ndevelopmentJob ⇒ Execute Shell ⇒ echo \"Development Team Jobs Info\" \n \nNow login with the tester1 and see the list of jobs. \nNow login with the developer1 and see the list of jobs. \n \nAs you can see, both the testing and development team jobs are visible across different teams, which would be a security \nconcern, but this is the default behavior of jenkins. \n \nManage Jenkins → Conϐigure Global Security → Authorization → Logged in users \n \nNow let’s see how we can secure the jenkins to make jobs only visible to testing and development teams. \n \n \nFor this we need to install new plugin called Role-based Authorization Strategy. \n \nLet’s see what are PLUGINS first \n \nPlugins: plugins enhances jenkins power and usability. \n \nInstalling Plugin \n============== \nManage Jenkins → Manage Plugins → Available → Search Role-based → Install without restart. \n \nManage Jenkins → Conϐigure Global Security → Authorization, now we can see the new option Role-Based Strategy. \n \nSelect the Role-Based Strategy → Apply → Save",
          "char_count": 1763,
          "ocr_used": false
        },
        {
          "page": 45,
          "text": "Now if i login with the tester or developer user i won’t have access, i can only access with the admin user. \n \nNow let’s see how we can go and create some roles and based on roles we should grant access to users: \nManage Jenkins → Manage and Assign Roles → Manage roles → \n \n \nGlobal Roles: check Role to add, give something like employee and click add \n \nand give overall read access and over all view access \n \n \n \n \n \nProject Roles: here we can create roles specific to a project, \n \nRole to add: developer && Pattern: dev.* and check everything, now developers will only have access to projects that \nstart with dev but nothing else. Click Apply and Save \n \nRole to add: tester && Pattern: test.* and check everything, now testers will only have access to projects that start with \ntest but nothing else. Click Apply and save \n● So we have created an employee role at global level and we created two roles developer and tester at project \nlevel.",
          "char_count": 958,
          "ocr_used": false
        },
        {
          "page": 46,
          "text": "Assigning Roles To Users \n====================== \n● Manage Jenkins → Manage and assign roles → Assign roles → Global Roles → User/group to add → Add \nUsers tester1, tester2, develoepr1 and developer2 to Global roles as employee → Apply \n \nUnder Item Roles User/group to add → Add Users tester1, tester2, developer1 and developer2 to Item roles → \nNow add Users tester1 & tester2 to Tester Roles and Users developer1 & developer2 to Developer Roles → \nApply \n○ \nSo we created users, we created roles and we assigned roles \n \nNow if you login with tester users you can only see jobs related to testing, and similarly if you login with developer you \ncan see only development related jobs. \n \n \nJenkins [Master - Slave Config] \n============================ \n \nWe know we have user jenkins, who has no shell and this user is the owner of jenkins application but beyond that it’s a \nnormal user. So what we are going to do is manage the global credentials.",
          "char_count": 965,
          "ocr_used": false
        },
        {
          "page": 47,
          "text": "Now we should make a decision that when we run jobs either locally or remotely we are going to run them with the \njenkins user using ssh so that we can control slaves. \n \nSo on our system(master) we are going to change jenkins user, so that we can login with jenkins user, let’s go and do it \n \n# vi /etc/passwd {jenkins change /bin/false to /bin/bash} \n# sudo su jenkins \n \nMake sure our jenkins user is a sudoer \n# usermod -aG google-sudoers jenkins \n# vi /etc/sudoers or # sudo visudo \nBelow root add the following \n \n# root ALL=(ALL)    \nALL \n \n# jenkins  ALL=(ALL) \nNOPASSWD:   ALL \n \nSwitch to jenkins user \n# sudo su jenkins   \n# cd  \n{ make sure you are in jenkins home /var/lib/jenkins} \n \n# ssh-keygen \n \n \nClick on Credentials on homepage of jenkins → Click on Global credentials → Adding some credentials → Kind (select SSH \nusername with private key) → Scope (global) → \nUsername (jenkins) → Private Key (From jenkins master ~/.ssh) → Ok \n \nNow this sets jenkins user account to be available for SSH key exchange with other servers. This is imp coz we want the \nability so that single jenkins user can be in control to run our jobs remotely so that master can off load its jobs to slaves. \n \nSlave \n===== \nNow create a new centos server(machine) where you will be getting new ip {1.2.3.4} \nCreate a user jenkins # useradd jenkins \n# usermod -aG google-sudoers jenkins \n \n# sudo su jenkins - \n# cd  \n{ make sure you are in jenkins home /home/jenkins} \n \n \nNow do password less authentication steps on both machines : \n \n# vi authorized_keys (add public key of other machine) \n# chmod 700 ~/.ssh \n# chmod 600 ~/.ssh/authorized_keys \nNow if everything is good then you should be able to login into the slave machine without password. \n \nDoing builds on slaves \n==================== \nSo we talked about this a lot, that we don’t want to do builds much on master.",
          "char_count": 1879,
          "ocr_used": false
        },
        {
          "page": 48,
          "text": "We are going to create dumb slave nodes, slaves doesn’t need to know anything about implementation they should be able \nto just run jobs and be controlled by master. \n \nWe are going to use this slave nodes in order to off-load the build processing to other machines so that the master server \ndoesn’t get CPU, IO or N/W load etc in managing large number of jobs across multiple servers multiple times a day. \n \nSo we are going to create a slave node: \n# Now make sure there is key exchange b/w both the machines. \n \n# Manage Jenkins (scroll down) → Manage Nodes → Master (Now master is always going to be included by \ndefault  here if we click on master) →  Conϐigure → Usage : Only build jobs with label matching nodes(for the most \npart we want master only to use for controlling jobs we have setup) → Save  \n \n \n \n \n \n# Manage Jenkins (scroll down) → Manage Nodes → New Node → Node name : Remote slave 1 → # of executors : 3 (up to 3 \nconcurrent jobs) → Remote root Directory: (/home/jenkins) Labels: remote1 →  Usage: (as much as possible) → Launch \nmethod: Launch slave via SSH → Host: ip of slave → Credentials: Service acc(Give settings of Kind: SSH username with \nprivate key && Private key: From jenkins master ~/.ssh) → Availability: Keep online as much as possible → Save \nInstall java and javac on all slaves. \nThen click on node and see the log. \n \nNow if i goto jenkins home i can see both master and slave and their executors \n \nBuilding a job on slave \n=================== \nNew job → Under General (✅ Restrict where this project can run)(Label expression: remote1 or expression we gave while \ncreating slave) → Build → Execute Shell → in command give # pwd # uname -a # hostname → Save → Run build. \n \n \n \n \nVAGRANT \n \nVagrant \n======= \nVagrant is a tool for building and managing virtual machine environments. \nWith an easy-to-use workflow and focus on automation, Vagrant lowers development environment setup time, increases \nproduction parity, and makes the \"works on my machine\" excuse a relic of the past. \nVagrant is suitable for development environment. \n \nWhy Vagrant ?? \n============== \nVagrant provides easy to configure, reproducible, and portable work environments built on top of industry-standard \ntechnology and controlled by a single consistent workflow to help maximize the productivity and flexibility of you and \nyour team. \n \nTo achieve its magic, Vagrant stands on the shoulders of giants. Machines are provisioned on top of VirtualBox, VMware,",
          "char_count": 2489,
          "ocr_used": false
        },
        {
          "page": 49,
          "text": "AWS, or any other provider. Then, industry-standard provisioning tools such as shell scripts, Chef, or Puppet, can \nautomatically install and configure software on the virtual machine. \n \nFor Developers \nIf you are a developer, Vagrant will isolate dependencies and their configuration within a single disposable, consistent \nenvironment, without sacrificing any of the tools you are used to working with (editors, browsers, debuggers, etc.). Once \nyou or someone else creates a single Vagrantfile, you just need to vagrant up and everything is installed and configured for \nyou to work. Other members of your team create their development environments from the same configuration, so \nwhether you are working on Linux, Mac OS X, or Windows, all your team members are running code in the same \nenvironment, against the same dependencies, all configured the same way. Say goodbye to \"works on my machine\" bugs. \nFor Operators \nIf you are an operations engineer or DevOps engineer, Vagrant gives you a disposable environment and consistent \nworkflow for developing and testing infrastructure management scripts. You can quickly test things like shell scripts, Chef \ncookbooks, Puppet modules, and more using local virtualization such as VirtualBox or VMware. Then, with the same \nconfiguration, you can test these scripts on remote clouds such as AWS or RackSpace with the same workflow. Ditch your \ncustom scripts to recycle EC2 instances, stop juggling SSH prompts to various machines, and start using Vagrant to bring \nsanity to your life. \nFor Everyone \nVagrant is designed for everyone as the easiest and fastest way to create a virtualized environment! \n \nWhat is the meaning of setting up environment ?? \n================================================ \nLet’s say you need web server \nLet’s say you need db server \nLet’s say you need app server \n \nLet's say you need some machine for R & D purpose quickly, configure that machine quickly and able to use that machine \nvery quickly, now using vagrant we can reduce installation time. \n \nTypically when we setup OS, you may take around 30-45 min to go along with that. \nBut with vagrant you can spin up the development environment very very quickly. \n \nNow how to spin up the environment ?? \n======================================= \nlets see how it can be done \n \nGoto the official website of vagrant vagrantup.com \n \nWe see something like find boxes right, let’s understand some terminology: \nSo when we are going with Base Installation of Linux, we need couple of things \n●    \n You need a CD or ISO image \n●    \n You need a physical machine \n●    \n You define some CPU & RAM \n●    \n Storage \n●    \n Network",
          "char_count": 2671,
          "ocr_used": false
        },
        {
          "page": 50,
          "text": "So whenever we are going with installation we need to go with all these steps always. \n   \nSo if we are working with cloud we have some images like CentOS, ubuntu etc \n●    \n AMI (Amazon Machine Images) \n●    \n Virtualization Layer \n●    \n CPU    {how much CPU i need} \n●    \n RAM {how much RAM i need} \n●    \n Storage {how much storage i need} \n●    \n Network \n \nI need to configure all the above \nSo in vagrant, we call these pre-installed Images/OS as BOXES \nNow you can easily download these boxes, and can spin up the virtual machines easily. \n \nAs i told to setup the OS it takes around 30-45 min, but using the vagrant it hardly takes 5-10 min depending on internet \nspeed. \n \n   \n Download and Install vagrant \n   \n Vagrant supports on following platforms: WIN/MAC/LINUX \n \n   But there is another dependency to vagrant, which is the virtualization layer. \n   So in our laptop/desktop, what is the virtualization layer we use: \n   \n Oracle Virtual Box \n \n   \n \n 1. Download and install {win - CMD} \n   \n \n # vagrant --version     \n \n \n   Now i don't have any box right, let’s download the box: \n   \n # goto vagrantup.com \n   \n # find BOXES {gives list of all boxes} \n \n   So once you go to find boxes, it shows what’s the virtualization layer is and the diff boxes available. \n \n   There is provider, lets understand the terminology \n   \n Here Virtualbox →    Provider \n   \n Provider: tool which is giving you the virtual layer \n   \n But we have diff providers available which provides virtualization layer. \n    \n \n \nGetting Started \n============= \nWe will use Vagrant with VirtualBox, since it is free, available on every major platform, and built-in to Vagrant. \nBut do not forget that Vagrant can work with many other providers.",
          "char_count": 1755,
          "ocr_used": false
        },
        {
          "page": 51,
          "text": "Providers \n======== \nWhile Vagrant ships out of the box with support for VirtualBox, Hyper-V, and Docker, Vagrant has the ability to manage \nother types of machines as well. This is done by using other providers with Vagrant. \n \nBefore you can use another provider, you must install it. Installation of other providers is done via the Vagrant plugin \nsystem. \n \nOnce the provider is installed, usage is straightforward and simple, as you would expect with Vagrant. \n \nYour project was always backed with VirtualBox. But Vagrant can work with a wide variety of backend providers, such as \nVMware, AWS, and more. \n \nOnce you have a provider installed, you do not need to make any modifications to your Vagrantfile, just vagrant up with \nthe proper provider and Vagrant will do the rest: \n \n# vagrant up --provider=vmware_fusion \n# vagrant up --provider=aws \n \n \n \nUp and Running \n=============== \n \n# vagrant init centos/7 \n \n# vagrant up \nAfter running the above two commands, you will have a fully running virtual machine in VirtualBox running. \nYou can SSH into this machine with # vagrant ssh, and when you are done playing around, you can terminate the virtual \nmachine with # vagrant destroy. \n \nProject Setup \n============ \nThe first step in configuring any Vagrant project is to create a Vagrantfile. The purpose of the Vagrantfile is: \n1. Mark the root directory of your project. Many of the configuration options in Vagrant are relative to this root directory. \n2. Describe the kind of machine and resources you need to run your project, as well as what software to install and how \nyou want to access it. \n \nVagrant has a built-in command for initializing a directory for usage with Vagrant: \n# vagrant init \nThis will place a Vagrantfile in your current directory. You can take a look at the Vagrantfile if you want, it is filled with \ncomments and examples. Do not be afraid if it looks intimidating, we will modify it soon enough. \n \nYou can also run vagrant init in a pre-existing directory to setup Vagrant for an existing project. \n \n# vagrant init centos/7 \n \nVagrantfile \n========= \nThe primary function of the Vagrantfile is to describe the type of machine required for a project, and how to configure \nand provision these machines.",
          "char_count": 2258,
          "ocr_used": false
        },
        {
          "page": 52,
          "text": "Vagrant is meant to run with one Vagrantfile per project, and the Vagrantfile is supposed to be committed to version \ncontrol. \nThe syntax of Vagrantfiles is Ruby, but knowledge of the Ruby programming language is not necessary to make \nmodifications to the Vagrantfile, since it is mostly simple variable assignment. \n \n# vagrant up \n========== \nIn less than a minute, this command will finish and you will have a virtual machine running centos 7. You will not actually \nsee anything though, since Vagrant runs the virtual machine without a UI. \n# vagrant ssh \n=========== \nThis command will drop you into a full-fledged SSH session. Go ahead and interact with the machine and do whatever you \nwant. \n  \n  How to work with vagrant \n   \n 1. create a project directory \n   \n 2. create Vagrantfile (configuration file - # vagrant init) \n   \n 3. define the image name you want to use \n \n   Let’s go and do these steps \n   \n # mkdir project1 \n   \n # cd project1 \n   \n It's similar to git, whenever you start with git we say # git init \n   \n similarly for vagrant we use \n   \n # vagrant init \n \n   \n Open Vagrantfile \n   \n \n →  It's a ruby conϐiguration ϐile \n   \n \n →  I'll remove unwanted things like commented section, just keep \n   \n \n \n \n Vagrant.configure(\"2\") do |config| \n   \n \n \n \n \n config.vm.box = \"base\" \n   \n \n \n \n end    \n  \n \nI want centos-7 box so let's search for it in vagrantup.com \n   \n Change the vagrantfile # vi Vagrantfile \n   \n \n \n \n Vagrant.configure(\"2\") do |config| \n   \n \n \n \n \n config.vm.box = \"centos/7\" \n   \n \n \n \n end \n \n   \n \n # vagrant up   \n \n \n \n \n# vagrant up    {this command does the following} \n   \n \n \n 1. It will create a VM with name \"default\" \n   \n \n \n 2. It downloads the centos image to project dir",
          "char_count": 1748,
          "ocr_used": false
        },
        {
          "page": 53,
          "text": "3. Start the VM (by defualt it will be 1 CPU, 512MB RAM) \n   \n \n \n 4. Creates NAT n/w \n   \n \n \n 5. Setup SSH port forwarding \n   \n \n \n 6. Installs SSH keys \n   \n \n \n 7. Maps the storage \n   \n \n \n 8. Execute provision script \n   \n# vagrant up    {execute the command, this brings up the machine} \n \n   \n \n # vagrant ssh {logs you into the machine}     \n \n   \n \n # vagrant halt    {brings down the machine} \n \n# vagrant status {status} \n \n   \n \n # vagrant port {port forwarding} \n \n \n \n \n \nNetwork \n======= \n \nIn order to access the Vagrant environment created, Vagrant exposes some high-level networking options for things such \nas forwarded ports, connecting to a public network, or creating a private network. \n \nPort Forwarding \n============= \nPort forwarding allows you to specify ports on the guest machine to share via a port on the host machine. This allows you \nto access a port on your own machine, but actually have all the network traffic forwarded to a specific port on the guest \nmachine. \n \n \nLet us setup a forwarded port so we can access Apache in our guest. Doing so is a simple edit to the Vagrantfile, which now \nlooks like this: \n \nVagrant.configure(\"2\") do |config| \n  config.vm.box = \"hashicorp/precise64\" \n  config.vm.provision :shell, path: \"bootstrap.sh\" \n  config.vm.network :forwarded_port, guest: 80, host: 4567 \nend \n \n \nRun a # vagrant reload or # vagrant up. Once the machine is running again, load http://127.0.0.1:4567 in your browser. \nYou should see a web page that is being served from the virtual machine that was automatically setup by Vagrant. \n \nVagrant also has other forms of networking, allowing you to assign a static IP address to the guest machine, or to bridge \nthe guest machine onto an existing network.",
          "char_count": 1768,
          "ocr_used": false
        },
        {
          "page": 54,
          "text": "By default vagrant uses NAT network provided by your provider aka Virtual Box. \nBut let say you want to use bridge network to get connected to router, just uncomment the “public_network”, and when \nyou do # vagrant up, it asks for the network to connect. \n \n# config.vm.network \"public_network\" \n \n# config.vm.network :public_network, bridge: \"en0: Wi-Fi (AirPort)\" \n \n \n \nConfigure Hostname \n================= \n \n# config.vm.hostname = “centos” \n \nChanging RAM \n============= \n# Customize the amount of memory on the VM: \nconfig.vm.provider \"virtualbox\" do |vb| \n \nvb.memory = \"1024\" \nend \n \nChanging CPU \n============= \n# Customize the number of CPU’s on the VM: \nconfig.vm.provider \"virtualbox\" do |vb| \n \nvb.cpus = \"2\" \nend \n \nProvisioning \n=========== \nProvisioners in Vagrant allow you to automatically install software, alter configurations, and more on the machine as part \nof the vagrant up process. \n \nOf course, if you want to just use vagrant ssh and install the software by hand, that works. But by using the provisioning \nsystems built-in to Vagrant, it automates the process so that it is repeatable. Most importantly, it requires no human \ninteraction. \n \nVagrant gives you multiple options for provisioning the machine, from simple shell scripts to more complex, industry-\nstandard configuration management systems. \n \nProvisioning happens at certain points during the lifetime of your Vagrant environment: \n● On the first vagrant up that creates the environment, provisioning is run. If the environment was already created \nand the up is just resuming a machine or booting it up, they will not run unless the --provision flag is explicitly \nprovided. \n● When vagrant provision is used on a running environment. \n● When vagrant reload --provision is called. The --provision flag must be present to force provisioning. \nYou can also bring up your environment and explicitly not run provisioners by specifying --no-provision.",
          "char_count": 1952,
          "ocr_used": false
        },
        {
          "page": 55,
          "text": "If we want to install web server in our server. We could just SSH in and install a webserver and be on our way, but then \nevery person who used Vagrant would have to do the same thing. Instead, Vagrant has built-in support for automated \nprovisioning. Using this feature, Vagrant will automatically install software when you vagrant up so that the guest machine \ncan be repeatedly created and ready-to-use. \n \n \nWe will just setup Apache for our basic project, and we will do so using a shell script. Create the following shell script and \nsave it as bootstrap.sh in the same directory as your Vagrantfile. \n \n \nbootstrap.sh \n========== \n \nyum -y install git \n \nyum -y install httpd \nsystemctl start httpd \nsystemctl enable httpd \n \ngit clone https://github.com/devopsguy9/food.git /var/www/html/ \n \nsystemctl restart httpd \n \nNext, we configure Vagrant to run this shell script when setting up our machine. We do this by editing the Vagrantfile, \nwhich should now look like this: \n \n \n \nVagrant.configure(\"2\") do |config| \n   \n \nconfig.vm.box = \"centos/7\" \n   \n \nconfig.vm.provision :shell, path: \"bootstrap.sh\" \n \nend \n \nThe \"provision\" line is new, and tells Vagrant to use the shell provisioner to setup the machine, with the bootstrap.sh file. \nThe file path is relative to the location of the project root (where the Vagrantfile is). \n \nAfter everything is configured, just run vagrant up to create your machine and Vagrant will automatically provision it. You \nshould see the output from the shell script appear in your terminal. If the guest machine is already running from a \nprevious step, run vagrant reload --provision, which will quickly restart your virtual machine. \n \nAfter Vagrant completes running, the web server will be up and running. You can see the website from your own browser. \nThis works because in shell script above we installed Apache and setup the website. \n \nLoad Balancing \n============= \nIn this project we are going to work with three different machines, \nIn our previous Load balance example we took one nginx and two apache servers. \n \nBut in this example we will be using three nginx servers:",
          "char_count": 2144,
          "ocr_used": false
        },
        {
          "page": 56,
          "text": "Vagrantfile \n========== \nVagrant.configure(\"2\") do |config| \n \nconfig.vm.define \"lb1\" do |lb1| \n    \nlb1.vm.box = \"ubuntu/trusty32\" \n    \nlb1.vm.network \"private_network\", ip: \"192.168.45.10\" \n   lb1.vm.network :forwarded_port, host: 7777, guest: 80 \n \n    \nlb1.vm.provision \"shell\", path: \"provision-nginx.sh\" \n \nend \n \nconfig.vm.define \"web1\" do |web1| \n    \nweb1.vm.box = \"ubuntu/trusty32\" \n    \nweb1.vm.network \"private_network\", ip: \"192.168.45.11\" \n   web1.vm.network :forwarded_port, host: 8888, guest: 80  \n    \nweb1.vm.provision \"shell\", path: \"provision-web1.sh\" \nend \n \nconfig.vm.define \"web2\" do |web2| \n    \nweb2.vm.box = \"ubuntu/trusty32\" \n    \nweb2.vm.network \"private_network\", ip: \"192.168.45.12\" \n   web2.vm.network :forwarded_port, host: 9999, guest: 80  \n    \nweb2.vm.provision \"shell\", path: \"provision-web2.sh\" \nend \n \nend \n \nprovision-nginx.sh \n================ \n#!/bin/bash \n \necho \"Starting Provision: Load balancer\" \nsudo apt-get -y install nginx \nsudo service nginx stop \nsudo rm -rf /etc/nginx/sites-enabled/default \nsudo touch /etc/nginx/sites-enabled/default \n \necho \"upstream testapp { \n \nserver 192.168.45.11; \n \nserver 192.168.45.12; \n} \n \nserver { \n \nlisten 80 default_server; \n \nlisten [::]:80 default_server ipv6only=on; \n \nserver_name  localhost;",
          "char_count": 1293,
          "ocr_used": false
        },
        {
          "page": 57,
          "text": "root      /usr/share/nginx/html; \n \n#index index.html index.htm; \n \n \nlocation / { \n \nproxy_pass http://testapp; \n} \n \n}\" >> /etc/nginx/sites-enabled/default \nsudo service nginx start \necho \"MACHINE: LOAD BALANCER\" >> /usr/share/nginx/html/index.html \necho \"Provision LB1 complete\" \n \n \nprovision-web1.sh \n================== \necho \"Starting Provision on A\" \nsudo apt-get install -y nginx \necho \"<h1>MACHINE: A</h1>\" >> /usr/share/nginx/html/index.html \necho \"Provision A complete\" \nprovision-web2.sh \n================== \necho \"Starting Provision on B\" \nsudo apt-get install -y nginx \necho \"<h1>MACHINE: B</h1>\" >> /usr/share/nginx/html/index.html \necho \"Provision B complete\" \n \n# vagrant status \n# vagrant global-status \n# vagrant port lb1 \n# vagrant port web1 \n \nRunning Provisioners \n================== \nProvisioners are run in three cases: \n# vagrant up \n# vagrant provision \n# vagrant reload --provision",
          "char_count": 940,
          "ocr_used": false
        },
        {
          "page": 58,
          "text": "DOCKER \n \nDocker \n======= \n \nDocker is a container management service. \n \nThe keywords of Docker are develop, ship and run anywhere. \n \nThe whole idea of Docker is for developers to easily develop applications, ship them into containers which can then be \ndeployed anywhere. \n \n \n \nRelease of Docker was in March 2013 and since then, it has become the buzzword for modern world development. \nFeatures of Docker \n================= \n● Docker has the ability to reduce the size of development by providing a smaller footprint of the operating \nsystem via containers. \n● With containers, it becomes easier for teams across different units, such as development, QA and Operations to \nwork seamlessly across applications. \n● You can deploy Docker containers anywhere, on any physical and virtual machines and even on the cloud. \n● Since Docker containers are pretty lightweight, they are easily scalable. \n \n \nWhy Virtualization ?? \n================== \n● Hardware Utilization \n● To reduce no of physical servers \n● Reduce Cost \n● More different OS",
          "char_count": 1049,
          "ocr_used": false
        },
        {
          "page": 59,
          "text": "Your whole design of virtualization, is to target the Applications. \n \nWe are focusing more on Hardware, Virtualization and OS. \nBut no one is focusing on Application side, \n \nOn Application side we need two fundamental characteristics: \n \nData Isolation & Data Protection \n \nLet say we are having 3 VM’s and minimum requirements for this system are: \n \n1 CPU & 1 GB RAM now like this i need 3 CPU and 3 GB RAM \n If the same things is needed for like 1000+ machines, it becomes more cumbersome. \n \nSo Docker took advantage of this, by using CONTAINERIZATION. \n \nUsing Docker we can build up entire application with OS. \n \nSometimes it happens like this application works fine on Linux OS, but doesn’t work on Unix and Win, these kind of \nproblems can be avoided by using Docker \n \nUsing docker we can create entire application with OS itslef(OS dependent files). \n \n \nDocker uses special file system, \n   Layered File system[COW - Copy On Write] \n \n   Box  \n→ \n VM's \n   AMI     → \n Instances \n   Images  \n→ \n Containers \n \nThree important things to check in docker: \n \nDocker Container, Docker Images & Docker Registry. \n \nDocker Container: is a running instance of an OS image. \n \n \n       Run time object \n \n \n \n \n Installation \n ============ \n    \n   # get.docker.com \n   # sudo usermod -aG docker <user-name> \n \n   Docker comes in two components SERVER & CLIENT \n   # systemctl start docker \n \n   # sudo docker <options>",
          "char_count": 1437,
          "ocr_used": false
        },
        {
          "page": 60,
          "text": "First thing we need to have is images, from those images will create containers. \n \n   \n # rpm -qa | grep docker \n   \n # sudo systemctl status docker \n \n   \n Let’s see do we have any images \n   \n # sudo docker images \n \nSo where do i get images from ?? \n========================== \nhub.docker.com {search for nexus, jenkins, tomcat and centos} \n \n# docker pull centos {this is how we get image from internet} \n \n# sudo docker images \n{ unique image id and size is also very less coz its limited OS } \n \n# docker info \n# docker images \n \nI want to see how many containers are running ?? \n# sudo docker ps \n \n \n# open two sessions of the same instance {we can do things simultaneously} \n# s1 - sudo docker ps {shows running containers} \n# s2 - sudo docker run -it centos /bin/bash \n{now we are inside container} \n \nit - terminal interactive \n \n \n# s1 - sudo docker ps {shows running containers} \nThis container is created from centos image \n \nIf we use container it will take at least 2 min but. here its 3 seconds \n# s1 - top {so many tasks} \n# s2 - top {literally 2 process} \n# s2 - ps -ef {same 2 process} \n# s2 - cat /etc/hosts {my hostname is container_id} \ns1 - sudo docker ps \n# s1 - sudo docker ps \n# s2 - exit {bash is finished - container is gone} \n# s1 - sudo docker ps \n \n# docker images",
          "char_count": 1310,
          "ocr_used": false
        },
        {
          "page": 61,
          "text": "# docker run -it { attached mode runs in foreground } \n \n# docker run -dt { detached mode runs in background } \n# docker exec -it <container-id> bash \n \nControlling the container \n===================== \n   \n \n \n \nnaming container \n================ \ns2 - # sudo docker run --rm -ti --name \"web-server01\" docker.io/centos /bin/bash \ns1 - sudo docker ps \ns2 - exit \n \nsetting hostname     \n================ \ns2 - sudo docker run --rm -ti --name \"web-server1\" --hostname \"web-server\" docker.io/centos /bin/bash \ns2 - cat /etc/hostname \ns2 - hostname \ns2 - exit \ns1 - sudo docker ps \n \nNow let’s do something interesting \n=========================== \n \n# docker run -it --name demo1 ubuntu /bin/bash \n \n# apt-get update \n \nMy image here is ubuntu, am i downloading the image again ?? \n \nI’m using the same image to build another container, \nso now my container is ubuntu + apt-get update. \n \nEverytime i work on an image, my container has an update. \nWhenever u start working on an image, that is when u start building a container. \n \nNow i’m doing new thing \n# apt-get install apache2 \nNow my container is  demo + apt update + install apache2 \n \nI’ll do exit \n# exit \n \nNow what is command to see the exit ed containers: \n \n# docker ps -a",
          "char_count": 1246,
          "ocr_used": false
        },
        {
          "page": 62,
          "text": "Container lifetime and Persistent data \n================================ \nContainers are usually immutable and ephemeral, just fancy buzzwords for unchanging and temporary or \ndisposable, but the idea here is that we can just throw away the container and create a new one from an image right!!!!. \n Containers are Ephemeral and once a container is removed, it is gone. \n \nWhat about scenarios where you want the applications running inside the container to write to some files/data and then \nensure that the data is still present. For e.g. let’s say that you are running an application that is generating data and it \ncreates files or writes to a database and so on. Now, even if the container is removed and in the future you launch another \ncontainer, you would like that data to still be there. \n \nIn other words, the fundamental thing that we are trying to get over here is to separate out the container lifecycle from the \ndata. Ideally we want to keep these separate so that the data generated is not destroyed or tied to the container lifecycle \nand can thus be reused. This is done via Volumes, which we shall see via several examples. \n \nSo we are not talking about actual limitation of containers, but more of design goal or best practise, this is the idea of \nimmutable infrastructure \n \nSo docker has two solutions to this problem known as Volumes and Bind mounts.  \n \nWorking with volumes   \n=================== \nThere are three main use cases for Docker data volumes: \n1. To keep data around when a container is removed \n2. To share data between the host filesystem and the Docker container \n \nBy Docker Volumes, we are essentially going to look at how to manage data within your Docker containers. \n \n \nFew points about volumes \n====================== \n \n \n● A data volume is a specially designed directory in the container. \n● It is initialized when the container is created. By default, it is not deleted when the container is stopped. It is not \neven garbage collected when there is no container referencing the volume. \n● The data volumes are independently updated. Data volumes can be shared across containers too. \n \n \nMounting a Data volume \n==================== \nWe are going to use the -v [/VolumeName] as an option to mount a volume for our container. \n \n # docker run -it -v /data --name container1 ubuntu bash \n \n \nIn the container do # ls \n Notice that a volume named data is visible now.",
          "char_count": 2424,
          "ocr_used": false
        },
        {
          "page": 63,
          "text": "Let us a do a cd inside the data volume and create a file named file1.txt \n \n# cd data \n# touch file1.txt \n \nSo what we have done so far is to mount a volume /data in the container. \nNow, let us exit the container by typing exit \n \n# exit \n# docker ps \n # docker ps -a \n# docker inspect container1 \n# sudo ls var/lib/docker/volumes \n \nYou will see that it shows our file1.txt that we created. \nNow that the container is stopped i.e. exited, let us restart the container (container1) and see if our volume is still \navailable and that file1.txt exists. \n \n# docker start container1 \n# docker exec -it container1 bash \n \nNow, let’s do an interesting thing. Exit the container and remove the container. \n# docker rm container1 \n# sudo ls var/lib/docker/volumes \n \nThis shows to you that though you have removed the container1, the data volume is still present on the host. This is a \ndangling or ghost volume and could remain there on your machine consuming space. \n \nDo remember to clean up if you want. Alternatively, there is also a -v option while removing the container. \n \n# docker volume rm data { Removes the volume data } \n \nWe got three different implementations in volumes: \n1. Anonymous volumes \n2. Named Volumes \n3. Bind Mounts \n \nAnonymous volumes: don’t have a name it’s not so easy to work with Anonymous volumes if there are multiple \nAnonymous volumes. \n \n#  docker run -it -v /my-data --name container1 ubuntu bash \n \nNamed volumes: have a name to identify them so it’s easy to work with names if there are multiple volumes we can easily \nidentify them with names. \n \n#  docker run -it -v vol1:/my-data --name container1 ubuntu bash",
          "char_count": 1660,
          "ocr_used": false
        },
        {
          "page": 64,
          "text": "Bind Mounts: same process of mounting a volume but this time we will mount an existing host folder in the Docker \ncontainer. This is an interesting concept and is very useful if you are looking to do some development where you \nregularly modify a file in a folder \n \n#  docker run -it -v /home/path:/my-data --name container1 ubuntu bash \n \nMySQL Volumes Example \n===================== \n \n# goto hub.docker.com \n# search for mysql and goto → Details → Click on latest Dockerϐile → Scroll down and you can see VOLUME \n/var/lib/mysql, this is the default location of MySQL Databases. \n \nThis mysql image is programmed in a way to tell docker, when we start a new container from it, it actually creates a new \nvolume location and assign it to this directory /var/lib/mysql, in the container, \n \n Which means any files we put in the container will outlive the container, until we manually delete the volume.   \n \nVolumes need manual deletion, you can’t clean them up just by removing the container that’s an extra setup with volumes, \nthe whole point of volume command is to say that this data is particularly important at least much more important than \ncontainer itself. \n \nNote: You might wanna do \n # docker volume prune \nto cleanup unused volumes and make it easier to see what you’re doing. \n \n# docker volume ls \n \nLet’s run a container from it: \n \n \n# docker container run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD=True mysql \n# docker container ls \n# docker volume ls \n# docker container inspect mysql { you can see Volumes /var/lib/mysql } \n \nAnd if you go up in the output, you can see Mounts, and this is actually the running container \nso actually the container actually thinks it’s getting data or writing data is from /var/lib/mysql, \n \nBut in this case, we can see the data is actually living in Source above line of /var/lib/mysql on the host. \n \nSo let’s do: \n # docker volume ls  \n \nIf you are doing this on a linux machine, You can actually navigate to the volume Source location { /var/lib/docker } and \ncan see the data, i.e some databases. \n \nAnd if i do just hit an up arrow and create a multiple mysql container:",
          "char_count": 2145,
          "ocr_used": false
        },
        {
          "page": 65,
          "text": "# docker container run -d --name mysql2 -e MYSQL_ALLOW_EMPTY_PASSWORD=True mysql \n# docker volume ls \n \n# docker container run -d --name mysql3 -e MYSQL_ALLOW_EMPTY_PASSWORD=True mysql \n# docker volume ls \n \n# docker container run -d --name mysql4 -e MYSQL_ALLOW_EMPTY_PASSWORD=True mysql \n# docker volume ls \n \n We can see two volumes but we can see the problem right ?? \nThere is no easy way to tell which volume belongs to which container. \n \n# docker container stop mysql \n# docker container stop mysql2 \n \n# docker container ls \n# docker volume ls \n # docker container rm -f mysql mysql2 mysql3 \n \n# docker volume ls \n{ my volumes are still there, my data is still safe, so we solved one prob } \n \nSo how we make little more user friendly ?? \n \nThat is where named volumes come in, the ability for us to specify names for docker volumes. \n \nNamed volume [ i can put a name in front of it with : that is known as named vol ] \n \n# docker container run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD=True -v mysql-db:/var/lib/mysql \nmysql   \n \n# docker volume ls \n{ you can see my new container is using a new volume and it’s using a friendly name } \n \n# docker volume inspect mysql-db { this is easier to use here} \n \nAnd if i removed my container: \n \n#  docker container rm -f mysql { -f coz it’s still running, if not stop & remove } \n \n \nAnd if i run another container with some other name and same volume: \n \n# docker container run -d --name mysql3 -e MYSQL_ALLOW_EMPTY_PASSWORD=True -v mysql-\ndb:/var/lib/mysql mysql \n \n# docker volume ls { only mysql-db is there }",
          "char_count": 1584,
          "ocr_used": false
        },
        {
          "page": 66,
          "text": "You can see that we haven’t created a new volume, but still using the same mysql-db volume from earlier. \n \n# docker container inspect mysql3 \n \n{ and we can see Volumes, changed the Source location to be a little friendlier as well } \n \nMore on Mysql - Volumes \n===================== \nRunning mysql container with password: \n# docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=mypassword -d mysql \n \n \nThe following command line will give you a bash shell inside your mysql container: \n# docker exec -it some-mysql bash \n \n# mysql -u root -p \n \nThe MySQL Server log is available through Docker's container log: \n# docker logs some-mysql \n \nRemoving volumes along with container: \n \n# docker container rm -f -v some-mysql \n \nPersistent Data: Bind Mounting \nLook at the same process of mounting a volume but this time we will mount an existing host folder in the Docker \ncontainer. This is an interesting concept and is very useful if you are looking to do some development where you \nregularly modify a file in a folder outside and expect the container to take note or even sharing the volumes across \ndifferent containers. \n \nBind mounts are actually cool, this helps how to use docker for local development. \n \nSo really a bind mount is just a mapping of the host files or directories into a container file or directory. \nIn background, it’s just having two locations pointing to the same physical location(file) on the disk. \n \nFull path rather than just a name like volumes, the way actually docker can tell the difference between named volume \nand bind mount, is that bind mounts starts with a forward slash /  { root } \n \nNow where it really comes to shine is with development and running services inside your container, that are accessing the \nfiles you are using on your host which you are changing. So let’s do that with nginx: \n \ninstallation of Nginx \n================== \n \n# sudo docker run -d -P --name web-server nginx { -P random ports } \n \nNginx \n ===== \n# docker container run -d --name nginx1 -p 8080:80 nginx",
          "char_count": 2037,
          "ocr_used": false
        },
        {
          "page": 67,
          "text": "# docker container run -d --name nginx2 -p 80:80 nginx \n \nCheck the site in host machine http://ip-address:8080 \nThe site which we are seeing is default nginx html file, actually coming from default nginx image. \n \n \nNginx with Mountpoints example \n======================== \n \n# docker container run -d --name nginx -p 80:80 -v ~/website:/usr/share/nginx/html nginx \n \n \nNginx Example ( Volumes ) \n====================== \n# mkdir nginxlogs \n# sudo docker run -d -v ~/nginxlogs:/var/log/nginx -p 80:80 nginx \n \nI do this because every time i don’t want to go into the container and check the logs. \n \nNow i have all the logs in host machine itself rather than container. \n \nUsing multiple volumes on Single Container \n==================================== \nSaving both logs and website data \n \n# docker container run -v ~/nginx-logs:/var/log/nginx -v ~/website:/usr/share/nginx/html nginx \n \nWordpress example - Backup \n======================== \n \n## Creating DB \n# docker run -d --name=wp-mysql -e MYSQL_ROOT_PASSWORD=mypassword -v ~/mysql-data:/var/lib/mysql mysql \n# docker exec -it wp-mysql bash \n# mysql -u root -p \n # create database wordpress; \n \n \n## Creating WP \n# docker run --name my-wordpress --link wp-mysql:mysql -p 8080:80 -d -v ~/wp-data:/var/www/html wordpress \n \nFor Custom Docker Images go with following url \nhttps://github.com/ravi2krishna/Node-Js-Sample-App",
          "char_count": 1399,
          "ocr_used": false
        },
        {
          "page": 68,
          "text": "NAGIOS \nNagios \n======Nagios is a monitoring tool. \n \nIn your organization, you will be working on different different environments like dev, test, pre-prod, prod etc. \n \nNow let's say you have 1000 systems in your infrastructure, so daily it's a tedious task to go and understand what is the \nstatus of these 1000 devices. \n \nFor monitoring all these servers we use NAGIOS. \n    \nWhat we monitor ?? \n================ \n   1. Health    \n  \n{device is up/down} \n   2. Performance    {RAM & CPU utilization} \n   3. Capacity    \n {Watch HDD capacity} \n \nThreshold of Monitoring \n====================== \n   Warning    \n 85% \n   Critical    95% \n \nParameters to monitor \n===================== \n   CPU \n   RAM \n   Storage \n   Network etc \n \n \nNAGIOS SERVER SETUP \n====================== \n \n \nNAGIOS CORE \n============= \n \nGoto nagios.com and nagios.org \n   On nagios.org →  Downloads →  Nagios Core \n \nOn Host Machine \n   # mkdir nagios-software \n   # cd nagios-software \n \ncopy #link of nagios-core.tar.gz",
          "char_count": 1007,
          "ocr_used": false
        },
        {
          "page": 69,
          "text": "# wget <nagios-core-link> \n     # extract the tar \n    \n# sudo yum install -y wget httpd php gcc glibc glibc-common gd gd-devel make net-snmp unzip openssl-devel \n# sudo yum install httpd php php-cli gcc glibc glibc-common gd gd-devel net-snmp openssl-devel wget unzip -y \n \n# sudo useradd nagios \n# sudo groupadd nagcmd \n# sudo usermod -a -G nagcmd nagios \n# sudo usermod -a -G nagcmd apache \n# cd nagios-4.2.0 \n# ./configure \n# make all \n# sudo make install \n# sudo make install-init \n# sudo make install-config \n# sudo make install-commandmode \n# sudo make install-webconf      \n# sudo cp -R contrib/eventhandlers/ /usr/local/nagios/libexec/ \n# sudo chown -R nagios:nagios /usr/local/nagios/libexec/eventhandlers \n \n# sudo /usr/local/nagios/bin/nagios -v /usr/local/nagios/etc/nagios.cfg  [checking for syntax errors and to see if \neverything is working fine ] \n \nCreating nagiosadmin user account \n# sudo htpasswd -c /usr/local/nagios/etc/htpasswd.users nagiosadmin \n \n# sudo service nagios restart \n# sudo service httpd restart \n \nNAGIOS PLUGINS \n================ \n \nOn nagios.org →  Downloads →  Nagios Core Plugin \n    \n# wget <link-nagios-plugins> \n# extract the tar \n \n# cd nagios-plugin-x \n# sudo ./configure \n# make \n# sudo make install \n \n \nNRPE PLUGIN \n=============",
          "char_count": 1295,
          "ocr_used": false
        },
        {
          "page": 70,
          "text": "To get monitor a system we are going to install NRPE plugin, \nNRPE - Nagios Remote Plugin Executor \n \nGoto nagios.org → Nagios core plugin → Find more plugins → General Addons → NRPE → Copy download URL \n \n \n# wget <link-nrpe> \n# cd nrpe \n# ./configure \n \n# sudo make all \n# sudo make install \n# ls -l /usr/local/nagios/libexec/check_nrpe    {installed successfully} \n \n# sudo service nagios restart \n# sudo service httpd restart \n \nTo view Nagios server Dashboard : # ip-add/nagios \n \n \n \n \n \nNAGIOS CLIENT/AGENT SETUP \n========================== \n \nUse another linux machine either on cloud or vm \n \n# sudo useradd nagios \n# sudo yum install -y wget php gcc glibc glibc-common gd gd-devel make net-snmp unzip openssl-devel net-tools xinetd \n \nNAGIOS PLUGINS \n================ \n \nDownloads →  Nagios Core Plugin \n    \n# wget <link-nagios-plugins> \n# extract the tar \n \n# cd nagios-plugin-x \n# ./configure \n# make all \n# sudo make install",
          "char_count": 954,
          "ocr_used": false
        },
        {
          "page": 71,
          "text": "NRPE PLUGIN \n \nTo get monitor a system we are going to install NRPE plugin, \nNRPE - Nagios Remote Plugin Executor \n \nGoto nagios.org → Nagios core plugin → Find more plugins → General Addons → NRPE → Copy download URL \n \n# wget <link-nrpe> \n# cd nrpe \n \n# sudo chown -R nagios:nagios /usr/local/nagios/libexec \n# ./configure \n# make \n# make all \n# sudo make install \n \n# sudo mkdir -p /usr/local/nagios/etc \n# cd nrpe {dir - make sure you are in nrpe directory } \n# sudo cp sample-config/nrpe.cfg /usr/local/nagios/etc \n# cd sample-config \n# sudo vi sample-config/nrpe.xinetd \n \nservice nrpe \n{ \n       flags           = REUSE \n       port            = 5666 \n       socket_type     = stream \n       wait            = no \n       user            = nagios \n       group           = nagios \n       server          = /usr/local/nagios/bin/nrpe \n       server_args     = -c /usr/local/nagios/etc/nrpe.cfg --inetd \n       log_on_failure  += USERID \n       disable         = no \n      only_from       = 127.0.0.1 <ip-add-server> \n} \n \n# sudo cp sample-config/nrpe.xinetd /etc/xinetd.d/nrpe \n# vi /etc/xinetd.d/nrpe \nChange allow from to nagios server ip \n \n# vi /etc/services \n   Add →  nrpe            5666/tcp                # NRPE service \n# ls -l /usr/local/nagios    {i should be nagios:nagios} \n# chown -R nagios:nagios /usr/local/nagios \n# sudo service xinetd start \n# netstat -ntpl",
          "char_count": 1389,
          "ocr_used": false
        },
        {
          "page": 72,
          "text": "Configuring Agent \nIn Server machine \n# cd /usr/local/nagios/etc \n# sudo touch hosts.cfg \n# sudo touch services.cfg \n# sudo vi /usr/local/nagios/etc/nagios.cfg [ goto OBJECT CONFIGURATION FILE(S) ] \n Add the following lines below templates.cfg \n# This config is to add agents \ncfg_file=/usr/local/nagios/etc/hosts.cfg \ncfg_file=/usr/local/nagios/etc/services.cfg \n \n## Default \n \ndefine host{ \nuse generic-host ; Inherit default values from a template \nhost_name c1 ; The name we're giving to this server \nalias CentOS 7 ; A longer name for the server \naddress 192.168.44.11; IP address of Remote Linux host \nmax_check_attempts 5; \n} \n \n# sudo vi /usr/local/nagios/etc/services.cfg \n \ndefine service{ \nuse generic-service \nhost_name c1 \nservice_description CPU Load \ncheck_command check_nrpe!check_load \n} \n \ndefine service{ \nuse generic-service \nhost_name c1 \nservice_description Total Processes \ncheck_command check_nrpe!check_total_procs \n} \n# sudo vi /usr/local/nagios/etc/objects/commands.cfg \n \nAdd the following to end of the file \n \n# Command to use NRPE to check remote host systems \ndefine command{ \ncommand_name check_nrpe \ncommand_line $USER1$/check_nrpe -H $HOSTADDRESS$ -c $ARG1$  }",
          "char_count": 1204,
          "ocr_used": false
        },
        {
          "page": 73,
          "text": "And check for any syntax errors \n# sudo /usr/local/nagios/bin/nagios -v /usr/local/nagios/etc/nagios.cfg \nYou should have Zero warnings and errors \nYou can see Checked hosts \n \n# sudo /etc/init.d/nagios restart \n# sudo systemctl restart httpd \n \n \n \nCHEF \n \nWhen it comes to learning chef \n● \nYou bring your business and problems \n● \nYou know about your infrastructure, you knew the challenges you face within your infrastructure and \nyou know how your infrastructure works \n● \nChef will provide a framework to solve those problems  \n \n \nThe Best way to learn chef is to use chef. \n \nIn chef terminology is very important \n \nCOMPLEXITY \n===========",
          "char_count": 660,
          "ocr_used": false
        },
        {
          "page": 74,
          "text": "System administrators will have a lot of complexity to manage. \nThis complexity can be from many items (Resources) across the infrastructure. \nWe call these items as resources. \n \n \nResources \n========= \nThe resources in your infrastructure can be files, directories, users that you need to manage, packages that should be \ninstalled, services that should be running and list goes on. \n \n \n \nLet’s look at typical application \n========================== \nYou usually start configuring and installing that application on a single server(node). \n \n \n \nTo set up this node we may need to install packages, manage their configurations, installing a database, installing web \nserver, installing application server and lots of things to make this application up and running. \n \nNow overtime you wanna take this application and make it available to the public/client. \n \nSo you are going to have a database server, so maybe now we are going to have multiple environments like staging, dev, \nqa or even production i.e multi tier environment.",
          "char_count": 1043,
          "ocr_used": false
        },
        {
          "page": 75,
          "text": "We have one server that handles all the Application requests and a separate server for Database. \n \nOf course once we have a database server, we want to make sure we don’t lose data, so we decide to make database \nserver redundant. We want to prevent failure and loss of the data. \n \n \n \nWe may as well make the Application server redundant as well. \n \n \n \nSo now we are up to 4 Servers. \n \n========",
          "char_count": 413,
          "ocr_used": false
        },
        {
          "page": 76,
          "text": "But of course as time goes on, load increases on our application, and we need to scale out the no of application servers that \nwe have and in order to do that we need to put a load balancer in front of our app servers, so that requests can be evenly \ndistributed. \n \n \n \n============ \nEventually we gonna reach web scale, this application has grown and grown and getting bigger and bigger and we are now \nat web scale. Look at all these servers now we have to manage. \n \n   \n \n======== \n \nNow we ran into another problem, our database just isn’t keeping up with the demands from our users, so we decided to \nadd a caching layer. \nNow i got even more servers and more complexity to infrastructure.",
          "char_count": 706,
          "ocr_used": false
        },
        {
          "page": 77,
          "text": "========== \n \nAs if this no of servers are not complex enough, and of course each infrastructure has its own topology. There are \nconnections between the load balancer and Application servers. \n \nThe load balancers need to know which application servers to talk to, which application server they are sending requests \nto. \n \nThe Application server in turn needs to know about the database server. \n \nThe Database cache servers need to know what are the backend database servers which they need to do caching. \n \nAll of this just adds up more and more complexity to your infrastructure.",
          "char_count": 605,
          "ocr_used": false
        },
        {
          "page": 78,
          "text": "========= \nAnd your complexity is only going to increase and its increasing quickly. \n \n \n \n \n============ \n \nSo chef solves this problem for you. \n \n \n \n========",
          "char_count": 178,
          "ocr_used": false
        },
        {
          "page": 79,
          "text": "So how can we manage complexity with chef ?? \n \nChef gives us a no of items/tools with which we can manage this complexity, we are going to look at this \n \n \n \n \nWe are going to look at organizations, and how organizations can help you manage complexity. \n \nSo let’s start with top level, let’s see Organizations: \n \n \n \nIf you think about organizations, let’s take digital lync itself it has its own infrastructure and TCS has its own infrastructure \netc etc \n \n \n=====",
          "char_count": 482,
          "ocr_used": false
        },
        {
          "page": 80,
          "text": "Organizations are independent tenants on enterprise chef. \n \nNothing is shared across the organizations, your organizations may represent different companies take Tata group as \nexample. \n \n======= \n \nThe next layer down is environments. \n \n======== \n \nNext comes Roles \n \nRoles is a way of identifying or classifying different types of servers that you have within your infrastructure.",
          "char_count": 400,
          "ocr_used": false
        },
        {
          "page": 81,
          "text": "Within your infrastructure you have multiple instances of servers that are in each one of these roles, you may have \nmultiple application servers, multiple database servers etc \nYou will specify this as roles in chef. \n \n=========== \n \nRoles allow you to define policy, they may include list of chef configuration files that should be applied to the servers or \nnodes that are within that role. \n \nWe call this list of chef configuration files that should be applied we call this a Run list.",
          "char_count": 509,
          "ocr_used": false
        },
        {
          "page": 82,
          "text": "================ \n \nStepping down from roles then we look at nodes",
          "char_count": 84,
          "ocr_used": false
        },
        {
          "page": 83,
          "text": "Belong to Environment : Server can be either in dev or testing or staging or production \n \nRoles : Each node may have zero or more roles, so a node maybe a database server or an application server or it can have \nboth roles the same node can have both database and the application servers. \n \n===========",
          "char_count": 322,
          "ocr_used": false
        },
        {
          "page": 84,
          "text": "Chef client will do all of the heavy lifting, it will make updates, it will configure the node, such that it adheres to the policy \nthat is specified on the chef server. \n \n======== \n \n \n \nBy capturing your infrastructure as code, you can reconstruct all of your business applications from three things a code \nrepository, a backup of your data and the bare metal resources or compute resources that are required to run your \napplications. \nThis puts you in a really really nice state and it’s a great way to manage your complexity of your infrastructure, with these \nthree simple things i can rebuild my applications. \n \n================",
          "char_count": 652,
          "ocr_used": false
        },
        {
          "page": 85,
          "text": "Store the configuration of infrastructure in version control: \nSo gone are the days when you have a server that was hand crafted lovingly by a system administrator and that system \nadministrator is the only person in your organization that knows all of the knobs, dials, tweaks, tricks, packages etc that \nhave been placed onto that server, now we can take the knowledge of that system administrator has and move it into a \nframework that can be stored in a Source code repository. \n \nFramework that allows for abstraction so that you can build up bits and pieces and transform the way you manage your \ninfrastructure. \n \n============ \n \n \nWith chef you can define policies that your infrastructure should follow, \n \nYour policy states what each resource should be in, but not how to get there \nFor example we will in our chef configuration file, we will say that a package should be installed but we will not need to \nspecify how to install that particular package, chef is smart enough to sort that out. \n \nSo for example if you want to install a package on a debian based system you may use apt package manager to install that \npackage, # apt-get install <pkg-name> and \nIf you are running on a redhat based distribution such as centos then apt will not work here we would rather use yum \npackage manager # yum install <pkg-name>",
          "char_count": 1344,
          "ocr_used": false
        },
        {
          "page": 86,
          "text": "Chef abstracts all of that away from you so that you can write configuration files that will work across the various \nplatforms. \n \n======== \n \n======== \n \n \n \n \nYou will take resources and gather them together into recipes. \n \n=========",
          "char_count": 249,
          "ocr_used": false
        },
        {
          "page": 87,
          "text": "Recipes are the real work forces within chef. \n \nSo resources are the building blocks and will take those building blocks and we would gather them together into recipes \nthat would help bring our systems in line with policy. \n \n \n \nLet’s see some example recipe code \n \n \n \n \nI will walk through you what happens when the chef client encounters this recipe code \n \nSo Chef-client is an application that runs on the nodes within your infrastructure. \nIt will gather it’s policy, where its run list from the chef server and it will inspect current system configuration and it will \nthen execute through this recipes or walk through these recipes and ensure the node is in desired state or the node \ncomplies with policy.",
          "char_count": 728,
          "ocr_used": false
        },
        {
          "page": 88,
          "text": "The first resource this recipe includes is a package resource and this package is named apache2, when the chef client sees \nthis, it knows that the package named apache2 should be installed on this particular server/node, if it is not installed the \nchef-client will go ahead and installs that for you. \n \nAgain we are not telling the chef-client, how to install apache2, it’s smart enough to figure that out on its own. \n \nSo our policy states that the package apache2 should be installed, if this is the case already, then the chef-client will  move \non to the next resource in our recipe. \n \nIf that package is not yet installed, chef-client will take care to install it and then move  on to next resource in our recipe. \n \nWith chef we are going to take this recipes and package them up into cookbooks. \n \n========= \n \nSo a cookbook is a container that we will use to describe, configuration data and configuration policies about our \ninfrastructure. \nA cookbook may certainly contain recipes, but it can also include templates, files etc \n \n \n \nSo the recipe we just looked at had template resource in it, the template resource itself had a source file \napache2.conf.erb, that source file is stored as part of the same apache2 cookbook. \nThis cookbooks allow code reuse and modularity. \n \n======== \n \nLet’s look what happens at very very high level, when the chef-client runs on the node.",
          "char_count": 1405,
          "ocr_used": false
        },
        {
          "page": 89,
          "text": "So node is a server in our infrastructure, on the node we have an application called chef-client, this is typically configured \nto execute on a regular interval maybe every 15-30 min using cron job. \n \nWhen chef-client executes it will ask the enterprise-chef(chef-server), what policy should i follow, all of our policy is \ndescribed on the chef-server, our policy includes things like our environments, our roles and our cookbooks. \n \nSo the joining of a node, to a set of policies, we call that as a run-list. \n \n \n \nSo the chef-client will download all of the necessary components, that make up the runlist and will move those down to \nthe node, so you’ll see here the run-list includes the recipe ntp-client, recipe to manage users, and the role to make this \nnode a web server. \n \n==========",
          "char_count": 809,
          "ocr_used": false
        },
        {
          "page": 90,
          "text": "Once the run-list has been downloaded to the node,the chef-clients job is to look at each of the recipes within that run-list \nand ensure that policy is enforced on that particular node, so it brings the node inline with policy. \n \n \n \n========= \n \nSo run-list is, how we specify policy for each node within our infrastructure. \nThe run-list is a collection of policies that node should follow. \nChef-client will obtain this run-list from the chef server. \nThen chef-client ensures the node complies with the policy in the run-list. \n \n \n \n=====",
          "char_count": 565,
          "ocr_used": false
        },
        {
          "page": 91,
          "text": "So with Chef this is how you manage complexity: \n \n \n \nThe first thing you will do, is to determine the desired state of your infrastructure. \nOne we done that, you will identify the resources required to meet that state, what resources are required to meet that \nstate, what resources are required users, services, packages etc. \nYou gather up each of these resources into recipe. \nAgain the run-list is the thing that gets applied to the node, you apply a run-list to each node within your environment and \nas the chef client runs on those nodes, your infrastructure will adhere to the policy modeled in the chef. \n \nThis is a great way to launch new servers to add the capacity to your infrastructure, this is how you add new capacity to an \nexisting infrastructure. \n \nAdding new capacity to an existing infrastructure is not, the only challenge that we face, the other challenge we face is \nConfiguration Drift. \n \n======= \n \n \n \n \n \n \nConfiguration drift happens when your infrastructure requirements change. \nNo infrastructure has static list of requirements, the requirements are certainly going to change over time.",
          "char_count": 1134,
          "ocr_used": false
        },
        {
          "page": 92,
          "text": "Additionally a server within your infrastructure may fall out of line with policy, perhaps a system administrator logged \ninto a server and changed the port number that an application was listening to, maybe that shouldn’t have happened it's \noutside the policy, how do we address that, well chef makes it very easy to manage this, we are going to model the new \nrequirements that we have in chef configuration files and then re-run the chef-client, so when chef-client runs it will \nenforce that each node within your infrastructure is following the current and accurate policy as stored on chef server, so \novertime you can manage change across the infrastructure and you can enforce this policies across the infrastructure. \n \nIn open source we can manage up to 25 nodes freely. \n \nOn workstation side we use chef development kit (chefdk) \nChef-server we can work directly on cloud and we use chef-manage \nOn nodes we use chef-client \n \nSo we totally need three systems to workout. \n \nWorkstation \n \n \nChef-Server  \n \n \n \n  Node \n   MACH1 \n \n \n    MACH2 \n \n \n \nMACH3 \n \nInstall Chef-Server \n=============== \nLet’s get the Chef-server from https://downloads.chef.io/chef-server \n \n# mkdir chef-sw \n# wget <link-of-rhel-7-distro-rpm> \n# sudo rpm -ivh chef-server.rpm \n \nOnce we installed it, we need to configure it by using command: \n \n# sudo chef-server-ctl reconfigure",
          "char_count": 1386,
          "ocr_used": false
        },
        {
          "page": 93,
          "text": "Above command will set up all of the required components, RabbitMQ, PostgreSQL DB, SSL Certificates etc. \n \nOnce it is done successfully, it means server has been set successfully. \n \nstatus of chef server we can use: # sudo chef-server-ctl status which is going to give all the services which are running in \nthe background. \n \nNow let’s access the chef-server by ip add https://192.168.33.10/ \n \nNow let’s setup the web interface to manage chef-server, to get the GUI we need to install another package which is \nchef-manage. \n \n# sudo chef-server-ctl install chef-manage \n \nThis installation of chef-manage will glue the chef-manage to chef-core means it will integrate chef-manage with your \nchef-core. \n \n# sudo chef-manage-ctl reconfigure \n \nNext accept the license agreement and go on with further steps say q for quit and yes. \nOnce the installation is successful use the ip of machine to see the web interface: \n \n \nhttps://ip_address_machine \n \n \nCreate admin user and organization \n============================= \n \nCreating User \n============ \n \nWe need to create an admin user. This user will have access to make changes to the infrastructure components in the \norganization we will be creating. \nBelow command will generate the RSA private key automatically and should be saved to a safe location. \n \n# sudo chef-server-ctl user-create --help \n \n# chef-server-ctl user-create <USER_NAME> <FIRST_NAME> <LAST_NAME> <EMAIL> 'PASSWORD' -f \nPATH_FILE_NAME \n \n# sudo chef-server-ctl user-create admin admin admin admin admin@digital.com password -f \n/etc/chef/admin.pem \n \npem file is your private key which will authenticate you while logging into the server and the corresponding public key \nwill be stored in the server. We should not share this pem file coz now anyone with this pem file can login into the server. \n \nNow you have created the user successfully.",
          "char_count": 1880,
          "ocr_used": false
        },
        {
          "page": 94,
          "text": "Creating Organization \nIt is the time for us to create an organization to hold the chef configurations. \n \n# sudo chef-server-ctl org-create --help \n \n# chef-server-ctl org-create short_name 'full_organization_name' --association_user user_name --filename \nORGANIZATION-validator.pem \n \n# sudo chef-server-ctl org-create dl \"Digital-lync-academy\" --association_user admin -f /etc/chef/dl-\nvalidator.pem \n \nThat’s it this is how we will be creating organization name. \n \nLet’s access the web interface now. \n \n \nhttps://ip_address_machine \n \nOnce we logged in we can see Nodes, but we didn’t setup any node so will see this in later part. \n \nWe can see the organization, on the top right corner. \n \n \nSetting Up Workstation \n=================== \n \nSetting up workstation is very important coz even if you don’t know how to set up chef-server it’s okay coz in real time \nchef-server is already set-up but work station we need to set up yourself. \n \nSteps involved \n============ \n● Create a new centos machine for workstation \n● Login to the workstation machine \n● Setup hostname and add all three in /etc/hosts \n● Download chef-dk on workstation (laptop/desktop/vm) \n○ \nhttps://downloads.chef.io/chefdk \n○ \n# wget <link-of-chef-dk> \n○ \n# sudo rpm -ivh chefdk-1.3.43-1.el7.x86_64.rpm \n○ \n# sudo chef-client -v \n● Download chef-dk on workstation (laptop/desktop/vm) \n● Install the chef-dk through rpm \n● # cd ~ { workstation } \n● Generate Chef-Repo using “chef generate repo” command. \n○ \n# chef generate repo chef-repo \n○ \nThis command places the basic chef repo structure into a directory called “chef-repo” in your home \ndirectory. \n● In chef server we created two pem files one is user and another is organization, now we need to bring this two \npem files into the workstation",
          "char_count": 1784,
          "ocr_used": false
        },
        {
          "page": 95,
          "text": "● Now we need to create a directory .chef in chef-repo where we put RSA keys, \n○ \n# cd ~/chef-repo; # mkdir .chef; cd .chef; \n● Copy the RSA keys to the workstation ~/chef-repo/.chef \n○ \nMake the handshake b/w chef-server & workstation \n○ \n# scp /etc/chef/*.pem vagrant@workstation:~/chef-repo/.chef/ \nሁ Run the above in chef-server \n● Knife is a command line interface for between a local chef-repo and the Chef server. To make the knife to work \nwith your chef environment, we need to configure it by creating knife.rb in the “~/chef-repo/.chef/” directory. \n○ \nGoto Web interface of chef-server → Click Administration → Select Organization (dl) → Click on \nsettings wheel(extreme right) → Click on the Generate Knife Config \n○ \nCopy the file contents and paste it in ~/chef-repo/.chef/knife.rb \n● Testing knife: test the configuration by running knife client list command. Make sure you are in ~/chef-repo/ \ndirectory. \n○ \n# cd ~/chef-repo/ \n○ \n# knife client list \n○ \n# knife ssl check \n○ \nTo resolve this issue, we need to fetch the Chef server’s SSL certificate on our workstation \n○ \n# knife ssl fetch \nሁ The above command will add the Chef server’s certificate file to trusted certificate directory. ( # \ntree .chef) \n● Once the SSL certificate has been fetched, run the previous command to test the knife configuration. \n# knife client list { output then verification completed successfully } \n \n \nSetting Up Node / Bootstrapping new node with knife \n============================================ \n \nBootstrapping a node is a process of installing chef-client on a target machine so that it can run as a chef-client node and \ncommunicate with the chef server. \n \nFrom the workstation, you can bootstrap the node with elevated user privileges. \n \n# setup new machine \n# Setup hostname and add all three in /etc/hosts \n# Install wget \n# Make handshake between workstation to node1 using vagrant user in both machines \n# knife node list {nothing is seen run this command in workstation} \n \nShow the web interface of chef-server, there are no nodes as of now. \n \n# knife bootstrap --help {shows options} \n \n# knife bootstrap 192.168.33.92 -x vagrant -P vagrant -N node1 --sudo \n \nThis command runs the following \n○ \nIt runs ohai process and collects the node info \n○ \nUploads the info to chef server",
          "char_count": 2311,
          "ocr_used": false
        },
        {
          "page": 96,
          "text": "○ \nRegisters the node with  chef server \n○ \nRuns any cookbooks/recipes if defined \n \n \nLet’s write first recipe [ Recipes are in Workstation under cookbooks dir ] \n============================================================ \n# chef generate cookbook <cook-book> \n \n→ \nNew style (Chef 12+) \n \n \nEverything will do it on workstation. \n \n1. Create cookbook on workstation \n2. Then will upload this cookbook on server \n3. Then will associate the cookbook with node \n4. Then will execute that cookbook on that particular node \n \n \n(last step is automatic in real time, but for now will do it manually) \n \n# chef generate cookbook file-test [ makes sure under cookbooks dir ] \n \nknife cookbook create is the legacy way to create a cookbook, it builds the structure of a cookbook with all possible \ndirectories. \n \nchef generate cookbook is part of Chef-DK and aims at generating a testable cookbook structure with minimal \ndirectories to use in test-kitchen. \n \nBoth can be tweaked, the chef generate is easiest to tweak as the command has been written in this way to allow all to \nbuild the cookbook structure that better fits their needs. \n \n \nOnce we create this cook we need to understand the structure of this cookbook, so let’s see the tree structure of our \ncookbook # tree file-test \n \nThe no.1 directory you need to check is recipes under which we have default.rb where you will write your code, write \nyour work \n \n1 default.rb \n========= \nSo what will you write in recipe?? What is recipe ?? \nRecipe is collection of resources with desired state. \nSo your resource can be a file, directory, package etc. \n \n2 metadata.rb \n=========== \nMetadata.rb has the information about version no of cookbook, dependencies, organization info and so on all the \ninformation related with cookbook will be here. \nIf this cookbook is dependent on other bookbook then this info is also stored here. \n \n# sudo vim cookbooks/file-test/metadata.rb \n(change Maintainer etc)",
          "char_count": 1965,
          "ocr_used": false
        },
        {
          "page": 97,
          "text": "Let’s start creating our resources : \n \n# vim cookbooks/file-test/recipes/default.rb \nSo here we will be defining resources in ruby language, now it’s not necessary that you should be master in ruby lang to \nunderstand this, coz structure is very easy to understand. \n \ndefault.rb \n======== \nfile '/tmp/sample.txt' do \n    \ncontent 'This file is created with CHEF' \n    \nowner 'root' \n    \ngroup 'root' \n    \nmode '644' \n \n#action :create \nend \n \nEvery resource has got a default action, the action here is to create. \n \n# knife cookbook test file-test \n \nNow we created the cookbook but we need to upload this cookbook to server. \n \n# knife cookbook list \n{when i execute this command, it fetches list of cookbooks from server} \n \nNow if we check the same in web interface [Policy are cookbooks], i don’t have file-test cookbook, so will upload the \ncookbook. \n \n# knife cookbook upload file-test \n \nUploaded successfully, let’s confirm it. \n \n# knife cookbook list \n \nNow let’s check the same in web interface \n \nNow we need to associate this cookbook with a node. \n \nFirst let’s see what nodes are available \n \n# knife node list \n \nMore info about node \n \n# knife node show <node-name> \n \n# knife node show node1 \n \n \nOur focus will be on run list: \n \n# knife node show node1",
          "char_count": 1288,
          "ocr_used": false
        },
        {
          "page": 98,
          "text": "There is nothing in Run List \n \nGoto web interface → Select node1 → Click on Edit run list { nothing }, let’s add one \n \nAdding node to run list \n \n# knife node run_list add <node-name> <cookbook-name> \n \n# knife node run_list add node1 file-test \n \nAgain i’ll go with show, previously we had empty run list \n# knife node show node1  { we can see file-test in run-list } \n \nNow let’s move to node machine \n \nRun the command # sudo chef-client \n \nThis command makes node to talk to server and asks server, do you have any runlist for me ?? Then it will run all the \ncookbooks mentioned in run_list, then it starts executing them. \n \n \n \n# sudo chef-client \n1. Runs the ohai process \n2. It will upload the latest host info to server \n3. It will get the list of runlist and its dependencies \n4. Downloads the cookbooks as mentioned in run_list \n5. Execute the run_list \n \nRESULT : you will get resource with desired state \n \nIf we run the command for the second time # sudo chef-client \n \nNothing is updated coz the resource is already in its desired state. \n \nLet’s do one thing, forcefully damage the system (someone made changes to sample file content) \n \n# vi /tmp/sample {made changes which corrupted the system} \n \nNow let’s run the # sudo chef-client again \n \nNow you can see that the file was not in desired state, bring it back to desired state. \n \nNow if you open and see the file content # vim /tmp/sample.txt it’s back to original all the old tampered data is gone away. \n \nThat’s the desired state, if you define the desired state, it will make sure that it has the desired state. \n \nLet’s tamper the permissions # chmod 777 sample.txt",
          "char_count": 1655,
          "ocr_used": false
        },
        {
          "page": 99,
          "text": "Now if i run # sudo chef-client then the file permissions will be back to normal desired state. \n \nNow that’s what chef is trying to do, whatever you define, it will try to control and make sure it stays that way, but \nif it’s already there in desired state then it will not make any changes and this is called IDEMPOTENCY. \n \nThat means if the state is already achieved then it will not disturb the state. \nNow imagine that you have 10000 systems, so you don’t have to worry about any of the system. You will write the \nrecipe and will execute them that’s it. \n \nNow let’s say you want to edit some content in sample.txt then you will be doing that on recipe, then it will automatically \nreflect on your node machines. \n \nThis is the basic resource we worked on, but in the upcoming session we would see some more recipes to work. \n \n \nToday let’s look deeper into cookbook and recipe configurations \n1. Will create the cookbook/recipe \n2. Will upload the recipe to server \n3. Will associate that particular cookbook with particular node \n4. Will login to node and will execute \n \n \nToday will install web server that is httpd, and will make sure that httpd package is up and running then will create one \nindex.html and will see everything is working fine as expected. Let’s do it one by one. \n \nSo we will create cookbook in workstation (under cookbooks directory) \n# chef generate cookbook webtest \n# vi webtest/metadata.rb (change maintainer and maintainer email) \n \nNow lets update the recipes \n \n# vi webtest/recipes/default.rb \n \n \nOn the command line we generally do \n \n# yum -y install httpd \n \n# systemctl start httpd \n \n# systemctl enable httpd \n \n# echo “WELCOME” > /var/www/html/index.html \n \nNow let’s login to node machine and disable firewall and set selinux to permissive \n \nNow we need to install the package, here in chef we have a resource with name package: \n \n# package ‘httpd’ do \n \n \naction :install \n {by default} \n \n# end",
          "char_count": 1958,
          "ocr_used": false
        },
        {
          "page": 100,
          "text": "By default the action is install, but we want more information like what we can do more with package, so chef has \nexcellent documentation, lets go and check the document and see what more we can do with the package. \n \n# docs.chef.io/resource_package.html \n \nWe can install the package like this, \npackage ‘httpd’ do \n \n \naction :install  \n \nend \nOr simply you can say \npackage ‘httpd’  \n \ncoz the default action is install \n \n \nNow if you want to remove the package you can figure it out from document (actions) \n \npackage ‘httpd’ do \n \n \naction :remove  \n \nend \n \n \nNow i need to start the service, so i will use the second resource module, \nNow let’s go to document section again and see resource_service \nhttps://docs.chef.io/resources.html { select service} \n \n \n \n \nservice ‘httpd’ do \n \n \naction  [:start, :enable] now i can specify two actions also \nend \n \nOr \n \nservice ‘httpd’ do \n \n \naction  :start \nend \n \nservice ‘httpd’ do \n \n \naction  :enable \nend \n \nNow the third thing is we need to go with index.html, so i’ll use the same resource file \n \nfile ‘/var/www/html/index.html’ do \n \n \ncontent “<h1>Welcome to APACHE - By CHEF</h1>” \n \n \nowner ‘root’ \n \n \ngroup ‘root’ \n \n \nmode ‘644’ \n \nend",
          "char_count": 1212,
          "ocr_used": false
        },
        {
          "page": 101,
          "text": "Now need to save the changes then test it and upload it. \n \n# knife cookbook test webtest { good no syntax error } \n \n \nOr \n \n# cookstyle webtest \n \n# knife cookbook upload webtest { we can check this in web interface } \n \n \n# knife node show node-name \n{show what’s already attached to this node} \n# knife node show node1 \n \n# knife node run_list add <node-name> <cookbook-name> \n \n# knife node run_list add node1 webtest \n \nNow if i say show node then i should see two recipes \n \n# knife node show node1 \n \n \nLet’s go and confirm it on server side as well \n \n# go to Nodes section → Select Node → Click Settings (gear) → Edit run list \n \n \nWe can see two run_lists \n \nNow we have done the association, now our task is to execute the chef-client on the node, that will install httpd service \nand starts the service and it will set index page. \n \nLogin back to node machine \n \n# sudo chef-client \n \nNow i’m always running the chef-client manually, i want to do it automatically, for that to happen we need to setup a cron \njob. \n \nWhat is cron ?? \nCron is a linux scheduler with which you can schedule a task at some regular frequency or at regular interval of times. \n \n# crontab -e \n \n{ create crontab } \n* * * * sudo chef-client \n \n# crontab -l \n \nTemplates and Cross-platform \n========================= \n \nNow let’s see some dynamic things with our node, maybe we can use some of our node attributes to update the home page \non our web server.",
          "char_count": 1461,
          "ocr_used": false
        },
        {
          "page": 102,
          "text": "How do i make this particular recipe platform, of course there is no apache2 package in centos or RHEL, it’s called httpd \nin centos/rhel. \n \nSo how do we model this, within the cookbook, let’s take a look: \n \nChef - Attributes \n=============== \n \nIt is a specific detail about a given node. \nI can define this attributes in chef cookbook, \n \nChef gets the attributes by ohai, \n \n# ohai | wc -l { almost gets 4000 parameters } \n \nHow to access these attributes inside the recipe ?? \n \nnode[‘platform’] \n \nnode[‘hostname’] \nLet’s write something that works across different platforms \n \n \nNow we know that in centos the package name, service name and document root is also different, from ubuntu. \n \nNow i’ll create some variables(attributes), that chef will be able to use, and i’ll call this variable's/attributes with \npackage_name, service_name \n \nCreate attributes directory under web-test where we put default.rb \n \n# mkdir cookbooks/webtest/attributes \n \n# vi cookbooks/webtest/attributes/default.rb \n# Creating attributes for multi os \n \ncase node[‘platform’] \n \nwhen “centos”,”rhel” \n \n \ndefault[“package_name”]=”httpd” \n \n \ndefault[“service_name”]=”httpd” \n \n \ndefault[“document_root”]=”/var/www/html” \nwhen “ubuntu”,”debain” \n \n \ndefault[“package_name”]=”apache2” \n \n \ndefault[“service_name”]=”apache2” \n \n \ndefault[“document_root”]=”/var/www” \nend \n \n \n \n \n \nNow change the default.rb in recipes to, \n \npackage node[“package_name”] do",
          "char_count": 1453,
          "ocr_used": false
        },
        {
          "page": 103,
          "text": "action :install \n \n \nend \n \nservice node[“service_name”] do \n \n \naction  :start \nend \nservice node[“service_name”] do \n \n \naction  :enable \nend \n \nfile ‘/var/www/html/index.html’ do \n \n \ncontent “<h1>Welcome to APACHE - By CHEF</h1>” \n \n \nowner ‘root’ \n \n \ngroup ‘root’ \n \n \nmode ‘644’ \n \nend \n \n \nWhat is template ?? \n \nIt is a script with which you will create static files. \n \nindex.html.erb → \nindex.html \nhttpd.conf.erb \n→ \nhttpd.conf \n \n# cd webtest \n# mkdir templates \n# vi webtest/templates/default/index.html.erb \n \n<html> \n \n<h1>Welcome to Chef <%= node[“nodename”] %></h1> \n \n<br> \n \n<h3>The host has total memory <%= node[“memory”][“total”] %></h3> \n \n</html> \n \n# knife cookbook test webtest \n# knife cookbook upload webtest \n \nSUPER MARKET \n=============== \nWith chef there is a beautiful feature, which is called SUPER MARKET. \n \nSuper market is place where we have huge no of readymade recipes, so just we can download them, install and \nstart working also its FREE. \n \n# knife supermarket \n \nWe can download recipe, install recipe, list etc",
          "char_count": 1071,
          "ocr_used": false
        },
        {
          "page": 104,
          "text": "# supermarket.chef.io \n \nSearch for java, \nLet’s see java example \n \n# login to node machine and do: \n \n# java  \n{ not installed } \n \n# javac { not installed } \n \n# knife supermarket download java_se \n \n# untar the java_se \n# cookstyle java_se \n# knife cookbook upload java_se \n# knife node show node1 \n# knife node run_list add node1 java_se \n \nRoles \nRoles help you configure multiple cookbooks to node at once. \nNormally we are going and running one cookbook at a time which would be cumbersome task to avoid this and \nadd multiple cookbooks at once we use roles. \n \nSo far we have been just adding recipes directly to a single node. \nIn practise, Roles make it easy to configure many nodes identically without repeating yourself each time. \n \nLet’s create our first role, \nThe node we are working on so far is a web server, so let’s go and create a web server role, login into the workstation  \nmachine \n# cd chef-repo/roles \n# vi webserver.rb \nname ‘web-server’ \n \n \n \n \n{ each role will have a name } \ndescription ‘Web Servers’ \n \n \n \n{ each role will have a description } \nrun_list ‘recipe[apachetest]’, ‘recipe[php-file]’ \n{ each role may have a run list } \n# Where apachetest is apache & new1 is php \n#your roles may have attributes as well, \n# vim php-file/recipes/default.rb \n                package 'php' do \n  notifies :restart, 'service[httpd]', :immediately \nend \n \ncookbook_file '/var/www/html/index.php' do \n \nsource 'index.php' \nend \n# knife role from file webserver.rb \n# knife role list \n# knife role show web-server \n# knife node run_list add NODE_NAME 'role[ROLE_NAME]'",
          "char_count": 1599,
          "ocr_used": false
        },
        {
          "page": 105,
          "text": "Go to chef server and check roles, now we got new role. \nAnd click on node, say edit run list, and you can see available roles. \n \nNow let’s do one thing, remove the apachetest from Current run list and save run list. \n \nNow goto the node1 and change the index.html file, and do # sudo chef-client, nothing is changed right, coz we have \nremoved the apachetest from the run list. \n \nLet’s apply the role now, Drag the webserver role from Available roles to Current Run List and save run list. \n \nEnvironments \n============= \nUsing environments combined with roles we can execute cookbook on multiple nodes. I can add multiple nodes to \nenvironment and can execute cookbooks on those multiple nodes. \n \nLet’s see what are environments and where they can be used. \nWill see the simple apache cookbook, \n # knife cookbook show apache \n# knife environment list \n \nThe _default environment is read-only and sets no policy at all, you can’t modify any configuration under this \nenvironment. \n \nSimilar to roles \n# vim environments/dev.rb \nname ‘dev’ \ndescription ‘My Dev Env’ \n \n# knife environment from file dev.rb \n# knife environment show dev \n# knife node show node1 \n \nBy default the nodes are part of _default environment, now will put our server in dev env \n# knife node environment_set node1 dev \n# knife node show node1 \n \nTo group all the nodes with one command: \n# knife exec -E 'nodes.find(\"chef_environment:dev\") {|n| puts n.run_list << \"role[web-server]\" unless \nn.run_list.include?(\"role[web-server]\"); n.save }' \n \n# sudo chef-client \n \nDatabags \n========= \nLet’ see a scenario where Data Bags can be used, let’s say we created a cookbook for creating users, \nSo what we do is we will create a new redhat user with some password. \n \n# useradd redhat",
          "char_count": 1767,
          "ocr_used": false
        },
        {
          "page": 106,
          "text": "# password: redhat \n \n# openssl passwd -1 -salt bacon lync123 \n \n# perl -e 'print crypt(\"password*\",\"\\$6\\$salt\\$\") . \"\\n\"' \n \n# perl -e 'print crypt(\"redhat\",\"\\$6\\$cx56hui\\$\") . \"\\n\"' \n \n# cd cookbooks \n# chef generate cookbook user-test \n# vim user-test/recipes/default.rb \ngroup 'redhat' do \n action :create \nend \n \nuser 'redhat' do \n uid '2000' \n password \n'$1$hPx7gY1X$pDm6ir7zJ2jirr4rriJTZ0$1$aISz6I6w$Q1X/dWJ7F2DBWzKZdABon.$1$oviO3aG4$DdUI5iFi6kSwX\nwWDo5qhc.' \n group 'redhat' \n shell ‘/bin/bash’ \n action :create \nend \n \n# knife cookbook test user-test \n# knife cookbook upload user-test \n# knife node show node1 \n# knife node run_list add node1 user-test \n# knife node show node1 \n \nGo back to node1 and check whether user redhat is created or not. \nIn node1 machine \n# id redhat \n# sudo chef-client \n# id redhat \n# sudo grep redhat /etc/shadow { same password which we have given in enc format } \n \nNow we have copied and pasted password in recipe, but never do it, coz recipes are easily accessible. \n \n \nThe otherway you have to assign the passwords is through Databags. \nWe store passwords in databags and databags are safely kept on chef server but not kept on this machine i.e workstation. \n \nLet’s see how to work with databags: \n \nDatabags are a global variables that are stored as JSON data format. \n \nCreating databag and upload it via file \n===============================",
          "char_count": 1401,
          "ocr_used": false
        },
        {
          "page": 107,
          "text": "# vi chef-repo/data_bags/redhat.json { any name } \n \n{ \n \n  “Id” : “redhat”, \n \n  “password” : \n“$1$hPx7gY1X$pDm6ir7zJ2jirr4rriJTZ0$1$aISz6I6w$Q1X/dWJ7F2DBWzKZdABon.$1$oviO3aG4$DdUI5iFi6kSwXwWDo5\nqhc.” \n \n} \n \n \nDatabags commands: \n# knife data bag list \n# knife data bag create redhat_password \n# knife data bag from file redhat_password redhat.json \n# knife data bag list \n# knife data bag show redhat_password \n# knife data bag show redhat_password redhat \n \n# vi cookbooks/user-test/recipes/default.rb \n \n# redhat_password = data_bag_item(‘dbname’,’key’) \n          redhat_password = data_bag_item(‘redhat_password’,’redhat’) \ngroup 'redhat' do \n action :create \nend \n \nuser 'redhat' do \n uid '2000' \n home '/home/redhat' \n password redhat_password[‘password’] \n group 'redhat' \n shell ‘/bin/bash’ \n action :create \nend \n \nIn node machine: \n# sudo cat /var/chef/cache/cookbooks/user-test/recipes/default.rb \nEarlier i was able to see the password.",
          "char_count": 981,
          "ocr_used": false
        },
        {
          "page": 108,
          "text": "Configuring JENKIN \nJenkins \n1) Download and install JDK. \n2) Define JAVA_HOME to <C:\\Program Files\\Java\\jdk1.8.0_**>. Version Used – 8u73 \n3) Define JRE_HOME to <C:\\Program Files\\Java\\jre1.8.0_**>. \n4) Point PATH to <C:\\Program Files\\Java\\jdk1.8.0_**>\\bin. \n5) Download and install Git from https://git-scm.com \nVersion Used– 2.8.3 \n6) Download and install Eclipse IDE for Java Developers from https://eclipse.org \n7) Download Jenkins **.*.zip (for Windows) from https://jenkins.io \nVersion Used – 2.7.1 \n8) Download ANT from http://ant.apache.org, define ANT_HOME as an environment variable (if \nyou would be using ANT). Version Used – 1.9.7 \n9) Download MAVEN from http://maven.apache.org. Install by entering in command prompt \n“mvn –v”. Define MAVEN_HOME as an environment variable. Version Used – 3.3.9 \n10) Install Jenkins in C:\\. Don’t install in C:\\Program Files (x86) as it may cause a permissions \nissue. \n11) Go to localhost:8080 (Default URL of Jenkins). Enter the secret key. \n12) Create a new user. This user will be the “Administrator” of the CI Server. \n13) After login using admin credentials, create a new user like this –",
          "char_count": 1157,
          "ocr_used": false
        },
        {
          "page": 109,
          "text": "Configuring proxy in Jenkins \n1) Click “Manage Jenkins->Manage Plugins->Advanced”. \n2) Enter “proxy.wdf.sap.corp” in “Server”. \n3)  \n4) Enter “8080” in “Port Number”. \n5) Don’t enter your SAP ID in “Username”, just enter your SAP password in “Password”.",
          "char_count": 275,
          "ocr_used": false
        },
        {
          "page": 110,
          "text": "6) Click “Submit” and then click “Check Now”. \n \nNote – This configuration is only needed if you are connected to SAP-Corporate. If you are \nconnected to SAP-Internet or any other network, clear these settings. Assuming you have done \neverything correctly in the project and you have properly defined your POM, run your project \nfirst, on SAP Internet, as SAP-Corporate blocks POM from downloading plugins, JARs and \ndependencies. Also, do not select the option “Delete workspace before build starts”, as it will \ndelete all the plugins, JARs and dependencies and download them again. When on SAP-Internet, \ndon’t use SonarQube as it will be configured for use on SAP-Corporate. \n \nPlugin to be installed \nClick “Manage Jenkins -> Manage Plugins”. Check which are installed by default and which \nneed to be installed. The required plugins are listed below –",
          "char_count": 864,
          "ocr_used": false
        },
        {
          "page": 111,
          "text": "Sheu Res Bur ; ize L nases\nus °\n| 0)\nina Sy F\n- . wy 5\n: 7 the Mago cC .\n° °\na =a\n; °\nthe QWWASP Java HTM Santizes e\n> =a *\nad Allows Hucson Jenin te integrate with Pastrce SCM repositories, ue =\nrs ts the BMD “\nat ips can b e",
          "char_count": 227,
          "ocr_used": true,
          "original_char_count": 6
        },
        {
          "page": 112,
          "text": "Jenkins Global Configuration \n1) Configure these only after all the tools are installed! \n2) Click “Manage Jenkins”. \n3) To configure JDK, Git, SonarQube Scanner, Ant, Maven & Node.js, click “Global Tool \nConfiguration”.",
          "char_count": 234,
          "ocr_used": false
        },
        {
          "page": 113,
          "text": "4) Click “Global Settings” to manage other settings. Please note that you need to have “http://” \nbefore the IPAddress in both Jenkins URL and SonarQube Server URL because the job will fail \nsince Jenkins would not be able to call SonarQube Server.",
          "char_count": 262,
          "ocr_used": false
        },
        {
          "page": 114,
          "text": "",
          "char_count": 0,
          "ocr_used": true,
          "original_char_count": 12
        },
        {
          "page": 115,
          "text": "",
          "char_count": 0,
          "ocr_used": true,
          "original_char_count": 18
        },
        {
          "page": 116,
          "text": "ESLint with Jenkins \n1) Download and install Node.js from https://nodejs.org/en/download/ \nVersion Used – 4.4.5 \n2) Add <Install Directory of NodeJS> and <Install Directory of NodeJS>\\node_modules\\npm\\bin \nto PATH. C:\\User\\<SAP User ID>\\AppData\\Roaming\\npm should also be in PATH. \n \nInstall ESLint by using commanXd “npm install –g eslint” on command prompt (admin). \n3) Use the command “eslint –c eslintrc.js –f checkstyle **.js > eslint.xml” in the Build Step \n“Execute Windows Batch Command” where -> \nI) \n–c points to the file whose config you have to use. \nII) \n“eslintrc.js” is the file which you get after running “eslint --init” at the location \n“C:\\User\\<SAP User ID>\\AppData\\Roaming\\npm\\node_modules\\eslint” (though the \nname of the file generated is “.eslintrc.js”, remove the leading “.”). This file decides the \nstyling format of the js file. \nIII) \n-f points to the ruleset which you are supposed to use. \nIV) \nCheckstyle is the ruleset which I am using. \nV) \n**.js is the name of the file(s) which you want to be analyzed. \nVI) \n“>” is used for pipelining the results to a file, here I have used “eslint.xml”. \n4) Now, you have to publish the contents of the “eslint.xml” file, so I used “Publish Checkstyle \nResults” in the Post-Build step and mentioned file name as “eslint.xml”. \n5) Now, when you make a build, you will see that execution of build fails, here is the solution-> \nI) \nGo to the “Services” app. \nII) \nFind the “Jenkins” service and right-click on it. \nIII) \nSelect “Properties” and go-to the “Log On” tab. \nIV) \nSelect “This Account” and enter your Account ID like this -> “GLOBAL\\<Your SAP \nID>”. \nV) \nEnter and reconfirm your SAP password. \nVI) \nClick “Apply” and “OK”. \nVII) \nStop the Jenkins service and then start the service. \n6) Now, click “Build Now” and you will see that “Batch command” executes successfully. \n7) In case, the above command doesn’t run, run this command “eslint –f checkstyle > eslint.xml”. \n \nCheckstyle, PMD, Cobertura \n1) Download and install Checkstyle, PMD and Code Coverage (Cobertura) plugins. \n2) Now in your project’s pom.xml, include plugins and dependencies required according to the \nformat mentioned on Maven Central repository http://search.maven.org \nor just search for it on Stack OverFlow or just Google it. You may find it on GitHub. \n3) Now, add Build Step “Invoke Top-Level Maven Targets” in your Freestyle Project and add the \nGoal for (separate, individual goals)-> \nI) \nCheckstyle -> “checkstyle:checkstyle –e” \nII) \nPMD -> “pmd:pmd –e” \nIII) \nCPD -> “pmd:cpd –e” \nIV) \nCobertura -> “cobertura:cobertura -Dcobertura.report.format=xml -e” \nHere I am using “–e“ flag to print a complete stack trace in-case any error occurs and its \nhighly recommended. Also, in advanced section of each, mention “pom.xml”. \n4) Publish Checkstyle, PMD, Cobertura reports in their respective Post-Build Step. \n5) Click “Build Now” and see the results.",
          "char_count": 2922,
          "ocr_used": false
        },
        {
          "page": 117,
          "text": "6) Below is the pom.xml needed for the above plugins and also include JUnit. \n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" \nxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \n \nxsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-\nv4_0_0.xsd\"> \n \n<modelVersion>4.0.0</modelVersion> \n \n<groupId>com.mycompany.app</groupId> \n \n<artifactId>junitmavenexample</artifactId> \n \n<packaging>jar</packaging> \n \n<version>1.0-SNAPSHOT</version> \n \n<name>junitmavenexample</name> \n \n<url>http://maven.apache.org</url> \n \n \n \n<build> \n \n \n<plugins> \n \n \n \n<plugin> \n \n \n \n \n<groupId>org.apache.maven.plugins</groupId> \n \n \n \n \n<artifactId>maven-site-plugin</artifactId> \n \n \n \n \n<version>3.5.1</version> \n \n \n \n \n<configuration> \n \n \n \n \n \n<reportPlugins> \n \n \n \n \n \n \n<plugin> \n \n \n \n \n \n \n \n<groupId>org.apache.maven.plugins</groupId> \n \n \n \n \n \n \n \n<artifactId>maven-checkstyle-plugin</artifactId> \n \n \n \n \n \n \n \n<version>2.17</version> \n \n \n \n \n \n \n</plugin> \n \n \n \n \n \n</reportPlugins> \n \n \n \n \n</configuration> \n \n \n \n</plugin> \n \n \n \n<plugin> \n \n \n \n \n<groupId>org.apache.maven.plugins</groupId> \n \n \n \n \n<artifactId>maven-site-plugin</artifactId> \n \n \n \n \n<version>3.5.1</version> \n \n \n \n \n<configuration> \n \n \n \n \n \n<reportPlugins> \n \n \n \n \n \n \n<plugin> \n \n \n \n \n \n \n \n<groupId>org.apache.maven.plugins</groupId> \n \n \n \n \n \n \n \n<artifactId>maven-pmd-plugin</artifactId> \n \n \n \n \n \n \n \n<version>3.6</version> \n \n \n \n \n \n \n \n<configuration> \n \n \n \n \n \n \n \n \n<!-- PMD options --> \n \n \n \n \n \n \n \n \n<targetJdk>1.8</targetJdk> \n \n \n \n \n \n \n \n \n<aggregate>true</aggregate> \n \n \n \n \n \n \n \n \n<format>xml</format> \n \n \n \n \n \n \n \n \n<rulesets> \n \n \n \n \n \n \n \n \n \n<ruleset>/pmd-rules.xml</ruleset> \n \n \n \n \n \n \n \n \n</rulesets>",
          "char_count": 1757,
          "ocr_used": false
        },
        {
          "page": 118,
          "text": "<!-- CPD options --> \n \n \n \n \n \n \n \n \n<minimumTokens>50</minimumTokens> \n \n \n \n \n \n \n \n \n<ignoreIdentifiers>true</ignoreIdentifiers> \n \n \n \n \n \n \n \n</configuration> \n \n \n \n \n \n \n</plugin> \n \n \n \n \n \n</reportPlugins> \n  \n \n \n \n</configuration> \n \n \n \n</plugin> \n \n \n \n<plugin> \n \n \n \n \n<groupId>org.codehaus.mojo</groupId> \n \n \n \n \n<artifactId>cobertura-maven-plugin</artifactId> \n \n \n \n \n<version>2.4</version> \n \n \n \n \n<configuration> \n \n \n \n \n \n<instrumentation> \n \n \n \n \n \n \n<includes> \n \n \n \n \n \n \n \n<include>**/*.class</include> \n \n \n \n \n \n \n</includes> \n \n \n \n \n \n</instrumentation> \n \n \n \n \n</configuration> \n \n \n \n \n<executions> \n \n \n \n \n \n<execution> \n \n \n \n \n \n<id>clean</id> \n \n \n \n \n \n<phase>pre-site</phase> \n \n \n \n \n \n<goals> \n \n \n \n \n \n \n<goal>clean</goal> \n \n \n \n \n \n</goals> \n \n \n \n \n \n</execution> \n \n \n \n \n \n<execution> \n \n \n \n \n \n \n<id>instrument</id> \n \n \n \n \n \n \n<phase>site</phase> \n \n \n \n \n \n \n<goals> \n \n \n \n \n \n \n \n<goal>instrument</goal> \n \n \n \n \n \n \n \n<goal>cobertura</goal> \n \n \n \n \n \n \n</goals> \n \n \n \n \n \n</execution> \n \n \n \n \n</executions> \n \n \n \n</plugin> \n \n \n</plugins> \n \n</build> \n \n \n<reporting> \n \n \n<plugins> \n \n \n \n<plugin> \n \n \n \n<!-- use mvn cobertura:cobertura to generate cobertura reports --> \n \n \n \n \n<groupId>org.codehaus.mojo</groupId> \n \n \n \n \n<artifactId>cobertura-maven-plugin</artifactId> \n \n \n \n \n<version>2.4</version>",
          "char_count": 1397,
          "ocr_used": false
        },
        {
          "page": 119,
          "text": "<configuration> \n \n \n \n \n \n<formats> \n \n \n \n \n \n \n<format>html</format> \n \n \n \n \n \n \n<format>xml</format> \n \n \n \n \n \n</formats> \n \n \n \n \n</configuration> \n \n \n \n</plugin> \n \n \n</plugins> \n \n</reporting> \n \n \n<dependencies> \n \n \n<dependency> \n \n \n \n<groupId>junit</groupId> \n \n \n \n<artifactId>junit</artifactId> \n \n \n \n<version>4.11</version> \n \n \n \n<scope>test</scope> \n \n \n</dependency> \n \n</dependencies> \n</project>\npom.xml\n \n \n \nSonarQube with Jenkins \n1) Download SonarQube Server from www.sonarqube.org/downloads and unzip. Version 5.6 \n2) Download SonarQube Scanner from www.sonarqube.org/downloads and unzip. Version – \n2.6.1 \n3) Download and install SonarQube plugin. \n4) Create environment variable SCANNER_RUNNER_HOME which points to <Directory of \nScanner>. \n5) In PATH environment variable, make an entry which will point to <Directory of Scanner>\\bin. \n6) Download MySQL Community from https://dev.mysql.com/downloads/ \nVersion 5.7.13 \n7) Install MySQL as Server Machine. \n8) Select Custom Installation. \n9) Install Server and Workbench only. \n10) Also select “Show Advanced Options”, which will give logging options. Select all logging \noptions. \n11) Also install X Protocol/MySQL as a Document Store (NoSQL). \n12) Add <Install Directory of MySQL>\\MySQL Server 5.x\\bin to PATH for easy access. \n13) Login to the Local Instance with the password provided during installation. \n14) Create an empty DB schema named “sonar”.",
          "char_count": 1453,
          "ocr_used": false
        },
        {
          "page": 120,
          "text": "Fie eit View Gary Crane Sener Teds Soy Hp\n886 O88a ae ®\nMANAGEMEN * GBF FA B| into icon + |%/G Qf ih hy | sor :\n© see sats i\nA en antes\nlS Sen as\n& oastot\n& datainpoainetre\nasta 9\n0 sti htoun\nA seve\nA ontenste\nsR\n@ outoows\nPerformance Reports !\nA eta Sema Sg\ncas e\nMANAGEMENT : hone: ET\n© server status Se\nRename References\nB cent connections\n© Users ana Prvteges cane GSE =\nstatus ana system Variables\n& data Export\n& ate inportRestore\nINSTANCE\nB startup /Snutsown\nA servertogt\nF options Fie\n@ oamndoars\nHF Pestormance Reports\nGS Pertormance Schema Setup\n>\nrows\nCc >| hoot Revert",
          "char_count": 581,
          "ocr_used": true,
          "original_char_count": 14
        },
        {
          "page": 121,
          "text": "Apply SQL Script to Database\nee ee Review the SQL Script to be Applied on the Database\n‘Online DOL\n1 CREATE SCHEMA ‘sonarl” ;\n2\n< >\naa oral\n\"Apply SOL Script to Database\na Applying SQL script to the database\nApply SQL Script\n‘The folowing tasks will now be executed. Please monitor the execution.\nPress Show Logs to see the execution logs.\n@& Execute SQ Statements\n‘SQL script was successfully applied to the database.\n| Shon tons | tek | [prc | [Gane",
          "char_count": 452,
          "ocr_used": true,
          "original_char_count": 14
        },
        {
          "page": 122,
          "text": "15) Create a new new user with username – root and password – **** from Users and Privileges. \nChange “localhost” to “IPAddress”. \n \n \n \n16) Now, set Administrative privileges of User “root” to DBA. \n \n \n \n17) Now, set Schema privileges of User “sonar” to access only the DB “sonar” and select all \nprivileges, except “GRANT OPTION”.",
          "char_count": 341,
          "ocr_used": false
        },
        {
          "page": 123,
          "text": "18) Now create a new server instance. Give it a name and IP Address. Clear the password and enter \nthe password you entered before. Select the default schema later.",
          "char_count": 184,
          "ocr_used": false
        },
        {
          "page": 124,
          "text": "19) Go to <Directory of SonarQube>\\conf – \n \nI) \nUncomment the following lines in “sonar-properties.xml”and set them – \nsonar.jdbc.url (MySQL) (Server Instance) – Change localhost to <IPAddress>  \nsonar.jdbc.username=root (Server Instance) \nsonar.jdbc.password=_____ (Server Instance) \n \n \n \nII) \nAdd the following line to “wrapper.conf” – \nset.TMPDIR=<Path of SonarQube>\\temp \n \n \n \n20) Install the SonarQube Server (Run as Admin) at <Directory of SonarQube>\\bin\\Windows-x86-\n64\\InstallNTService. \n21) Check if the service is running from the task manager. If not, start it. \n22) Run the SonarQube Server service (Run as Admin) at<Directory of SonarQube>\\bin\\Windows-\nx86-64\\StartNTService. \n23) Use mysql –u root –p –h <IPAddress> to check DB from cmd. Use “SHOW DATABASES;” to \ncheck the list of DBs, use “USE DBNAME;” to select the DB and “SHOW TABLES;” to see its \ntables.",
          "char_count": 889,
          "ocr_used": false
        },
        {
          "page": 125,
          "text": "24) Go to Jenkins and install SonarQube plugin and set build step of SonarQube in project. \n25) Make sonar-project.properties according to language in root directory of project. \n26) Start the Build and get the results. Location of SonarQube is <IPAddress>:9000.",
          "char_count": 283,
          "ocr_used": false
        },
        {
          "page": 126,
          "text": "Selenium Webdriver \n1) Download and install Selenium plugin. \n2) Selenium Hub has been setup at localhost:4444. \n3) Also download and unzip selenium-server-standalone-*.**.*.jar from \nhttp://docs.seleniumhq.org/download/ \n4) Download ChromeDriver from here - http://chromedriver.storage.googleapis.com/index.html \n5) Now, create a simple Java project which invokes the webdriver and the browser (in this case, \nChrome). \n6) Add the location of Chrome WebDriver to PATH. \n7) Install ChromeDriver plugin. \n8) There is a line in LaunchBrowser code which may be like this “http:localhost:4444/wd/hub”. If \nso, provide your laptop’s “IPaddress” there in place of “localhost. \n9) Start the node using this command “java –jar selenium-server-standalone-*.**.*.jar  \n-role node –hub <nodeIPaddress>:4444/grid/register -port <port no>” in the location where \nthis JAR is located. By default, the node runs on the port number 5555, so you don’t really need \nthe part “-port <port no>”. Don’t close the command prompt window. \n10) Check Selenium Grid at “localhost:4444/grid/console” to see if a node has been created. \n11) Invoke “Top-Level Maven Targets” in the Build Step in your Freestyle Project and enter Goal as \n“test –e”. \n12) Click “Build Now” and see that the browser page you wished to see in the browser window \nstart and close automatically. \n \nPostman \n1) Install “postman” by using command “npm install –g newman” on command prompt (admin). \n2) Get a public postman collections JSON file. \n3) Use the command “newman -c *.json -H *.html” in the Build Step “Execute Windows Batch \nCommand” where –H points to the HTML file where test results should be written or else, use \n“-o” instead of “-H” to use other file format. \n4) Click “Build Now” and see the report published in the file. \n \nQTP/UFT \n1) Ensure that your laptop is connected to SAP-Corporate. \n2) Download and install QTP/UFT. \n3) Create a QTP script and save it. \n4) Launch QTP. Select Tools-> Options->Run Sessions-> Configure the results to be saved as .html. \n5) Create a simple VB script which launches QTP, run the test and save the results in a specified \nlocation. Ensure that it works while running the script on command prompt. \n6) Create a new job in Jenkins. \n7) Use the command “CScript “<Path of VBScript>.vbs” “, which will launch QTP, run the test and \nclose it after execution. \n8) If you are making more than 1 build, use “Execute Windows Batch Command” to move the \nexisting .html files to another folder and save the latest report to original folder so that while \npublishing the HTML Report, there are no issues. The command is “move “<Location of .html \nfiles>\" \"<New Location>\" “. Now the “CScript” command should follow after the “move” \ncommand.",
          "char_count": 2744,
          "ocr_used": false
        },
        {
          "page": 127,
          "text": "Creating .zip using Hudson Post-Build Task Plugin \n1) Install Post-Build task “Add Hudson Post-Build Task” plugin. \n2) Select Logical Operation “OR”. \n3) In the Script box, enter the command “jar –cMf *.zip .” \nHere, * is the name of the zip we want to create. “.” Is used to address the parent of the \nfolder/location where the job is. So, whenever this command is run, it will create a ZIP of the \nworkspace. “jar” is used because we are using Java to run this command. Use “jar –cMf” on \ncommand prompt first for information on the flags.  \n \n \n \nNagios: \nNagios is an open source software that can be used for network and \ninfrastructure monitoring. Nagios will monitor servers, switches, applications \nand services. It alerts the System Administrator when something went wrong and \nalso alerts back when the issues has been rectified. \nNagios is useful for keeping an inventory of your servers, and making sure your \ncritical services are up and running. Using a monitoring system, like Nagios, is \nan essential tool for any production server environment. \n \nWith Nagios you can: \n– Monitor your entire IT infrastructure. \n– Identify problems before they occur. \n– Know immediately when problems arise. \n– Share availability data with stakeholders \n– Detect security breaches. \n– Plan and budget for IT upgrades. \n– Reduce downtime and business losses.",
          "char_count": 1373,
          "ocr_used": false
        }
      ],
      "metadata": {
        "format": "PDF 1.7",
        "title": "Microsoft Word - Devops material",
        "author": "SH20064543",
        "subject": "",
        "keywords": "",
        "creator": "",
        "producer": "Microsoft: Print To PDF",
        "creationDate": "D:20190308155237+05'30'",
        "modDate": "D:20190308155237+05'30'",
        "trapped": "",
        "encryption": null
      },
      "char_count": 154692,
      "word_count": 23586,
      "ocr_pages_count": 7,
      "error": null,
      "file_id": "1qv34ku3ZmB4aOC3HapzycAmcLrruR6_P",
      "filename": "DevOps Complete Package.pdf",
      "filepath": "downloads/DevOps Complete Package.pdf",
      "download_link": "https://drive.google.com/uc?export=download&id=1qv34ku3ZmB4aOC3HapzycAmcLrruR6_P"
    },
    {
      "success": true,
      "text": "docker CLI & Dockerfile Cheat Sheet\nTable of Contents\nIntroduction\nContainer Architecture\nIntroduction \n1\n1. docker CLI \n2\n \n1.1 Container Related Commands \n2\n \n1.2 Image Related Commands \n4\n \n1.3 Network Related Commands \n5\n \n1.4 Registry Related Commands \n6\n \n1.5 Volume Related Commands \n6\n \n1.6 All Related Commands \n6\n2. Dockerfile \n6\nAbout the Authors \n8\nContainers allow the packaging of your application (and everything that you need to run it) \nin a “container image”. Inside a container you can include a base operating system, libraries, \nfiles and folders, environment variables, volume mount-points, and your application binaries. \nA “container image” is a template for the execution of a container — It means that you can \nhave multiple containers running from the same image, all sharing the same behavior, which \npromotes the scaling and distribution of the application. These images can be stored in a \nremote registry to ease the distribution.\nOnce a container is created, the execution is managed by the container runtime. You can \ninteract with the container runtime through the “docker” command. The three primary \ncomponents of a container architecture (client, runtime, & registry) are diagrammed below:\nRuntime\nDaemon\nRegistry\nClient\nImage registry\nImages\nContainers\nRemote API\nLocal\nor\n1. docker CLI\n1.1 Container Related Commands\nExamples \nAll examples shown work in Red Hat Enterprise Linux \n1. Run a container in interactive mode:\n \n#Run a bash shell inside an image\n$ docker run -it rhel7/rhel bash \n \n#Check the release inside a container\n[root@.../]# cat /etc/redhat-release\n2. Run a container in detached mode:\n$ docker run --name mywildfly -d -p 8080:8080 jboss/wildfly\n3. Run a detached container in a previously created container network:\n$ docker network create mynetwork\n$ docker run --name mywildfly-net -d --net mynetwork \\ \n\t\n-p 8080:8080 jboss/wildfly\n4. Run a detached container mounting a local folder inside the container:\n$ docker run --name mywildfly-volume -d \\\n\t\n-v myfolder/:/opt/jboss/wildfly/standalone/deployments/ \\\n\t\n-p 8080:8080 jboss/wildflyjboss/wildfly\n5. Follow the logs of a specific container:\n$ docker logs -f mywildfly\n$ docker logs -f [container-name|container-id]\n6. List containers:\n \n# List only active containers\n$ docker ps\n \n# List all containers\n$ docker ps -a\n7. Stop a container:\n \n# Stop a container\n$ docker stop [container-name|container-id]\n \n# Stop a container (timeout = 1 second)\n$ docker stop -t1\n8. Remove a container:\n \n# Remove a stopped container\n$ docker rm [container-name|container-id]\n \n# Force stop and remove a container\n$ docker rm -f [container-name|container-id]\n \n# Remove all containers\n$ docker rm -f $(docker ps-aq)\n \n# Remove all stopped containers\n$ docker rm $(docker ps -q -f “status=exited”)\n9. Execute a new process in an existing container:\n \n# Execute and access bash inside a WildFly container\n$ docker exec -it mywildfly bash\ndocker [CMD] [OPTS] [CONTAINER]\nCommand\ndaemon\nattach\ncommit\ncp\ncreate\ndif f\nexec\nexport\nkill\nlogs\npause\nport\nps\nrename\nrestart\nrm\nrun\nstart\nstats\nstop\ntop\nunpause\nupdate\nwait\nRun the persistent process that manages containers\nAttach to a running container to view its ongoing output or to \ncontrol it interactively\nCreate a new image from a container’s changes\nCopy files/folders between a container and the local filesystem\nCreate a new container\nInspect changes on a container’s filesystem\nRun a command in a running container\nExport the contents of a container’s filesystem as a tar archive\nKill a running container using SIGKILL or a specified signal\nFetch the logs of a container\nPause all processes within a container\nList port mappings, or look up the public-facing port that is NAT-\ned to the PRIVATE_PORT\nList containers\nRename a container\nRestart a container\nRemove one or more containers\nRun a command in a new container\nStart one or more containers\nDisplay one or more containers’ resource usage statistics\nStop a container by sending SIGTERM then SIGKILL after a grace \nperiod\nDisplay the running processes of a container\nUnpause all processes within a container\nUpdate configuration of one or more containers\nBlock until a container stops, then print its exit code\nDescription\n5. Tag an image:\n\t # Creates an image called “myimage” with the tag “v1” for the image jboss/wildfly:latest\n$ docker tag jboss/wildfly myimage:v1\n\t # Creates a new image with the latest tag\n$ docker tag <image-name> <new-image-name>\n\t # Creates a new image specifying the “new tag” from an existing image and tag\n$ docker tag <image-name>[:tag][username/] <new-image-name>.[:new-tag]\n6. Exporting and importing an image to an external file:\n\t # Export the image to an external file\n$ docker save -o <filename>.tar\n\t # Import an image from an external file\n$ docker load -i <filename>.tar\n7 Push an image to a registry:\n$ docker push [registry/][username/]<image-name>[:tag]\n1.2 Image Related Commands\nExamples \nAll examples shown work in Red Hat Enterprise Linux \n1. Build an image using a Dockerfile:\n \n#Build an image\n$ docker build -t [username/]<image-name>[:tag] <dockerfile-path>\n \n#Build an image called myimage using the Dockerfile in the same folder where the command was executed\n$ docker build -t myimage:latest .\n3: List the images:\n$ docker images\n4: Remove an image from the local registry:\n$ docker rmi [username/]<image-name>[:tag]\n2. Check the history of an image:\n \n# Check the history of the jboss/wildfly image\n$ docker history jboss/wildfly\n \n# Check the history of an image\n$ docker history [username/]<image-name>[:tag]\ndocker [CMD] [OPTS] [IMAGE]\nbuild\nhistory\nimages\nimport\ninfo\ninspect \nload\npull\npush\nrmi\nsave\nsearch\ntag\nconnect\ncreate\ndisconnect\ninspect\nls\nrm \nBuild images from a Dockerfile\nShow the history of an image\nList images\nCreate an empty filesystem image and import the contents of the\ntarball into it\nDisplay system-wide information\nReturn low-level information on a container or image\nLoad an image from a tar archive or STDIN\nPull an image or a repository from the registry\nPush an image or a repository to the registry\nRemove one or more images\nSave one or more images to a tar archive \n(streamed to STDOUT by default)\nSearch one or more configured container registries for images\nTag an image into a repository\nConnects a container to a network\nCreates a new network with the specified name\nDisconnects a container from a network\nDisplays detailed information on a network\nLists all the networks created by the user\nDeletes one or more networks\n1.3 Network related commands\ndocker network [CMD] [OPTS]\nCommand\nDescription\nCommand\nDescription\nlogin\nlogout\ncreate\ninspect\nls\nrm\nevents\ninspect\nLog in to a container registry server. If no server is specified then \ndefault is used\nLog out from a container registry server. If no server is specified \nthen default is used\nCreate a volume\nReturn low-level information on a volume\nLists volumes\nRemove a volume\nGet real time events from the server\nShow version information\n1.4 Network related commands\n1.5 Volume related commands\n1.6 Related commands\nDefault is https://index.docker.io/v1/\ndocker volume [CMD] [OPTS]\n2. Dockerfile\nThe Dockerfile provides the instructions to build a container image through the \n`docker build -t [username/]<image-name>[:tag] <dockerfile-path>`\ncommand. It starts from a previously existing Base image (through the FROM clause) \nfollowed by any other needed Dockerfile instructions. \nThis process is very similar to a compilation of a source code into a binary output, but in \nthis case the output of the Dockerfile will be a container image.\nExample Dockerfile\nThis example creates a custom WildFly container with a custom administrative user. It also \nexposes the administrative port 9990 and binds the administrative interface publicly through \nthe parameter ‘bmanagement’.\n# Use the existing WildFly image\nFROM jboss/wildfly\n# Add an administrative user\nRUN /opt/jboss/wildfly/bin/add-user.sh admin Admin#70365 --silent\n#Expose the administrative port\nEXPOSE 8080 9990\n#Bind the WildFly management to all IP addresses\nCMD [“/opt/jboss/wildfly/bin/standalong.sh”, “-b”, “0.0.0.0”, \n“-bmanagement”, “0.0.0.0”]\nCommand\nDescription\nCommand\nDescription\nCommand\nDescription\n# Build the WildFly image\n$ docker build -t mywildfly .\n \n#Run a WildFly server\n$ docker run -it -p 8080:8080 -p 9990:9990 mywildfly\n \n#Access the WildFly administrative console and log in with the credentials admin/Admin#70635\nopen http://<docker-daemon-ip>:9990 in a browser\nUsing the example Dockerfile\nFROM\nMAINTAINER\nRUN\nCMD\nLABEL\nEXPOSE\nENV\nADD\nCOPY\nENTRYPOINT\nVOLUME\nUSER\nWORKDIR\nARG\nONBUILD\nSTOPSIGNAL\nSets the base image for subsequent\nSets the author field of the generated images\nExecute commands in a new layer on top of the current image and \ncommit the results\nAllowed only once (if many then last one takes effect)\nAdds metadata to an image\nInforms container runtime that the container listens on the speci­\nfied network ports at runtime\nSets an environment variable\nCopy new files, directories, or remote file URLs from into the \nfilesystem of the container\nCopy new files or directories into the filesystem of the container\nAllows you to configure a container that will run as an executable\nCreates a mount point and marks it as holding externally mounted \nvolumes from native host or other containers\nSets the username or UID to use when running the image\nSets the working directory for any RUN, CMD, ENTRYPOINT, COPY, \nand ADD commands\nDefines a variable that users can pass at build-time to the builder \nusing --build-arg\nAdds an instruction to be executed later, when the image is used \nas the base for another build\nSets the system call signal that will be sent to the container to exit\nDockerfile instruction arguments\nCommand\nDescription\n$ mkdir -p www/\n$ echo “Server is up” > www/index.html\n$ docker run -d \\\n  -p 8000:8000 \\\n  --name=pythonweb \\\n  -v `pwd`/www:/var/www/html \\\n  -w /var/www/html \\\n  rhel7/rhel \\\n  /bin/python \\\n  -m SimpleHTTPServer 8000 \n$ curl <container-daemon-ip>:8000\n$ docker ps\n$ docker inspect pythonweb | less\n$ docker exec -it pythonweb bash\nExample: Running a web server container\n# Create a directory (if it doesn’t already exist)\n# Make a text file to serve later\n# Run process in a container as a daemon\n# Map port 8000 in container to 8000 on host\n# Name the container “pythonweb”\n# Map container html to host www directory\n# Set working directory to /var/www/html\n# Choose the rhel7/rhel directory\n# Run the Python command for\n  a simple web server listening to port 8000\n# Check that the server is working\n# See that the container is running\n# Inspect the container\n# Open the running container and look inside\nAbout the authors\nBachir Chihani, Ph.D. holds an engineering degree from Ecole \nSuperieure d’Informatique (Algeria) as well as a PhD degree in \nComputer Science from Telecom SudParis (France). Bachir has \nworked as a data engineer, software engineer, and research \nengineer for many years. Previously, he worked as a network \nengineer and got a CCNA Cisco-certification. Bachir has been \nprogramming for many years in Scala/Spark, Java EE, Android \nand Go. He has a keen interest in Open Source technologies \nparticularly in the fields of Automation, Distributed Computing \nand Software/System Design and he likes sharing his experience \nthrough blogging. \nBachir authored many research papers in the field of Context-\nAwareness and reviewed many papers for International \nconferences. He also served as a technical reviewer for many \nbooks including Spring Boot in Action (Manning, 2016) and Unified \nLog Processing (Manning, 2016).\nRafael Benevides is a Director of Developer Experience at Red \nHat. In his current role he helps developers worldwide to be more \neffective in software development, and he also promotes tools \nand practices that help them to be more productive. He worked \nin several fields including application architecture and design. \nBesides that, he is a member of Apache DeltaSpike PMC - a Duke’s \nChoice Award winner project. And a speaker in conferences like \nJUDCon, TDC, JavaOne and Devoxx\nTwitter: @rafabene\nLinkdeIn: https://www.linkedin.com/in/rafaelbenevides\nwww.rafabene.com.",
      "page_count": 8,
      "pages": [
        {
          "page": 1,
          "text": "docker CLI & Dockerfile Cheat Sheet\nTable of Contents\nIntroduction\nContainer Architecture\nIntroduction \n1\n1. docker CLI \n2\n \n1.1 Container Related Commands \n2\n \n1.2 Image Related Commands \n4\n \n1.3 Network Related Commands \n5\n \n1.4 Registry Related Commands \n6\n \n1.5 Volume Related Commands \n6\n \n1.6 All Related Commands \n6\n2. Dockerfile \n6\nAbout the Authors \n8\nContainers allow the packaging of your application (and everything that you need to run it) \nin a “container image”. Inside a container you can include a base operating system, libraries, \nfiles and folders, environment variables, volume mount-points, and your application binaries. \nA “container image” is a template for the execution of a container — It means that you can \nhave multiple containers running from the same image, all sharing the same behavior, which \npromotes the scaling and distribution of the application. These images can be stored in a \nremote registry to ease the distribution.\nOnce a container is created, the execution is managed by the container runtime. You can \ninteract with the container runtime through the “docker” command. The three primary \ncomponents of a container architecture (client, runtime, & registry) are diagrammed below:\nRuntime\nDaemon\nRegistry\nClient\nImage registry\nImages\nContainers\nRemote API\nLocal\nor",
          "char_count": 1312,
          "ocr_used": false
        },
        {
          "page": 2,
          "text": "1. docker CLI\n1.1 Container Related Commands\nExamples \nAll examples shown work in Red Hat Enterprise Linux \n1. Run a container in interactive mode:\n \n#Run a bash shell inside an image\n$ docker run -it rhel7/rhel bash \n \n#Check the release inside a container\n[root@.../]# cat /etc/redhat-release\n2. Run a container in detached mode:\n$ docker run --name mywildfly -d -p 8080:8080 jboss/wildfly\n3. Run a detached container in a previously created container network:\n$ docker network create mynetwork\n$ docker run --name mywildfly-net -d --net mynetwork \\ \n\t\n-p 8080:8080 jboss/wildfly\n4. Run a detached container mounting a local folder inside the container:\n$ docker run --name mywildfly-volume -d \\\n\t\n-v myfolder/:/opt/jboss/wildfly/standalone/deployments/ \\\n\t\n-p 8080:8080 jboss/wildflyjboss/wildfly\n5. Follow the logs of a specific container:\n$ docker logs -f mywildfly\n$ docker logs -f [container-name|container-id]\n6. List containers:\n \n# List only active containers\n$ docker ps\n \n# List all containers\n$ docker ps -a\n7. Stop a container:\n \n# Stop a container\n$ docker stop [container-name|container-id]\n \n# Stop a container (timeout = 1 second)\n$ docker stop -t1\n8. Remove a container:\n \n# Remove a stopped container\n$ docker rm [container-name|container-id]\n \n# Force stop and remove a container\n$ docker rm -f [container-name|container-id]\n \n# Remove all containers\n$ docker rm -f $(docker ps-aq)\n \n# Remove all stopped containers\n$ docker rm $(docker ps -q -f “status=exited”)\n9. Execute a new process in an existing container:\n \n# Execute and access bash inside a WildFly container\n$ docker exec -it mywildfly bash\ndocker [CMD] [OPTS] [CONTAINER]",
          "char_count": 1655,
          "ocr_used": false
        },
        {
          "page": 3,
          "text": "Command\ndaemon\nattach\ncommit\ncp\ncreate\ndif f\nexec\nexport\nkill\nlogs\npause\nport\nps\nrename\nrestart\nrm\nrun\nstart\nstats\nstop\ntop\nunpause\nupdate\nwait\nRun the persistent process that manages containers\nAttach to a running container to view its ongoing output or to \ncontrol it interactively\nCreate a new image from a container’s changes\nCopy files/folders between a container and the local filesystem\nCreate a new container\nInspect changes on a container’s filesystem\nRun a command in a running container\nExport the contents of a container’s filesystem as a tar archive\nKill a running container using SIGKILL or a specified signal\nFetch the logs of a container\nPause all processes within a container\nList port mappings, or look up the public-facing port that is NAT-\ned to the PRIVATE_PORT\nList containers\nRename a container\nRestart a container\nRemove one or more containers\nRun a command in a new container\nStart one or more containers\nDisplay one or more containers’ resource usage statistics\nStop a container by sending SIGTERM then SIGKILL after a grace \nperiod\nDisplay the running processes of a container\nUnpause all processes within a container\nUpdate configuration of one or more containers\nBlock until a container stops, then print its exit code\nDescription",
          "char_count": 1260,
          "ocr_used": false
        },
        {
          "page": 4,
          "text": "5. Tag an image:\n\t # Creates an image called “myimage” with the tag “v1” for the image jboss/wildfly:latest\n$ docker tag jboss/wildfly myimage:v1\n\t # Creates a new image with the latest tag\n$ docker tag <image-name> <new-image-name>\n\t # Creates a new image specifying the “new tag” from an existing image and tag\n$ docker tag <image-name>[:tag][username/] <new-image-name>.[:new-tag]\n6. Exporting and importing an image to an external file:\n\t # Export the image to an external file\n$ docker save -o <filename>.tar\n\t # Import an image from an external file\n$ docker load -i <filename>.tar\n7 Push an image to a registry:\n$ docker push [registry/][username/]<image-name>[:tag]\n1.2 Image Related Commands\nExamples \nAll examples shown work in Red Hat Enterprise Linux \n1. Build an image using a Dockerfile:\n \n#Build an image\n$ docker build -t [username/]<image-name>[:tag] <dockerfile-path>\n \n#Build an image called myimage using the Dockerfile in the same folder where the command was executed\n$ docker build -t myimage:latest .\n3: List the images:\n$ docker images\n4: Remove an image from the local registry:\n$ docker rmi [username/]<image-name>[:tag]\n2. Check the history of an image:\n \n# Check the history of the jboss/wildfly image\n$ docker history jboss/wildfly\n \n# Check the history of an image\n$ docker history [username/]<image-name>[:tag]\ndocker [CMD] [OPTS] [IMAGE]",
          "char_count": 1371,
          "ocr_used": false
        },
        {
          "page": 5,
          "text": "build\nhistory\nimages\nimport\ninfo\ninspect \nload\npull\npush\nrmi\nsave\nsearch\ntag\nconnect\ncreate\ndisconnect\ninspect\nls\nrm \nBuild images from a Dockerfile\nShow the history of an image\nList images\nCreate an empty filesystem image and import the contents of the\ntarball into it\nDisplay system-wide information\nReturn low-level information on a container or image\nLoad an image from a tar archive or STDIN\nPull an image or a repository from the registry\nPush an image or a repository to the registry\nRemove one or more images\nSave one or more images to a tar archive \n(streamed to STDOUT by default)\nSearch one or more configured container registries for images\nTag an image into a repository\nConnects a container to a network\nCreates a new network with the specified name\nDisconnects a container from a network\nDisplays detailed information on a network\nLists all the networks created by the user\nDeletes one or more networks\n1.3 Network related commands\ndocker network [CMD] [OPTS]\nCommand\nDescription\nCommand\nDescription",
          "char_count": 1015,
          "ocr_used": false
        },
        {
          "page": 6,
          "text": "login\nlogout\ncreate\ninspect\nls\nrm\nevents\ninspect\nLog in to a container registry server. If no server is specified then \ndefault is used\nLog out from a container registry server. If no server is specified \nthen default is used\nCreate a volume\nReturn low-level information on a volume\nLists volumes\nRemove a volume\nGet real time events from the server\nShow version information\n1.4 Network related commands\n1.5 Volume related commands\n1.6 Related commands\nDefault is https://index.docker.io/v1/\ndocker volume [CMD] [OPTS]\n2. Dockerfile\nThe Dockerfile provides the instructions to build a container image through the \n`docker build -t [username/]<image-name>[:tag] <dockerfile-path>`\ncommand. It starts from a previously existing Base image (through the FROM clause) \nfollowed by any other needed Dockerfile instructions. \nThis process is very similar to a compilation of a source code into a binary output, but in \nthis case the output of the Dockerfile will be a container image.\nExample Dockerfile\nThis example creates a custom WildFly container with a custom administrative user. It also \nexposes the administrative port 9990 and binds the administrative interface publicly through \nthe parameter ‘bmanagement’.\n# Use the existing WildFly image\nFROM jboss/wildfly\n# Add an administrative user\nRUN /opt/jboss/wildfly/bin/add-user.sh admin Admin#70365 --silent\n#Expose the administrative port\nEXPOSE 8080 9990\n#Bind the WildFly management to all IP addresses\nCMD [“/opt/jboss/wildfly/bin/standalong.sh”, “-b”, “0.0.0.0”, \n“-bmanagement”, “0.0.0.0”]\nCommand\nDescription\nCommand\nDescription\nCommand\nDescription",
          "char_count": 1607,
          "ocr_used": false
        },
        {
          "page": 7,
          "text": "# Build the WildFly image\n$ docker build -t mywildfly .\n \n#Run a WildFly server\n$ docker run -it -p 8080:8080 -p 9990:9990 mywildfly\n \n#Access the WildFly administrative console and log in with the credentials admin/Admin#70635\nopen http://<docker-daemon-ip>:9990 in a browser\nUsing the example Dockerfile\nFROM\nMAINTAINER\nRUN\nCMD\nLABEL\nEXPOSE\nENV\nADD\nCOPY\nENTRYPOINT\nVOLUME\nUSER\nWORKDIR\nARG\nONBUILD\nSTOPSIGNAL\nSets the base image for subsequent\nSets the author field of the generated images\nExecute commands in a new layer on top of the current image and \ncommit the results\nAllowed only once (if many then last one takes effect)\nAdds metadata to an image\nInforms container runtime that the container listens on the speci­\nfied network ports at runtime\nSets an environment variable\nCopy new files, directories, or remote file URLs from into the \nfilesystem of the container\nCopy new files or directories into the filesystem of the container\nAllows you to configure a container that will run as an executable\nCreates a mount point and marks it as holding externally mounted \nvolumes from native host or other containers\nSets the username or UID to use when running the image\nSets the working directory for any RUN, CMD, ENTRYPOINT, COPY, \nand ADD commands\nDefines a variable that users can pass at build-time to the builder \nusing --build-arg\nAdds an instruction to be executed later, when the image is used \nas the base for another build\nSets the system call signal that will be sent to the container to exit\nDockerfile instruction arguments\nCommand\nDescription",
          "char_count": 1564,
          "ocr_used": false
        },
        {
          "page": 8,
          "text": "$ mkdir -p www/\n$ echo “Server is up” > www/index.html\n$ docker run -d \\\n  -p 8000:8000 \\\n  --name=pythonweb \\\n  -v `pwd`/www:/var/www/html \\\n  -w /var/www/html \\\n  rhel7/rhel \\\n  /bin/python \\\n  -m SimpleHTTPServer 8000 \n$ curl <container-daemon-ip>:8000\n$ docker ps\n$ docker inspect pythonweb | less\n$ docker exec -it pythonweb bash\nExample: Running a web server container\n# Create a directory (if it doesn’t already exist)\n# Make a text file to serve later\n# Run process in a container as a daemon\n# Map port 8000 in container to 8000 on host\n# Name the container “pythonweb”\n# Map container html to host www directory\n# Set working directory to /var/www/html\n# Choose the rhel7/rhel directory\n# Run the Python command for\n  a simple web server listening to port 8000\n# Check that the server is working\n# See that the container is running\n# Inspect the container\n# Open the running container and look inside\nAbout the authors\nBachir Chihani, Ph.D. holds an engineering degree from Ecole \nSuperieure d’Informatique (Algeria) as well as a PhD degree in \nComputer Science from Telecom SudParis (France). Bachir has \nworked as a data engineer, software engineer, and research \nengineer for many years. Previously, he worked as a network \nengineer and got a CCNA Cisco-certification. Bachir has been \nprogramming for many years in Scala/Spark, Java EE, Android \nand Go. He has a keen interest in Open Source technologies \nparticularly in the fields of Automation, Distributed Computing \nand Software/System Design and he likes sharing his experience \nthrough blogging. \nBachir authored many research papers in the field of Context-\nAwareness and reviewed many papers for International \nconferences. He also served as a technical reviewer for many \nbooks including Spring Boot in Action (Manning, 2016) and Unified \nLog Processing (Manning, 2016).\nRafael Benevides is a Director of Developer Experience at Red \nHat. In his current role he helps developers worldwide to be more \neffective in software development, and he also promotes tools \nand practices that help them to be more productive. He worked \nin several fields including application architecture and design. \nBesides that, he is a member of Apache DeltaSpike PMC - a Duke’s \nChoice Award winner project. And a speaker in conferences like \nJUDCon, TDC, JavaOne and Devoxx\nTwitter: @rafabene\nLinkdeIn: https://www.linkedin.com/in/rafaelbenevides\nwww.rafabene.com.",
          "char_count": 2420,
          "ocr_used": false
        }
      ],
      "metadata": {
        "format": "PDF 1.4",
        "title": "",
        "author": "",
        "subject": "",
        "keywords": "",
        "creator": "Adobe InDesign CC 2015 (Macintosh)",
        "producer": "Adobe PDF Library 15.0",
        "creationDate": "D:20160804114156-05'00'",
        "modDate": "D:20160804114156-05'00'",
        "trapped": "",
        "encryption": null
      },
      "char_count": 12201,
      "word_count": 1900,
      "ocr_pages_count": 0,
      "error": null,
      "file_id": "1thQ_EPYjlfTY9ImECNquClaSBAvpUOBv",
      "filename": "Docker.pdf",
      "filepath": "downloads/Docker.pdf",
      "download_link": "https://drive.google.com/uc?export=download&id=1thQ_EPYjlfTY9ImECNquClaSBAvpUOBv"
    },
    {
      "success": true,
      "text": "https://www.linkedin.com/in/karun-karthik\nDynamic Programming - 1\nContents \n0. Introduction \n1. Climbing Stairs\n2. Fibonacci Number\n3. Min Cost Climbing Stairs\n4. House Robber\n5. House Robber - II\n6. Nth Tribonacci Number\n7. 0-1 Knapsack\n8. Partition Equal Subset Sum\n9. Target Sum\n10. Count no of Subsets with given Difference\n11. Delete and Earn\n12. Knapsack with Duplicate Items\n13. Coin Change - II\n14. Coin Change\n15. Rod cutting\n- Karun Karthik\nhttps://www.linkedin.com/in/karun-karthik\nDynamic programming is a technique to solve problems by breaking it down into a colle\nction of sub-problems, solving each of those sub-problems just once and storing these \nsolutions inside the cache memory in case the same problem occurs the next time. \nDynamic Programming is mainly an optimization over plain recursion . \nWherever we see a recursive solution that has repeated calls for same inputs, we can o\nptimize it using Dynamic Programming. \nThis simple optimization reduces the time complexities from exponential to polynomial.\nThere are two different ways to store our values so that they can be reused at a later i\nnstance. They are as follows:\n1. Memoization or the Top Down Approach.\n2. Tabulation or the Bottom Up approach.\nIn Memoization we start from the extreme state and compute result by using values th\nat can reach the destination state i.e the base state.\nIn Tabulation we start from the base state and then compute results all the way till the\n extreme state.\nNote: To store the intermediate results we can use Array, Matrix, Hashmap etc., all we \nneed is data storage and retrieval with a specific key.\nHow to find the use case of Dynamic Programming?\nYou can use DP if the problem can be,\n1. Divided into sub-problems\n2. Solved using a recursive solution \n3. Containing repetitive sub-problems\nhttps://www.linkedin.com/in/karun-karthik\n\nCD) Uirbaing tous -» 4yorn a. valu 'N\", sth the number ¢ ways to\nAroch N & Gumps posible ar ONG ot two.\nGq Nod > 0 a5 so spr Ne2 x\nOo —>R Loe howe 2 voouys (\ni)\n\\ \\ \\\nn=3 > 0 —>l—s2— > 3\no 4,243 ser N=3 3\nwe have 3 wos z\n9 252-53 :\ni)\nn=¥Y\nS Yor aug atDud cs\nte f 2 => C$+2\nMaw yer [cs+i | [cs+2|\ncS\n31a> Sway . _\nif seen\nLo - ale 2 Sulu |\n% On ’ “on FX ipcsan\ngx : y e0* I cn 0\ny\nNe f\n. (2) Oa 5 ont ie nay\nwe ole » ts 1—> 24\nLy, 253 Yy\n( 5w o—\noS) \\C ey yg ay\n) 9 32 —>4\nator wante OO |\nthe subgrroblen wv) burg done multiple hm, pr con solve wang\nhttps://www.linkedin.com/in/karun-karthik\nhttps://www.linkedin.com/in/karun-karthik\n\nOe,\n{\ntotalWays( currentStair, targetStair, < i. > &memo){\nif(currentStair==targetStair){\nreturn 1;\n}\nif(currentStair > targetStair){\nreturn 0;\n}\ncurrentKey = currentStair;\nif (memo. find(currentKey) !=memo.end()){\nreturn memo[currentKey];\n}\noneStep = totalWays(currentStair+1, targetStair, memo);\ntwoStep = totalWays(currentStair+2, targetStair, memo);\nmemo[currentKey] = oneStep+twoStep;\nreturn oneStep+twoStep;\n}\nclimbStairs( n) {\nunordered_map< . > memo;\nreturn totalWays(@,n,memo) ;\n}\n}3\nhttps://www.linkedin.com/in/karun-karthik\nhttps://www.linkedin.com/in/karun-karthik\n\n@) Hhonacci Numb cn) = (n-t) +¢n-2) | TH=0\n{O21\n- id :\nCode\n—_—\nee\nSolution {\nhelper(int n, unordered_map<int, int>&memo){\n(n<=1){\nn;\n}\nt currentKey = n;\n(memo. find(currentKey) !=memo.end()){\nmemo[currentKey ] ;\n}\nt a = helper(n-1,memo) ;\nt b = helper(n-2,memo) ;\nmemo[currentKey] = a+b;\nmemo[currentKey ] ;\n}\nt fib(int n) {\nunordered_map<int, int>memo;\nhelper(n,memo) ;\n}\n35\nhttps://www.linkedin.com/in/karun-karthik\nhttps://www.linkedin.com/in/karun-karthik\n\nfiner Crs OMY find rnin cost fo Seach the end Ataating {vom\not makug 1 at ,\n1 & akung qemrs at\n| au\n“ws f__ xa _ 7\nae — x+(Cr41| x+ [222]\nC1\nGy cost = [1,1S, 20] min (2S, 30) <\na O a 2 3 “ (2s, as.\nvmin( 35'S) = is vA VA SS mun (20,20) 20\noF 2,\nwl 2 6 A V5 = VAS\nmin 2040, 20 t00) Ay 2° ‘eP> O ent\nCO, C) GS\n6 20\njF\n7 ow C1>3\nos (LoS\ngun ro) won ad]\n“Atoshing ob \\ min (38 25) = 95),\nee NL CQ 's\\ .\n+\nmin (Pi) 1S eS)\n“mun (tS, 2 = tS\nos Ae oO C1L>3\nquan © mun 0\nhttps://www.linkedin.com/in/karun-karthik\nhttps://www.linkedin.com/in/karun-karthik\n\n————\ni Solution {\nt minCost(vector<int>&cost, t currentIndex, unordered_map< P > &m){\n(currentIndex == cost.size()){\n9;\n}\n(currentIndex > cost.size()){\n1000;\n}\n(m.find(currentIndex) !=m.end()){\nm[currentIndex];\ni\ni oneJump = cost[currentIndex] + minCost(cost,currentIndex+1, m);\nint twoJump = cost[currentIndex] + minCost(cost,currentIndex+2, m);\nm[currentIndex] = min(oneJump, twoJump) ;\nm[currentIndex];\n}\nt minCostClimbingStairs(vector<int>& cost) {\nunordered_map< 5 > m;\nmin( minCost(cost,®,m), minCost(cost,1,m));\n}\n33\nhttps://www.linkedin.com/in/karun-karthik\nhttps://www.linkedin.com/in/karun-karthik\n\nHouse Robber . ;\nfind max amount, that can be mobbed umlhout choosing ths.\nay ace Apusn -\n- = IQ\n= c (2ay O02 4\nto awoid odyacut\n— 4 rbba ror howe , thin amounk = numu[cL] q Cl=Cctt2\nud CIl-ecrt+1\nnob\nKegan mony tech (SE no 0\nrye ALUMNA = _ LN L Via\nrama (ec) + (C142 CH\n& [a,4,3, 6 J\n0123\n23 max (8, )e 10 —\nare (0) 10 —\nmaxQQ=b ~/ y max (10,6) +70\nRy CS x roan 3,4) = 6\n3 6\nJ e (3) (2) max(G O= 6\nf(y) (3) Ae ~\\3 6\neM / fey ; + A So\nys\nrutawn O (oz)\n% ood it's vole +0 the num\niy Avatid , de conbrure-\nhttps://www.linkedin.com/in/karun-karthik\nhttps://www.linkedin.com/in/karun-karthik\n\n— - >»\nclass Solution {\npublic:\nint helper(vector<int>&nums, int currentIndex, unordered_map<int, int>&m){\nif(currentIndex >= nums.size()){\nreturn @;\n}\nint currentKey = currentIndex;\nif (m.find(currentKey) !=m.end()){\nreturn m[currentKey];\n}\nint rob = nums[currentKey] + helper(nums, currentIndex+2, m);\nint noRob = helper(nums, currentIndex+1, m);\nm[currentIndex] = max(rob, noRob);\nreturn m[currentIndex];\n}\nint rob(vector<int>& nums) {\nunordered_map<int,int> m;\nreturn helper (nums,@,m) ;\n}\n}3\nhttps://www.linkedin.com/in/karun-karthik\nhttps://www.linkedin.com/in/karun-karthik\n\nto Ww\nuse Robber - _,\n—- Ss =\ndn thu pooblem , te approach urdf be Aimifar fo PupUs ONL,\nbut te Ads ar in code, whith meant thot\n.\nx we Stout fom (sr house, thin we cont wb the\nLot hourr.\n. nd the\nx4 we stot fom 2 hour , thy we con Nob\nLou hour.\nclur betwen ho 24 hou\nx and tN mox vi IT hour y\n| Only | houde 1 Put , then Hod uw dinschty.\nCod.\n_—_-\nclass Soluti {\npublic\nint helper(vector<int>&nums, int currentIndex, int lastIndex, unordered_map<int, int>&m){\nif(currentIndex > lastIndex){\nreturn Q;\n}\nint currentKey = currentIndex;\nif(m.find(currentKey) !=m.end()){\nreturn m[currentkey];\n}\nint rob = nums[currentKey] + helper(nums, currentIndex+2, lastIndex, m);\nint noRob = helper(nums, currentIndex+1, lastIndex, m);\nm[currentIndex] = max(rob, noRob);\nreturn m[currentIndex];\n}\nint rob(vector<int>& nums) {\nint n = nums.size();\nif(n==1) return nums[@];\nunordered_map<int, int> memo1,memo2;\nint firstHouse = helper(nums, @, n-2, memo1);\nint secondHouse = helper(nums, 1, n-1, memo2);\nreturn max(firstHouse, secondHouse) ;\n}\n}5\nhttps://www.linkedin.com/in/karun-karthik\nhttps://www.linkedin.com/in/karun-karthik\n\nN-th Tmbonacy . .\n(6) Net imponac Yuen n, pnd Tn\nTreg = Tt Init ez 4 N20 T= 0» Te UTED\nc 4 (atte!)\nhz\na ey ON\n~ \\ i)\nSe\n( s d 2\n(2) (©) ae (0)\n—\nee\ni) Solution {\nt helper( n, unordered_map<int,int> &m){\n(n<=1){\nn5\n}\n(n==2){\n1;\n}\nint currentNum = n;\n(m.find(currentNum) !=m.end()){\nm[currentNum] ;\n}\nt a = helper(n-1,m);\nt b = helper(n-2,m);\nint c = helper(n-3,m);\nm[currentNum] = a+b+c;\nm[currentNum] ;\n}\nt tribonacci(int n) {\nunordered_map<int, int>m;\nhelper(n,m);\n}\n}3\nhttps://www.linkedin.com/in/karun-karthik\nhttps://www.linkedin.com/in/karun-karthik\n\n&® O~ 1 Knapsack Foblen apnd Max profit uch thay th,\ne123 ‘ a tema <5 ausry. .\nwots [34.6,5] W) UGhE & Cap ony\npoops = [a,3,1,4| — if we Adee 0fF3,\ncapocity = thn fara unghrs we Lode 2]\nEg § profits are Reus Thost max profit powble\no1, nu\nWe~[le3 capacity = (a, 9)\nPrpre[4,5, 0] ino (4,8) 4 Z x\nv C 5\nuv, 3 Lt x\nG \\\n51/22 BY Jo.\n5+ 3\nBS\n\\ ! 3,4\n3,2) Coe 6\n.\nJ. Ok wry ALP -\n+ at 4 decking an yndex thn uduce cop aul by wt [eL |\n— if not a indurunt C1 by !\n_s tind max (Lut, suaht)\nhttps://www.linkedin.com/in/karun-karthik\nhttps://www.linkedin.com/in/karun-karthik\n\ncode,\n—— >\n{\nhelper ( W, wt[], val[], n, curr,\n< e > &memo){\nif(curr==n) return 0;\nstring currkey = to_string(curr)+\"_\"+to_string(W) ;\nif (memo. find(currKkey) !=memo.end()) return memo[currKey];\ncurrWt = wt[curr]\ncurrVal = val[curr];\nselected = 0;\nif (currwWt<=W){\nselected = currVal + helper(W-currwWt, wt, val, n, curr+1, memo);\n}\nnotSelected = helper(W, wt, val, n, curr+1, memo);\nmemo[currKey] = max(selected, notSelected) ;\nreturn memo[currKey] ;\n}\nknapSack( W, wt[], val[], n)\n{\nunordered_map<string, > memo;\nreturn helper(W, wt, val, n, @, memo);\n}\n}5\nhttps://www.linkedin.com/in/karun-karthik\nhttps://www.linkedin.com/in/karun-karthik\n\nPartition Equel subst sun\na _\ndiner an ouroy. find {ik can be dred any two Aubduts\nathgAn sum ia 2quad.\nCy numa = [1,5 11,5] con be dirdad aunty 81=4),8,54 § 827810\n4 Aum & Al == Aum y gf. MU Pure .\nDi Aum us odd then weGun False\na4 hum wv wen) thin quo cued.\n—> and a Aubsde uyhose vou = = Swn/g\nWhich muon thar the oth, Aubset wil hom\nvolut == Aum/p .\n— ts Au tgs dum/a (+48 4 Tange sum)\n> At andex,, we hove & Chowss\nd yp ana irdlaes\n1) Que rut thn tse t3—nums[(cL |\n%\nCT= Cit!\nx) x we donor suur thn ts = ts Cit rumauns\nctecs1 40\n3) nln OR D Lift 4 raght branch .\nhttps://www.linkedin.com/in/karun-karthik\nhttps://www.linkedin.com/in/karun-karthik\n\neo:1 2 3\n=> numa = [1 i) 1,5]\nAumz 2&\nHur 4um 72 ==50\nlund ) int? 2 Aubsett wv prssible .\nExplananon\now T\nGp Teve\nx\n(11-1 =!©) ~y -\n(10-529), A, x\nNo accept\nBranch os\n(5-5: 0) ‘\nt$=aco\n2 thay Auber uv yound\n4% retin Thue\nQh we Ow wing oR, ont Trur bronth 4 sfiuart\nhttps://www.linkedin.com/in/karun-karthik\nhttps://www.linkedin.com/in/karun-karthik\n\n—_ >\n{\nisPossible( targetSum, currentIndex, < >&nums ,\n< s > &memo){\n\nif(targetSum == @)\n\nreturn >\nif(currentIndex >= nums.size())\n\nreturn H\nstring currentKey = to_string(currentIndex)+\"_\"“+to_string(targetSum) ;\nif (memo. find(currentKey) !=memo.end()){\n\nreturn memo[currentKey];\n}\n\npossible = :\nif(nums[currentIndex]<=targetSum)\n\npossible = isPossible(targetSum-nums[currentIndex], currentIndex+1, nums, memo);\nif (possible) {\n\nmemo[currentKey] = possible;\n\nreturn :\n}\n\nnotPossible = isPossible(targetSum, currentIndex+1, nums, memo);\nmemo[currentKey] = possible||notPossible;\nreturn memo[currentKey] ;\n\n}\n\ncanPartition( < >& nums) {\n\ntotal = 9;\nfor( it:nums) total+= it;\nif(total%2!=0) return Ff\nunordered_map<string, > memo;\nreturn isPossible(total/2,0, nums,memo) ;\n\n}\ni\nhttps://www.linkedin.com/in/karun-karthik\nhttps://www.linkedin.com/in/karun-karthik\n\n(4) Target hum _,\nyi an any § fort , nd the tumbu ¢ woyd to Meath Forge\nby using + & — before each Unmet m onnay.\n6 nput: nums = [1,1,1,1,1], target =\n= ote {1,1,1,1,1], target = 3\nExplanation: There are 5 ways to assign symbols to make the sum\nof nums be target 3.\n—1+1+1+1+1=3\n41 -1+1+1+12=3\n+1+4+1-1+1+1=3\n+1+1+1-1+1=3\n41+1+1+1-1=3\n— a may udbs ae Can um + ot — Sign\n4 +tnt=t -G nums(¢I]) => + —numg(cr]\n4 — Tun t 2% -(-nume(¢r]) => +t +nums(cr]\nas os\n> OF enw node, AuTi Me fur VOL Yeon\nLge mphr Beco we nual 1 Thind the\ntotal numbu \"% ari\nhttps://www.linkedin.com/in/karun-karthik\nhttps://www.linkedin.com/in/karun-karthik\n\n_—_—\nee\nSolution {\ntotalWays( currentIndex, vector< >&nums, target, unordered_map<string, > &memo){\n(target==0 and currentIndex==nums.size()){\n1;\n}\n(currentIndex>=nums.size() and target!=0){\n®;\n}\nstring key = to_string(currentIndex)+ +to_string(target) ;\n(memo. find(key) !=memo.end()){\nmemo[key];\n}\nplus = totalWays(currentIndex+1, nums, target-nums[currentIndex],memo) ;\nminus = totalWays(currentIndex+1, nums, target+nums[currentIndex],memo) ;\nmemo[key] = plus+minus;\nplus+minus;\n}\nfindTargetSumWays(vector< >& nums, target) {\nunordered_map<string, > memo;\ntotalWays(@,nums, target, memo) ;\n}\n}5\nhttps://www.linkedin.com/in/karun-karthik\nhttps://www.linkedin.com/in/karun-karthik\n\nCounr mumber ae Aub wsith given dieu, —\n+ Tha uv Aimila to Target am.\nYon the dapperncr puwen fwo Aub, and an anny\nfind 70. 4 Aubses uoith the diffeane.\nApproach\nLe fay Al - Sp = Ueffueree (porn) —_ a)\nwe con calorlola AUN § wy dharma doa 20m\n4 can be Aud that yor a dubs Ary de\nAi+ Adc Aum ._— ©)\nnow O12) » gd) = difpowot Aun\nfie (diguuna + tum) /2.\nas dmplenut Toxger dum with —Torger value. = £I\n———————— =\nhttps://www.linkedin.com/in/karun-karthik\nhttps://www.linkedin.com/in/karun-karthik\n\n@) Dek and Eon\nYou are given an integer array nums . You want to maximize the number of points\nyou get by performing the following operation any number of times:\ne Pick any nums[i} and delete it to earn nums[i} points. Afterwards, you\nmust delete every element equal to nums{i] - 1 and every element\nequal to nums[i] + 1.\nReturn the maximum number of points you can earn by applying the above\noperation some number of times.\nEg num = (a, 2,3,3,3,4]\n> we Ata dudahing Dt tw -arra aX\nthn ruumy = [3,3,3, 4]\nwe nud fo delet al! Atl YAH => mums = Cu]\n> ue dilete 4 then pow = 4+ = B\n(or)\n. ; = =@\n= we Arour dkehng 3, thn sulk = 3+343 =\nthn num = [2a,2,4]\n=> mum =\n% we nud dude all 3-1 Bt > Ud\n(or)\n> Y we Ata duking Up then Miuly = &\nthn numu-:[a,2,3,3,3)\nwe purd fo delet olf Yt YY => num = C2, 2]\nwe duly 2 then por = 4H = ZB\nhttps://www.linkedin.com/in/karun-karthik\nhttps://www.linkedin.com/in/karun-karthik\n\nwre the Similar approach\nnuma = [a,a, 3, 3,3, 4 - fofo}a} ali)\n7? ? 2 2-9\no 6 2 3 y\nC1+1\nCL* qua (cr} +\\CI+2\n(ods =\nee\nSolution {\nmaxPoints(vector<int>& freq, currentIndex, unordered_map<int, int>&memo){\n(currentIndex >= freq.size()) Q;\nkey = currentIndex;\n(memo.find(key) != memo.end()) memo[key] ;\nDelete = currentIndex*freq[currentIndex] + maxPoints(freq, currentIndex+2, memo) ;\nNotDelete = maxPoints(freq, currentIndex+1, memo) ;\nmemo[key] = max(Delete, NotDelete) ;\nmemo[key ];\n}\ndeleteAndEarn(vector<int>& nums) {\nmaxi = *max_element(nums.begin(), nums.end());\nvector<int> freq(maxi+1, @);\n( i: nums) freq[i]++;\nunordered_map< 7 > memo;\nmaxPoints(freq, @, memo);\n}\n35\nhttps://www.linkedin.com/in/karun-karthik\nhttps://www.linkedin.com/in/karun-karthik\n\nimulou to 0-1 Knapdack bul allows\n(12) Unbounoid Knapdack. Aimilos oP\nwa & chorde On Umm more than One\n(an\nwr = [2,1] , than Fans\n; it boundid Knopdack propre &\nvou - [1,1]\nit Lnboundud knapsack then profit = 3\nCapaury =3 Rl tie\nfor unboundsol Knapsack (tp)\nxX\n_\nee\nSolution{\nhelper ( W, wt[], val[], N, curr, vector<vector< >>&memo) {\n(W==@) 8;\n(curr==N) Q;\n(memo[curr][W]!=-1) memo[curr][W];\ncurrWt = wt[curr];\ncurrVal = val[curr];\nt selected = @;\n(currWt<=W){\nselected = currVal + helper(W-currWt, wt, val, N, curr, memo);\n}\nnotSelected = helper(W, wt, val, N, curr+1, memo);\nmemo[curr][W] = max(selected, notSelected) ;\nmemo[curr][W];\n}\nt knapSack( N, tw, t val[], t wt[])\n{\nvector<vector< >> memo( N , vector<int> (W+1, -1));\nhelper(W, wt, val, N, @, memo);\n}\ni)\nhttps://www.linkedin.com/in/karun-karthik\nhttps://www.linkedin.com/in/karun-karthik\n\na .\n(3) Gin Chany 2 ( Binidas & unbounded Knapdaek )\nw\n——“~—-Y ——_ -\nLyasen an aunay 3 Com % Omount, tind toto} numba & ways ]\nae amaks that Omount .\nLombinahons 0 up\n. lritititl\nEg wim = [1,2,5] Lelth42\nl+242\namounk = 5\n5\ninc x .\nCoins [ __*L_ _l X= comm [ec]\n+ Cl,\nCA, 0, amount CO, O&\nCC, a-x ?\noda\n_—_—\nclass Solution {\npublic:\nint totalWays(int currentIndex, vector<int>& coins, int amount, vector<vector<int>>&memo) {\nif(amount == @) return 1;\nif(currentIndex >= coins.size()) return @;\nif (memo[currentIndex][amount]!=-1) return memo[currentIndex] [amount];\nint consider = 0;\nif (coins[currentIndex]<=amount) {\nconsider = totalWays(currentIndex, coins, amount-coins[currentIndex],memo) ;\n}\nint notConsider = totalWays(currentIndex+1, coins, amount, memo) ;\nmemo[currentIndex][amount] = consider+notConsider;\nreturn memo[currentIndex] [amount] ;\n}\nint change(int amount,vector<int>& coins) {\nvector<vector<int>>memo(coins.size()+1,vector<int>(amount+1, -1));\nreturn totalWays(@,coins, amount, memo) ;\n}\nis\nhttps://www.linkedin.com/in/karun-karthik\nhttps://www.linkedin.com/in/karun-karthik\n\n. im f unbounded Knapiack \\\n(14) Coin Changs C Bimidos appa,\nJ -\n\n; . . .\nLiven an oboy TE Wonk % amount» tind Just numba @ come\n40 maks Up that amount, run — 4 uw not possible\n\nute\n——>\n= for > fe. MM\nEg coins = [1,2,8])\n142424242422 171\namount = | :\nl45+S = C1\nwins [ x l\na X= cotm [ec\nr [ec] Cl,\n2 &, anu CoH, &\n[+ cc, Qa-xX +H,\ncode —>\nee\nSolution {\nminimumCoins( currentIndex, vector<int>& coins, int amount, vector<vector<int>>&memo){\n(amount == @) e;\n(currentIndex >= coins.size()) 102000;\n(memo[currentIndex] [amount] !=-1) memo[currentIndex] [amount] ;\nconsider = 100000;\n(coins[currentIndex]<=amount) {\nconsider = 1 + minimumCoins(currentIndex, coins, amount-coins[currentIndex], memo) ;\ni\nnotConsider = minimumCoins(currentIndex+1, coins, amount, memo);\nmemo[currentIndex] [amount ]= min(consider ,notConsider) ;\nmemo[currentIndex] [amount] ;\n}\ni coinChange(vector<int>& coins, i amount) {\nvector<vector< >>memo(coins.size()+1,vector< >(amount+1, -1));\nans = minimumCoins(®@, coins, amount, memo);\n(ans==100000)? -1 : ans;\n}\n}5\nhttps://www.linkedin.com/in/karun-karthik\nhttps://www.linkedin.com/in/karun-karthik\n\n] Red Lustig\n— 77>\nY6 a hod Q Langfh N and amay ff prea. find the maa\nvalue that can be obfavud M4 Cutting nod\n1234 5 & 4\nbs N=¢ pric = [1,5,8,4, 10,1819, 20]\no (23 4 5 6 9\n# pviieg 6} oO piece jr prics [CT J , UsOSe Lingtn u Citi\na 6\nwf we wut Ow JOA intd A pieces t Length A,e we gee\nrox volun 4 5412 bb AA,\n— thur onight be othe wouyt but thir portion\nx gt any anatanty Length * eww pin & Cit!\nws, yz MO ut\nprias(et) +(cz, 4 (ex)\nfor\nhttps://www.linkedin.com/in/karun-karthik\nhttps://www.linkedin.com/in/karun-karthik\n\n_—_—\nee\nSolution{\nmaxProfit( price[], currentIndex, n, vector<vector< >>&memo) {\n(n==0) e;\n(currentIndex>=n) @;\n(memo[currentIndex][n]!=-1) memo[currentIndex][n];\nselected = 0;\n(currentIndex+1<=n) {\nselected = price[currentIndex]+maxProfit(price, currentIndex, n-(currentIndex+1), memo);\n}\nnotSelected = maxProfit(price, currentIndex+1, n, memo);\nmemo[currentIndex][n] = max(selected, notSelected) ;\nmemo[currentIndex][n];\n}\ncutRod( price[], n) {\nvector<vector< >> memo(n+1, vector< >(n+1,-1));\nmaxProfit(price,@,n,memo) ;\n}\n35\nhttps://www.linkedin.com/in/karun-karthik\nhttps://www.linkedin.com/in/karun-karthik\n\nFind the rest on\n\nhttps://linktr.ee/KarunKarthik\n\nFollow Karun Karthik For More Amazing Content !\nhttps://www.linkedin.com/in/karun-karthik",
      "page_count": 27,
      "pages": [
        {
          "page": 1,
          "text": "https://www.linkedin.com/in/karun-karthik\nDynamic Programming - 1\nContents \n0. Introduction \n1. Climbing Stairs\n2. Fibonacci Number\n3. Min Cost Climbing Stairs\n4. House Robber\n5. House Robber - II\n6. Nth Tribonacci Number\n7. 0-1 Knapsack\n8. Partition Equal Subset Sum\n9. Target Sum\n10. Count no of Subsets with given Difference\n11. Delete and Earn\n12. Knapsack with Duplicate Items\n13. Coin Change - II\n14. Coin Change\n15. Rod cutting\n- Karun Karthik",
          "char_count": 451,
          "ocr_used": false
        },
        {
          "page": 2,
          "text": "https://www.linkedin.com/in/karun-karthik\nDynamic programming is a technique to solve problems by breaking it down into a colle\nction of sub-problems, solving each of those sub-problems just once and storing these \nsolutions inside the cache memory in case the same problem occurs the next time. \nDynamic Programming is mainly an optimization over plain recursion . \nWherever we see a recursive solution that has repeated calls for same inputs, we can o\nptimize it using Dynamic Programming. \nThis simple optimization reduces the time complexities from exponential to polynomial.\nThere are two different ways to store our values so that they can be reused at a later i\nnstance. They are as follows:\n1. Memoization or the Top Down Approach.\n2. Tabulation or the Bottom Up approach.\nIn Memoization we start from the extreme state and compute result by using values th\nat can reach the destination state i.e the base state.\nIn Tabulation we start from the base state and then compute results all the way till the\n extreme state.\nNote: To store the intermediate results we can use Array, Matrix, Hashmap etc., all we \nneed is data storage and retrieval with a specific key.\nHow to find the use case of Dynamic Programming?\nYou can use DP if the problem can be,\n1. Divided into sub-problems\n2. Solved using a recursive solution \n3. Containing repetitive sub-problems",
          "char_count": 1362,
          "ocr_used": false
        },
        {
          "page": 3,
          "text": "https://www.linkedin.com/in/karun-karthik\n\nCD) Uirbaing tous -» 4yorn a. valu 'N\", sth the number ¢ ways to\nAroch N & Gumps posible ar ONG ot two.\nGq Nod > 0 a5 so spr Ne2 x\nOo —>R Loe howe 2 voouys (\ni)\n\\ \\ \\\nn=3 > 0 —>l—s2— > 3\no 4,243 ser N=3 3\nwe have 3 wos z\n9 252-53 :\ni)\nn=¥Y\nS Yor aug atDud cs\nte f 2 => C$+2\nMaw yer [cs+i | [cs+2|\ncS\n31a> Sway . _\nif seen\nLo - ale 2 Sulu |\n% On ’ “on FX ipcsan\ngx : y e0* I cn 0\ny\nNe f\n. (2) Oa 5 ont ie nay\nwe ole » ts 1—> 24\nLy, 253 Yy\n( 5w o—\noS) \\C ey yg ay\n) 9 32 —>4\nator wante OO |\nthe subgrroblen wv) burg done multiple hm, pr con solve wang\nhttps://www.linkedin.com/in/karun-karthik",
          "char_count": 635,
          "ocr_used": true,
          "original_char_count": 42
        },
        {
          "page": 4,
          "text": "https://www.linkedin.com/in/karun-karthik\n\nOe,\n{\ntotalWays( currentStair, targetStair, < i. > &memo){\nif(currentStair==targetStair){\nreturn 1;\n}\nif(currentStair > targetStair){\nreturn 0;\n}\ncurrentKey = currentStair;\nif (memo. find(currentKey) !=memo.end()){\nreturn memo[currentKey];\n}\noneStep = totalWays(currentStair+1, targetStair, memo);\ntwoStep = totalWays(currentStair+2, targetStair, memo);\nmemo[currentKey] = oneStep+twoStep;\nreturn oneStep+twoStep;\n}\nclimbStairs( n) {\nunordered_map< . > memo;\nreturn totalWays(@,n,memo) ;\n}\n}3\nhttps://www.linkedin.com/in/karun-karthik",
          "char_count": 578,
          "ocr_used": true,
          "original_char_count": 42
        },
        {
          "page": 5,
          "text": "https://www.linkedin.com/in/karun-karthik\n\n@) Hhonacci Numb cn) = (n-t) +¢n-2) | TH=0\n{O21\n- id :\nCode\n—_—\nee\nSolution {\nhelper(int n, unordered_map<int, int>&memo){\n(n<=1){\nn;\n}\nt currentKey = n;\n(memo. find(currentKey) !=memo.end()){\nmemo[currentKey ] ;\n}\nt a = helper(n-1,memo) ;\nt b = helper(n-2,memo) ;\nmemo[currentKey] = a+b;\nmemo[currentKey ] ;\n}\nt fib(int n) {\nunordered_map<int, int>memo;\nhelper(n,memo) ;\n}\n35\nhttps://www.linkedin.com/in/karun-karthik",
          "char_count": 462,
          "ocr_used": true,
          "original_char_count": 42
        },
        {
          "page": 6,
          "text": "https://www.linkedin.com/in/karun-karthik\n\nfiner Crs OMY find rnin cost fo Seach the end Ataating {vom\not makug 1 at ,\n1 & akung qemrs at\n| au\n“ws f__ xa _ 7\nae — x+(Cr41| x+ [222]\nC1\nGy cost = [1,1S, 20] min (2S, 30) <\na O a 2 3 “ (2s, as.\nvmin( 35'S) = is vA VA SS mun (20,20) 20\noF 2,\nwl 2 6 A V5 = VAS\nmin 2040, 20 t00) Ay 2° ‘eP> O ent\nCO, C) GS\n6 20\njF\n7 ow C1>3\nos (LoS\ngun ro) won ad]\n“Atoshing ob \\ min (38 25) = 95),\nee NL CQ 's\\ .\n+\nmin (Pi) 1S eS)\n“mun (tS, 2 = tS\nos Ae oO C1L>3\nquan © mun 0\nhttps://www.linkedin.com/in/karun-karthik",
          "char_count": 547,
          "ocr_used": true,
          "original_char_count": 42
        },
        {
          "page": 7,
          "text": "https://www.linkedin.com/in/karun-karthik\n\n————\ni Solution {\nt minCost(vector<int>&cost, t currentIndex, unordered_map< P > &m){\n(currentIndex == cost.size()){\n9;\n}\n(currentIndex > cost.size()){\n1000;\n}\n(m.find(currentIndex) !=m.end()){\nm[currentIndex];\ni\ni oneJump = cost[currentIndex] + minCost(cost,currentIndex+1, m);\nint twoJump = cost[currentIndex] + minCost(cost,currentIndex+2, m);\nm[currentIndex] = min(oneJump, twoJump) ;\nm[currentIndex];\n}\nt minCostClimbingStairs(vector<int>& cost) {\nunordered_map< 5 > m;\nmin( minCost(cost,®,m), minCost(cost,1,m));\n}\n33\nhttps://www.linkedin.com/in/karun-karthik",
          "char_count": 609,
          "ocr_used": true,
          "original_char_count": 42
        },
        {
          "page": 8,
          "text": "https://www.linkedin.com/in/karun-karthik\n\nHouse Robber . ;\nfind max amount, that can be mobbed umlhout choosing ths.\nay ace Apusn -\n- = IQ\n= c (2ay O02 4\nto awoid odyacut\n— 4 rbba ror howe , thin amounk = numu[cL] q Cl=Cctt2\nud CIl-ecrt+1\nnob\nKegan mony tech (SE no 0\nrye ALUMNA = _ LN L Via\nrama (ec) + (C142 CH\n& [a,4,3, 6 J\n0123\n23 max (8, )e 10 —\nare (0) 10 —\nmaxQQ=b ~/ y max (10,6) +70\nRy CS x roan 3,4) = 6\n3 6\nJ e (3) (2) max(G O= 6\nf(y) (3) Ae ~\\3 6\neM / fey ; + A So\nys\nrutawn O (oz)\n% ood it's vole +0 the num\niy Avatid , de conbrure-\nhttps://www.linkedin.com/in/karun-karthik",
          "char_count": 589,
          "ocr_used": true,
          "original_char_count": 42
        },
        {
          "page": 9,
          "text": "https://www.linkedin.com/in/karun-karthik\n\n— - >»\nclass Solution {\npublic:\nint helper(vector<int>&nums, int currentIndex, unordered_map<int, int>&m){\nif(currentIndex >= nums.size()){\nreturn @;\n}\nint currentKey = currentIndex;\nif (m.find(currentKey) !=m.end()){\nreturn m[currentKey];\n}\nint rob = nums[currentKey] + helper(nums, currentIndex+2, m);\nint noRob = helper(nums, currentIndex+1, m);\nm[currentIndex] = max(rob, noRob);\nreturn m[currentIndex];\n}\nint rob(vector<int>& nums) {\nunordered_map<int,int> m;\nreturn helper (nums,@,m) ;\n}\n}3\nhttps://www.linkedin.com/in/karun-karthik",
          "char_count": 582,
          "ocr_used": true,
          "original_char_count": 42
        },
        {
          "page": 10,
          "text": "https://www.linkedin.com/in/karun-karthik\n\nto Ww\nuse Robber - _,\n—- Ss =\ndn thu pooblem , te approach urdf be Aimifar fo PupUs ONL,\nbut te Ads ar in code, whith meant thot\n.\nx we Stout fom (sr house, thin we cont wb the\nLot hourr.\n. nd the\nx4 we stot fom 2 hour , thy we con Nob\nLou hour.\nclur betwen ho 24 hou\nx and tN mox vi IT hour y\n| Only | houde 1 Put , then Hod uw dinschty.\nCod.\n_—_-\nclass Soluti {\npublic\nint helper(vector<int>&nums, int currentIndex, int lastIndex, unordered_map<int, int>&m){\nif(currentIndex > lastIndex){\nreturn Q;\n}\nint currentKey = currentIndex;\nif(m.find(currentKey) !=m.end()){\nreturn m[currentkey];\n}\nint rob = nums[currentKey] + helper(nums, currentIndex+2, lastIndex, m);\nint noRob = helper(nums, currentIndex+1, lastIndex, m);\nm[currentIndex] = max(rob, noRob);\nreturn m[currentIndex];\n}\nint rob(vector<int>& nums) {\nint n = nums.size();\nif(n==1) return nums[@];\nunordered_map<int, int> memo1,memo2;\nint firstHouse = helper(nums, @, n-2, memo1);\nint secondHouse = helper(nums, 1, n-1, memo2);\nreturn max(firstHouse, secondHouse) ;\n}\n}5\nhttps://www.linkedin.com/in/karun-karthik",
          "char_count": 1115,
          "ocr_used": true,
          "original_char_count": 42
        },
        {
          "page": 11,
          "text": "https://www.linkedin.com/in/karun-karthik\n\nN-th Tmbonacy . .\n(6) Net imponac Yuen n, pnd Tn\nTreg = Tt Init ez 4 N20 T= 0» Te UTED\nc 4 (atte!)\nhz\na ey ON\n~ \\ i)\nSe\n( s d 2\n(2) (©) ae (0)\n—\nee\ni) Solution {\nt helper( n, unordered_map<int,int> &m){\n(n<=1){\nn5\n}\n(n==2){\n1;\n}\nint currentNum = n;\n(m.find(currentNum) !=m.end()){\nm[currentNum] ;\n}\nt a = helper(n-1,m);\nt b = helper(n-2,m);\nint c = helper(n-3,m);\nm[currentNum] = a+b+c;\nm[currentNum] ;\n}\nt tribonacci(int n) {\nunordered_map<int, int>m;\nhelper(n,m);\n}\n}3\nhttps://www.linkedin.com/in/karun-karthik",
          "char_count": 556,
          "ocr_used": true,
          "original_char_count": 42
        },
        {
          "page": 12,
          "text": "https://www.linkedin.com/in/karun-karthik\n\n&® O~ 1 Knapsack Foblen apnd Max profit uch thay th,\ne123 ‘ a tema <5 ausry. .\nwots [34.6,5] W) UGhE & Cap ony\npoops = [a,3,1,4| — if we Adee 0fF3,\ncapocity = thn fara unghrs we Lode 2]\nEg § profits are Reus Thost max profit powble\no1, nu\nWe~[le3 capacity = (a, 9)\nPrpre[4,5, 0] ino (4,8) 4 Z x\nv C 5\nuv, 3 Lt x\nG \\\n51/22 BY Jo.\n5+ 3\nBS\n\\ ! 3,4\n3,2) Coe 6\n.\nJ. Ok wry ALP -\n+ at 4 decking an yndex thn uduce cop aul by wt [eL |\n— if not a indurunt C1 by !\n_s tind max (Lut, suaht)\nhttps://www.linkedin.com/in/karun-karthik",
          "char_count": 566,
          "ocr_used": true,
          "original_char_count": 42
        },
        {
          "page": 13,
          "text": "https://www.linkedin.com/in/karun-karthik\n\ncode,\n—— >\n{\nhelper ( W, wt[], val[], n, curr,\n< e > &memo){\nif(curr==n) return 0;\nstring currkey = to_string(curr)+\"_\"+to_string(W) ;\nif (memo. find(currKkey) !=memo.end()) return memo[currKey];\ncurrWt = wt[curr]\ncurrVal = val[curr];\nselected = 0;\nif (currwWt<=W){\nselected = currVal + helper(W-currwWt, wt, val, n, curr+1, memo);\n}\nnotSelected = helper(W, wt, val, n, curr+1, memo);\nmemo[currKey] = max(selected, notSelected) ;\nreturn memo[currKey] ;\n}\nknapSack( W, wt[], val[], n)\n{\nunordered_map<string, > memo;\nreturn helper(W, wt, val, n, @, memo);\n}\n}5\nhttps://www.linkedin.com/in/karun-karthik",
          "char_count": 645,
          "ocr_used": true,
          "original_char_count": 42
        },
        {
          "page": 14,
          "text": "https://www.linkedin.com/in/karun-karthik\n\nPartition Equel subst sun\na _\ndiner an ouroy. find {ik can be dred any two Aubduts\nathgAn sum ia 2quad.\nCy numa = [1,5 11,5] con be dirdad aunty 81=4),8,54 § 827810\n4 Aum & Al == Aum y gf. MU Pure .\nDi Aum us odd then weGun False\na4 hum wv wen) thin quo cued.\n—> and a Aubsde uyhose vou = = Swn/g\nWhich muon thar the oth, Aubset wil hom\nvolut == Aum/p .\n— ts Au tgs dum/a (+48 4 Tange sum)\n> At andex,, we hove & Chowss\nd yp ana irdlaes\n1) Que rut thn tse t3—nums[(cL |\n%\nCT= Cit!\nx) x we donor suur thn ts = ts Cit rumauns\nctecs1 40\n3) nln OR D Lift 4 raght branch .\nhttps://www.linkedin.com/in/karun-karthik",
          "char_count": 653,
          "ocr_used": true,
          "original_char_count": 42
        },
        {
          "page": 15,
          "text": "https://www.linkedin.com/in/karun-karthik\n\neo:1 2 3\n=> numa = [1 i) 1,5]\nAumz 2&\nHur 4um 72 ==50\nlund ) int? 2 Aubsett wv prssible .\nExplananon\now T\nGp Teve\nx\n(11-1 =!©) ~y -\n(10-529), A, x\nNo accept\nBranch os\n(5-5: 0) ‘\nt$=aco\n2 thay Auber uv yound\n4% retin Thue\nQh we Ow wing oR, ont Trur bronth 4 sfiuart\nhttps://www.linkedin.com/in/karun-karthik",
          "char_count": 350,
          "ocr_used": true,
          "original_char_count": 42
        },
        {
          "page": 16,
          "text": "https://www.linkedin.com/in/karun-karthik\n\n—_ >\n{\nisPossible( targetSum, currentIndex, < >&nums ,\n< s > &memo){\n\nif(targetSum == @)\n\nreturn >\nif(currentIndex >= nums.size())\n\nreturn H\nstring currentKey = to_string(currentIndex)+\"_\"“+to_string(targetSum) ;\nif (memo. find(currentKey) !=memo.end()){\n\nreturn memo[currentKey];\n}\n\npossible = :\nif(nums[currentIndex]<=targetSum)\n\npossible = isPossible(targetSum-nums[currentIndex], currentIndex+1, nums, memo);\nif (possible) {\n\nmemo[currentKey] = possible;\n\nreturn :\n}\n\nnotPossible = isPossible(targetSum, currentIndex+1, nums, memo);\nmemo[currentKey] = possible||notPossible;\nreturn memo[currentKey] ;\n\n}\n\ncanPartition( < >& nums) {\n\ntotal = 9;\nfor( it:nums) total+= it;\nif(total%2!=0) return Ff\nunordered_map<string, > memo;\nreturn isPossible(total/2,0, nums,memo) ;\n\n}\ni\nhttps://www.linkedin.com/in/karun-karthik",
          "char_count": 861,
          "ocr_used": true,
          "original_char_count": 42
        },
        {
          "page": 17,
          "text": "https://www.linkedin.com/in/karun-karthik\n\n(4) Target hum _,\nyi an any § fort , nd the tumbu ¢ woyd to Meath Forge\nby using + & — before each Unmet m onnay.\n6 nput: nums = [1,1,1,1,1], target =\n= ote {1,1,1,1,1], target = 3\nExplanation: There are 5 ways to assign symbols to make the sum\nof nums be target 3.\n—1+1+1+1+1=3\n41 -1+1+1+12=3\n+1+4+1-1+1+1=3\n+1+1+1-1+1=3\n41+1+1+1-1=3\n— a may udbs ae Can um + ot — Sign\n4 +tnt=t -G nums(¢I]) => + —numg(cr]\n4 — Tun t 2% -(-nume(¢r]) => +t +nums(cr]\nas os\n> OF enw node, AuTi Me fur VOL Yeon\nLge mphr Beco we nual 1 Thind the\ntotal numbu \"% ari\nhttps://www.linkedin.com/in/karun-karthik",
          "char_count": 629,
          "ocr_used": true,
          "original_char_count": 42
        },
        {
          "page": 18,
          "text": "https://www.linkedin.com/in/karun-karthik\n\n_—_—\nee\nSolution {\ntotalWays( currentIndex, vector< >&nums, target, unordered_map<string, > &memo){\n(target==0 and currentIndex==nums.size()){\n1;\n}\n(currentIndex>=nums.size() and target!=0){\n®;\n}\nstring key = to_string(currentIndex)+ +to_string(target) ;\n(memo. find(key) !=memo.end()){\nmemo[key];\n}\nplus = totalWays(currentIndex+1, nums, target-nums[currentIndex],memo) ;\nminus = totalWays(currentIndex+1, nums, target+nums[currentIndex],memo) ;\nmemo[key] = plus+minus;\nplus+minus;\n}\nfindTargetSumWays(vector< >& nums, target) {\nunordered_map<string, > memo;\ntotalWays(@,nums, target, memo) ;\n}\n}5\nhttps://www.linkedin.com/in/karun-karthik",
          "char_count": 684,
          "ocr_used": true,
          "original_char_count": 42
        },
        {
          "page": 19,
          "text": "https://www.linkedin.com/in/karun-karthik\n\nCounr mumber ae Aub wsith given dieu, —\n+ Tha uv Aimila to Target am.\nYon the dapperncr puwen fwo Aub, and an anny\nfind 70. 4 Aubses uoith the diffeane.\nApproach\nLe fay Al - Sp = Ueffueree (porn) —_ a)\nwe con calorlola AUN § wy dharma doa 20m\n4 can be Aud that yor a dubs Ary de\nAi+ Adc Aum ._— ©)\nnow O12) » gd) = difpowot Aun\nfie (diguuna + tum) /2.\nas dmplenut Toxger dum with —Torger value. = £I\n———————— =\nhttps://www.linkedin.com/in/karun-karthik",
          "char_count": 496,
          "ocr_used": true,
          "original_char_count": 42
        },
        {
          "page": 20,
          "text": "https://www.linkedin.com/in/karun-karthik\n\n@) Dek and Eon\nYou are given an integer array nums . You want to maximize the number of points\nyou get by performing the following operation any number of times:\ne Pick any nums[i} and delete it to earn nums[i} points. Afterwards, you\nmust delete every element equal to nums{i] - 1 and every element\nequal to nums[i] + 1.\nReturn the maximum number of points you can earn by applying the above\noperation some number of times.\nEg num = (a, 2,3,3,3,4]\n> we Ata dudahing Dt tw -arra aX\nthn ruumy = [3,3,3, 4]\nwe nud fo delet al! Atl YAH => mums = Cu]\n> ue dilete 4 then pow = 4+ = B\n(or)\n. ; = =@\n= we Arour dkehng 3, thn sulk = 3+343 =\nthn num = [2a,2,4]\n=> mum =\n% we nud dude all 3-1 Bt > Ud\n(or)\n> Y we Ata duking Up then Miuly = &\nthn numu-:[a,2,3,3,3)\nwe purd fo delet olf Yt YY => num = C2, 2]\nwe duly 2 then por = 4H = ZB\nhttps://www.linkedin.com/in/karun-karthik",
          "char_count": 911,
          "ocr_used": true,
          "original_char_count": 42
        },
        {
          "page": 21,
          "text": "https://www.linkedin.com/in/karun-karthik\n\nwre the Similar approach\nnuma = [a,a, 3, 3,3, 4 - fofo}a} ali)\n7? ? 2 2-9\no 6 2 3 y\nC1+1\nCL* qua (cr} +\\CI+2\n(ods =\nee\nSolution {\nmaxPoints(vector<int>& freq, currentIndex, unordered_map<int, int>&memo){\n(currentIndex >= freq.size()) Q;\nkey = currentIndex;\n(memo.find(key) != memo.end()) memo[key] ;\nDelete = currentIndex*freq[currentIndex] + maxPoints(freq, currentIndex+2, memo) ;\nNotDelete = maxPoints(freq, currentIndex+1, memo) ;\nmemo[key] = max(Delete, NotDelete) ;\nmemo[key ];\n}\ndeleteAndEarn(vector<int>& nums) {\nmaxi = *max_element(nums.begin(), nums.end());\nvector<int> freq(maxi+1, @);\n( i: nums) freq[i]++;\nunordered_map< 7 > memo;\nmaxPoints(freq, @, memo);\n}\n35\nhttps://www.linkedin.com/in/karun-karthik",
          "char_count": 760,
          "ocr_used": true,
          "original_char_count": 42
        },
        {
          "page": 22,
          "text": "https://www.linkedin.com/in/karun-karthik\n\nimulou to 0-1 Knapdack bul allows\n(12) Unbounoid Knapdack. Aimilos oP\nwa & chorde On Umm more than One\n(an\nwr = [2,1] , than Fans\n; it boundid Knopdack propre &\nvou - [1,1]\nit Lnboundud knapsack then profit = 3\nCapaury =3 Rl tie\nfor unboundsol Knapsack (tp)\nxX\n_\nee\nSolution{\nhelper ( W, wt[], val[], N, curr, vector<vector< >>&memo) {\n(W==@) 8;\n(curr==N) Q;\n(memo[curr][W]!=-1) memo[curr][W];\ncurrWt = wt[curr];\ncurrVal = val[curr];\nt selected = @;\n(currWt<=W){\nselected = currVal + helper(W-currWt, wt, val, N, curr, memo);\n}\nnotSelected = helper(W, wt, val, N, curr+1, memo);\nmemo[curr][W] = max(selected, notSelected) ;\nmemo[curr][W];\n}\nt knapSack( N, tw, t val[], t wt[])\n{\nvector<vector< >> memo( N , vector<int> (W+1, -1));\nhelper(W, wt, val, N, @, memo);\n}\ni)\nhttps://www.linkedin.com/in/karun-karthik",
          "char_count": 853,
          "ocr_used": true,
          "original_char_count": 42
        },
        {
          "page": 23,
          "text": "https://www.linkedin.com/in/karun-karthik\n\na .\n(3) Gin Chany 2 ( Binidas & unbounded Knapdaek )\nw\n——“~—-Y ——_ -\nLyasen an aunay 3 Com % Omount, tind toto} numba & ways ]\nae amaks that Omount .\nLombinahons 0 up\n. lritititl\nEg wim = [1,2,5] Lelth42\nl+242\namounk = 5\n5\ninc x .\nCoins [ __*L_ _l X= comm [ec]\n+ Cl,\nCA, 0, amount CO, O&\nCC, a-x ?\noda\n_—_—\nclass Solution {\npublic:\nint totalWays(int currentIndex, vector<int>& coins, int amount, vector<vector<int>>&memo) {\nif(amount == @) return 1;\nif(currentIndex >= coins.size()) return @;\nif (memo[currentIndex][amount]!=-1) return memo[currentIndex] [amount];\nint consider = 0;\nif (coins[currentIndex]<=amount) {\nconsider = totalWays(currentIndex, coins, amount-coins[currentIndex],memo) ;\n}\nint notConsider = totalWays(currentIndex+1, coins, amount, memo) ;\nmemo[currentIndex][amount] = consider+notConsider;\nreturn memo[currentIndex] [amount] ;\n}\nint change(int amount,vector<int>& coins) {\nvector<vector<int>>memo(coins.size()+1,vector<int>(amount+1, -1));\nreturn totalWays(@,coins, amount, memo) ;\n}\nis\nhttps://www.linkedin.com/in/karun-karthik",
          "char_count": 1097,
          "ocr_used": true,
          "original_char_count": 42
        },
        {
          "page": 24,
          "text": "https://www.linkedin.com/in/karun-karthik\n\n. im f unbounded Knapiack \\\n(14) Coin Changs C Bimidos appa,\nJ -\n\n; . . .\nLiven an oboy TE Wonk % amount» tind Just numba @ come\n40 maks Up that amount, run — 4 uw not possible\n\nute\n——>\n= for > fe. MM\nEg coins = [1,2,8])\n142424242422 171\namount = | :\nl45+S = C1\nwins [ x l\na X= cotm [ec\nr [ec] Cl,\n2 &, anu CoH, &\n[+ cc, Qa-xX +H,\ncode —>\nee\nSolution {\nminimumCoins( currentIndex, vector<int>& coins, int amount, vector<vector<int>>&memo){\n(amount == @) e;\n(currentIndex >= coins.size()) 102000;\n(memo[currentIndex] [amount] !=-1) memo[currentIndex] [amount] ;\nconsider = 100000;\n(coins[currentIndex]<=amount) {\nconsider = 1 + minimumCoins(currentIndex, coins, amount-coins[currentIndex], memo) ;\ni\nnotConsider = minimumCoins(currentIndex+1, coins, amount, memo);\nmemo[currentIndex] [amount ]= min(consider ,notConsider) ;\nmemo[currentIndex] [amount] ;\n}\ni coinChange(vector<int>& coins, i amount) {\nvector<vector< >>memo(coins.size()+1,vector< >(amount+1, -1));\nans = minimumCoins(®@, coins, amount, memo);\n(ans==100000)? -1 : ans;\n}\n}5\nhttps://www.linkedin.com/in/karun-karthik",
          "char_count": 1123,
          "ocr_used": true,
          "original_char_count": 42
        },
        {
          "page": 25,
          "text": "https://www.linkedin.com/in/karun-karthik\n\n] Red Lustig\n— 77>\nY6 a hod Q Langfh N and amay ff prea. find the maa\nvalue that can be obfavud M4 Cutting nod\n1234 5 & 4\nbs N=¢ pric = [1,5,8,4, 10,1819, 20]\no (23 4 5 6 9\n# pviieg 6} oO piece jr prics [CT J , UsOSe Lingtn u Citi\na 6\nwf we wut Ow JOA intd A pieces t Length A,e we gee\nrox volun 4 5412 bb AA,\n— thur onight be othe wouyt but thir portion\nx gt any anatanty Length * eww pin & Cit!\nws, yz MO ut\nprias(et) +(cz, 4 (ex)\nfor\nhttps://www.linkedin.com/in/karun-karthik",
          "char_count": 522,
          "ocr_used": true,
          "original_char_count": 42
        },
        {
          "page": 26,
          "text": "https://www.linkedin.com/in/karun-karthik\n\n_—_—\nee\nSolution{\nmaxProfit( price[], currentIndex, n, vector<vector< >>&memo) {\n(n==0) e;\n(currentIndex>=n) @;\n(memo[currentIndex][n]!=-1) memo[currentIndex][n];\nselected = 0;\n(currentIndex+1<=n) {\nselected = price[currentIndex]+maxProfit(price, currentIndex, n-(currentIndex+1), memo);\n}\nnotSelected = maxProfit(price, currentIndex+1, n, memo);\nmemo[currentIndex][n] = max(selected, notSelected) ;\nmemo[currentIndex][n];\n}\ncutRod( price[], n) {\nvector<vector< >> memo(n+1, vector< >(n+1,-1));\nmaxProfit(price,@,n,memo) ;\n}\n35\nhttps://www.linkedin.com/in/karun-karthik",
          "char_count": 613,
          "ocr_used": true,
          "original_char_count": 42
        },
        {
          "page": 27,
          "text": "https://www.linkedin.com/in/karun-karthik\n\nFind the rest on\n\nhttps://linktr.ee/KarunKarthik\n\nFollow Karun Karthik For More Amazing Content !\nhttps://www.linkedin.com/in/karun-karthik",
          "char_count": 183,
          "ocr_used": true,
          "original_char_count": 42
        }
      ],
      "metadata": {
        "format": "PDF 1.5",
        "title": "",
        "author": "",
        "subject": "",
        "keywords": "",
        "creator": "",
        "producer": "",
        "creationDate": "",
        "modDate": "",
        "trapped": "",
        "encryption": null
      },
      "char_count": 18432,
      "word_count": 2896,
      "ocr_pages_count": 25,
      "error": null,
      "file_id": "1L5T_hIf_AliZBOiClu6mhmHmRg4UsOXA",
      "filename": "Dynamic programm notes.pdf",
      "filepath": "downloads/Dynamic programm notes.pdf",
      "download_link": "https://drive.google.com/uc?export=download&id=1L5T_hIf_AliZBOiClu6mhmHmRg4UsOXA"
    },
    {
      "success": true,
      "text": "FortiOS - REST API Reference\nVersion 5.6.11\n\nFi RTINET.\nFortiOS - REST API Reference\nVersion 5.6.11\nFORTINET DOCUMENT LIBRARY\nhttps://docs.fortinet.com\nFORTINET VIDEO GUIDE\nhttps://video.fortinet.com\nFORTINET BLOG\nhttps://blog.fortinet.com\nCUSTOMER SERVICE & SUPPORT\nhttps://support.fortinet.com\nFORTINET TRAINING & CERTIFICATION PROGRAM\nhttps://www.fortinet.com/support-and-training/training.html\nNSE INSTITUTE\nhttps://training.fortinet.com\nFORTIGUARD CENTER\nhttps://fortiguard.com/\nEND USER LICENSE AGREEMENT\nhttps://www.fortinet.com/doc/legal/EULA.pdf\nFEEDBACK\nEmail: techdoc@fortinet.com\nAugust 15, 2019\nFortiOS 5.6.11 REST API Reference\n01-5611-414177-20190815\nTABLE OF CONTENTS\nChange Log\n5\nIntroduction\n6\nWhat's New in the REST API\n6\nAuthentication\n6\nSession-based authentication\n6\nToken-based authentication\n8\nAuthorization\n11\nSupported HTTP methods\n11\nResponse codes\n11\nDebugging\n12\nCMDB API\n14\nURL path\n14\nURL parameters\n14\nGeneric parameters\n15\nSpecific parameters\n15\nBody data\n16\nLimitation\n16\nFilter with multiple key/value pairs\n17\nFilter Syntax\n17\nFilter Operators\n17\nCombining Filters\n17\nReserved Characters\n18\nList of Methods\n18\ncollection\n19\nresource\n19\nExamples\n22\nRetrieve table\n22\nRetrieve table schema\n23\nRetrieve table default\n23\nPurge table\n23\nRetrieve object\n23\nCreate object\n24\nEdit object\n24\nDelete object\n24\nClone object\n24\nMove object\n25\nAppend child object\n25\nEdit child object\n25\nDelete child object\n25\nPurge child table\n25\nRetrieve complex table\n26\nFortiOS REST API Reference\nFortinet Inc.\nEdit complex table\n26\nGlobal requests (apply to all accessible vdoms)\n26\nMonitor API\n27\nURL path\n27\nURL parameters\n27\nGeneric parameters\n27\nSpecific parameters\n28\nBody data\n28\nFile upload\n28\nFile upload via JSON data\n28\nFile upload via multi-part file\n29\nFile download\n29\nFile download via browser\n29\nFile download via script\n29\nList of Methods\n30\nendpoint-control\n39\nfirewall\n44\nfortiview\n55\ngeoip\n56\nips\n57\nlicense\n57\nlog\n59\nrouter\n63\nsystem\n66\nswitch-controller\n82\nvpn-certificate\n99\nextender-controller\n103\nuser\n104\nutm\n110\nvirtual-wan\n112\nwebfilter\n112\nvpn\n115\nwanopt\n118\nwebproxy\n120\nwebcache\n120\nwifi\n121\ncoverage\n126\nExamples\n127\nFortiOS REST API Reference\nFortinet Inc.\n4\nFortiOS REST API Reference\nFortinet Inc.\nChange Log\nDate\nChange Description\n2019-08-15\nInitial release.\nFortiOS REST API Reference\nFortinet Inc.\nIntroduction\nThis document provides the REST API information supported in FortiOS 5.6.11. This document covers a reference of\nthe REST API supported by the FortiOS GUI.\nFortiOS 5.6.11 supports the following REST APIs:\nl\nCMDB API\nl\nRetrieve object meta data (default, schema)\nl\nRetrieve object/table (with filter, format, start, count, other flags)\nl\nCreate object\nl\nModify object\nl\nDelete object\nl\nClone object\nl\nMove object\nl\nMonitor API\nl\nRetrieve/Reset endpoint stats (with filter, start, count)\nl\nPerform endpoint operations\nl\nUpload/Download file\nl\nRestore/Backup config\nl\nUpgrade/Downgrade firmware\nl\nRestart/Shutdown FGT\nWhat's New in the REST API\nNo major change to the REST API endpoints.\nAuthentication\nStarting in FortiOS 5.6.1, there are two ways that user can authenticate against the API:\nl\nSession-based authentication (legacy)\nl\nToken-based authentication (5.6.1 and newer)\nSession-based authentication\nAs the name suggests, the authentication is valid per login session. The user needs to send a login request to obtain a\nauthentication cookie and CSRF token to be used for subsequent requests. The user then needs to send a logout\nrequest to invalidate the authentication cookie and CSRF token.\nFortiOS REST API Reference\nFortinet Inc.\nAuthentication Cookie\nAuthentication cookie (APSCOOKIE) is provided by the API after a successful login request. All subsequent requests\nmust include this cookie to be authorized by the API. Any request without the cookie or with mismatched cookie will be\ndenied access to the API (HTTP 401 error code).\nCSRF Tokens\nCross-Site Request Forgery (CSRF) Tokens are alphanumeric values that are passed back-and-forth between client and\nserver to ensure that a user's form submission does not originate from an offsite document.\nThe CSRF token is available in the session ccsrftoken cookie, which must be included in the request header under\nX-CSRFTOKEN. See test script sample for how to handle CSRF token.\nOnly write requests (HTTP POST/PUT/DELETE) need CSRF tokens. Read requests (HTTP\nGET) do not require CSRF tokens.\nSetting Up an Authenticated Session\nTo authenticate with the FortiGate and request a session, send a POST request to the log in request handler with your\nusername and password.\nLogin URL\n/logincheck\nThe request body must contain the following keys in URL form encoding:\nKey\nType\nDescription\nusername\nString\nThe admin username.\nsecretkey\nString\nThe password for that admin.\najax\nInt (1)\nRequired: Format the response for easier parsing. Enable using 1.\nExample:\nPOST /logincheck\nusername=AdminUser&secretkey=AdminPassword&ajax=1\nEnsure that you're using the correct protocol. By default, a FortiGate? will redirect HTTP requests to HTTPS and your\nlogin requests may fail. As well, FortiGate? 's will use a self-signed server certificate by default. Refer to the\ndocumentation for the specific library or framework that you're using to validate the certificate manually.\nThe response to this request will be in the following format:\n<status_code><javascript>\nA successful login response would be:\n1document.location=\"/ng/prompt?viewOnly&redir=%2Fng%2F\";\nFor most uses, you only need to read the first character of the response body to get the response status code.\nIntroduction\n7\nFortiOS REST API Reference\nFortinet Inc.\nCode\nDescription\n0\nLog in failure. Most likely an incorrect username/password combo.\n1\nSuccessful log in*\n2\nAdmin is now locked out\n3\nTwo-factor Authentication is needed**\n* In some cases users may receive a successful login status but not be completely authenticated, such as when there is\na post-login-banner configured.\n** For Two-Factor log in, make another POST request with the same username and password, but include the token_\ncode parameter with the value of the one-time-password.\nOnce you've received a successful login status, read each Set-Cookie header and retain the following Cookies:\nName\nDescription\nAPSCOOKIE_<NUMBER>\nThis cookie authenticates you with the FortiGate . You must present this cookie\nwith every subsequent request you make after logging in.\nccsrftoken\nThis is the (c)CSRF token. As described in Authorization on page 11, you must\nprovide the value of this cookie as a X-CSRFTOKEN header. *\n*There may be two ccsrftoken cookies, one with a number suffix that matches the APSCOOKIE. For simplicity, you\ndon't need to locate that cookie and can rely on the ccsrftoken cookie.\nLogging out of an Authenticated Session\nAuthenticated sessions will remain active with the device until any of the following occurs:\nl\nThe admin logs out\nl\nThe session remains inactive for longer than the timeout specified by the admintimeout setting in config\nsystem global\nl\nThe admin is disconnected by another admin\nThere are a limited number of admins that can have active sessions on the device, therefore it's recommended that you\nlog out when you're finished using the device.\nTo log out, a POST request to the /logout URL will remove the current session.\nLogout URL\n/logout\nBody data\nnone needed\nToken-based authentication\nThe authentication is done via a single API token. This token is only generated once when creating an API admin. The\nuser must store this token in a safe place because it cannot be retrieved again. The user can however regenerate the\ntoken at any time. Each API request must include the token in order to be authenticated as the associated API admin.\nIntroduction\n8\nFortiOS REST API Reference\nFortinet Inc.\nOnly HTTPS access is allowed with token-based authentication to ensure maximum security.\nCreate API admin\nIn order to use the token-based authentication, user must first create a special API admin. The user can assign vdom\nprovision and admin profile to this API admin which defines the admin's privileges.\nOnly Super admin can create or modify API admin.\nconfig system api-user\nedit \"api-admin\"\nset comments \"admin for API access only\"\nset api-key ENC SH23sQt? +/9D9/mKb1oQoDvlP32ggn/cpQeGcY/VGUe5S5WIr5nqU20xcNMYDQE=\nset accprofile \"API profile\"\nset vdom \"root\"\nconfig trusthost\nedit 1\nset ipv4-trusthost 192.168.10.0 255.255.255.0\nnext\nend\nnext\nend\nGUI does not allow user to pick super admin or prof_admin profile for API admin to\nencourage user to use a special profile.\nTrusted host\nAt least one trusted host must be configured for the API admin. The user can define multiple trusted host/subnet. IPv6\nhosts are also supported.\nPKI Certificate\nToken-based also supports certificate matching as an extra layer of security (set PKI group in api-user). Both client\ncertificate and token must match to be granted access to the API. PKI option is enabled by default.\nCORS permission\nToken-based also supports Cross Origin Resource Sharing (CORS) allowing third-party web apps to make API requests\nto FGT using the token. CORS is disabled by default.\nIntroduction\n9\nFortiOS REST API Reference\nFortinet Inc.\nGenerate API token\nAfter creating the api-user, user can generate new token via CLI command, GUI, or REST API. The token is only shown\nonce and cannot be retrieved after. The user needs to generate new token if they forget.\nCLI command:\nexecute api-user generate-key [API user name]\nNew API key: fccys3cfbhyhqbqghkyzm1QGNnm31r\nThe bearer of this API key will be granted all access privileges assigned to the api-user api-\nadmin.\nREST API\nRequest\nBody data\nDescription\nPOST /api/v2/monitor/system/api-\nuser/generate-key?vdom=root\n{'api-user':\"api-\nadmin\"}\n\"Generate a new api-key for the specified\napi-key-auth admin. The old api-key will be\nreplaced. The response contains the only\nchance to read the new api-key plaintext in\nthe access_token field.\"\nUse the API token\nThe API token can be included in any REST API request via either request header or URL parameter\nPassing API token via request header\nThe user needs to explicitly add the following field to the request header: 'Authorization': 'Bearer ' +\n[api_token]\nAuthorization: Bearer fccys3cfbhyhqbqghkyzm1QGNnm31r\nPassing API token via request URL parameter\nThe user needs to explicitly include the following field in the request URL parameter: access_token=[api_token]\nMethod\nURL\nBody\ndata\nDescription\nGET\n/api/v2/cmdb/firewall/address\n?vdom=root&access_\ntoken=fccys3cfbhyhqbqghkyzm1QGNnm31r\nRetrieve all IPv4\nfirewall\naddresses,\nvdom root\nIntroduction\n10\nFortiOS REST API Reference\nFortinet Inc.\nAuthorization\nAfter the request is authenticated, the API will check if the associated admin has the permission to perform the\noperation. Each admin or API admin has an admin profile and vdom scope which define the privileges of the admin. For\nexample, if the admin has vdom scope set to \"vdom1\" and a profile that only has read-only permission to Firewall\nobjects access group, the admin can only access vdom1 resource, and cannot make change to Firewall objects (policy,\naddress, etc).\nEach endpoint requires specific group permission defined in 'Access Group' of the endpoint summary table. Request to\nthe endpoint will be checked against this access group to ensure the admin has proper permission to access the\nresource. Make sure the administrative account you login with has the permissions required to perform the intended\nactions.\nAdmin with read-only permission to the resource can only send read requests (HTTP GET) to the resource. Admin with\nwrite permission to the resource can send read/write requests (HTTP GET/POST/PUT/DELETE) to the resource. Admin\nwith no permission to the resource cannot access the resource.\nRequest with insufficient profile permission will return 403 error.\nSupported HTTP methods\nFortiOS REST APIs support the following HTTP methods:\nHTTP Method\nDescription\nGET\nRetrieve a resource or collection of resources.\nPOST\nCreate a resource or execute actions.\nPUT\nUpdate a resource.\nDELETE\nDelete a resource or collection of resources.\nFor any action other than GET, you must provide the X-CSRFTOKEN header in the request. The value of this header is\nthe value of the ccsrftoken cookie that is provided by the FortiGate when you log in.\nIf the request is submitted using HTTP POST , the HTTP method can also be overridden using the \"X-HTTP-Method-\nOverride\" HTTP header.\nResponse codes\nFortiOS APIs use well-defined HTTP status codes to indicate the results of queries to the API.\nThe following table shows how some of the HTTP status codes are used in the context of FortiOS REST APIs.\nIntroduction\n11\nFortiOS REST API Reference\nFortinet Inc.\nHTTP Response Code\nDescription\n200 - OK\nRequest returns successful.\n400 - Bad Request\nRequest cannot be processed by the API.\n401 - Not Authorized\nRequest without successful login session.\n403 - Forbidden\nRequest is missing CSRF token or administrator is missing access profile\npermissions.\n404 - Resource Not Found\nUnable to find the specified resource.\n405 - Method Not Allowed\nSpecified HTTP method is not allowed for this resource.\n413 - Request Entity Too Large\nRequest cannot be processed due to large entity.\n424 - Failed Dependency\nFail dependency can be duplicate resource, missing required parameter, missing\nrequired attribute, invalid attribute value.\n429 - Too many requests\nThe request is actively blocked by FGT due to a rate limit. For example, if an\nadmin uses invalid credentials too many times, there will be a timeout before\nthey can try again.\n500 - Internal Server Error\nInternal error when processing the request.\nDebugging\nVerbose debug output can be enabled in the FortiGate CLI with the following commands:\ndiagnose debug enable\ndiagnose debug application httpsd -1\nThis will produce the following output when the REST API for IPv4 policy statistics is queried:\n[httpsd 228 - 1418751787] http_config.c[558] ap_invoke_handler -- new request (handler='api_\nmonitor_v2-handler', uri='/api/v2/monitor/firewall/policy', method='GET')\n[httpsd 228 - 1418751787] http_config.c[562] ap_invoke_handler -- User-Agent: Mozilla/5.0\n(Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko)\nChrome/39.0.2171.71 Safari/537.36\n[httpsd 228 - 1418751787] http_config.c[565] ap_invoke_handler -- Source: 192.168.1.100:56256\nDestination: 192.168.1.99:443\n[httpsd 228 - 1418751787] api_monitor.c[1427] api_monitor_v2_handler -- received api_monitor_\nv2_request from '192.168.1.100'\n[httpsd 228 - 1418751787] aps_access.c[3652] aps_chk_rolebased_perm -- truncated URI\n(/api/v2/monitor/firewall/policy) to (/api/v2/monitor) for permission check\n[httpsd 228 - 1418751787] api_monitor.c[1265] handle_req_v2_vdom -- attempting to change from\nvdom \"root\" to vdom \"root\"\n[httpsd 228 - 1418751787] api_monitor.c[1280] handle_req_v2_vdom -- new API request\n(action='select',path='firewall',name='policy',vdom='root',user='admin')\n[httpsd 228 - 1418751787] api_monitor.c[1286] handle_req_v2_vdom -- returning to original vdom\n\"root\"\n[httpsd 228 - 1418751787] http_config.c[581] ap_invoke_handler -- request completed\n(handler='api_monitor_v2-handler' result==0)\nIntroduction\n12\nFortiOS REST API Reference\nFortinet Inc.\nThis debug will also include all requests to/from the FortiOS web interface, in addition to\nREST API requests.\nIntroduction\n13\nFortiOS REST API Reference\nFortinet Inc.\nCMDB API\nCMDB API is used to retrieve and modify CLI configurations. For example, create/edit/delete firewall policy.\nURL path\nAll CMDB requests start with '/api/v2/cmdb/'. Below is the format of CMDB URL path.\n/api/v2/cmdb/<path>/<name>/<mkey>(optional)/<child_name>(optional)/<child_mkey>(optional)/\nCMDB URL path follows CLI commands syntax with an exception of vdom configuration.\nCLI\nCommand\npath\nname\nmkey\nchild_\nname\nchild_\nmkey\nFull URL\nconfigure vdom\nsystem\nvdom\n/api/v2/cmdb/system/vdom/\nconfigure vdom, edit vdom1\nsystem\nvdom\nvdom1\n/api/v2/cmdb/system/\nvdom/vdom1/\nconfigure firewall schedule\nrecurring\nfirewall.\nschedule\nrecurring\n/api/v2/cmdb/\nfirewall.schedule/recurring/\nconfigure firewall policy\nfirewall\npolicy\n/api/v2/cmdb/firewall/policy/\nconfigure firewall policy,\nedit 1\nfirewall\npolicy\n1\n/api/v2/cmdb/firewall/policy/1/\nconfigure firewall policy,\nedit 1, set srcintf\nfirewall\npolicy\n1\nsrcintf\n/api/v2/cmdb/firewall/\npolicy/1/srcintf/\nconfigure firewall policy,\nedit 1, delete srcintf lan\nfirewall\npolicy\n1\nsrcintf\nlan\n/api/v2/cmdb/firewall/\npolicy/1/srcintf/lan/\nFor operations on the entire table, mkey is not needed. For instance, add new entry, get all entries, purge table.\nFor operations on a specific resource, mkey is required. For example, edit/delete/clone/move a firewall policy.\nFor operations on the child table, child_name is required. For example, retrieve child table, purge child table, add new\nentry to child table.\nFor operations on the child table entry, child_mkey is required. For example, delete/move child object.\nURL parameters\nIn addition to the URL path, user can specify URL parameters which are appended to the URL path.\nFortiOS REST API Reference\nFortinet Inc.\nGeneric parameters\nThe following URL parameters are generic to all CMDB requests.\nURL parameter\nExample\nDescription\nvdom=root\nGET\n/api/v2/cmdb/firewall/address/?vdom=root\nReturn result/apply changes on the specified\nvdom. If vdom parameter is not provided, use\ncurrent vdom instead. If admin does not have\naccess to the vdom, return permission error.\nglobal=1\nGET\n/api/v2/cmdb/firewall/address/?global=1\nReturn a list of results/apply changes on all\nprovisioned vdoms. The request is only\napplicable to vdoms that the admin has access\nto.\nSpecific parameters\nEach CMDB method may require extra URL parameters which are unique to the method. Those extra parameters are\ndocumented in the \"Extra Parameters\" section of each CMDB method.\nBelow are some examples.\nURL parameter\nExample\nDescription\naction=schema\nGET /api/v2/cmdb/firewall/policy\n/?action=schema\nReturn schema of the resource table\naction=default\nGET /api/v2/cmdb/firewall/policy\n/?action=default\nReturn default attributes of the resource\naction=move\nPUT /api/v2/cmdb/firewall/policy/1\n/?action=move&after=2\nMove policy 1 to after policy 2\naction=clone\nPOST\n/api/v2/cmdb/firewall/address/address1\n/?action=clone&nkey=address1_clone\nClone 'address1' to 'address1_clone'\nskip=1\nGET\n/api/v2/cmdb/firewall/policy/?skip=1\nReturn a list of all firewall policy but only show\nrelevant attributes\nskip=1\nGET\n/api/v2/cmdb/firewall/policy/1/?skip=1\nReturn firewall policy 1 but only show relevant\nattributes\nformat=policyid|action\nGET /api/v2/cmdb/firewall/policy\n/?format=policyid|action\nReturn a list of all firewall policy, however,\nonly show policyid and action for each policy\nformat=policyid|action\nGET /api/v2/cmdb/firewall/policy\n/1?format=policyid|action\nReturn firewall policy 1, however, only show\npolicyid and action\nstart=0&count=10\nGET /api/v2/cmdb/firewall/address\n/?start=0&count=10\nReturn the first 10 firewall addresses\nCMDB API\n15\nFortiOS REST API Reference\nFortinet Inc.\nURL parameter\nExample\nDescription\nkey=type&pattern=fqdn\nGET /api/v2/cmdb/firewall/address\n/?key=type&pattern=fqdn\nReturn all addresses with type fqdn\nfilter=type==fqdn\nGET /api/v2/cmdb/firewall/address\n/?filter=type==fqdn\nReturn all addresses with type fqdn\nfilter=type==\nfqdn,type==ipmask&filter=\nvisibility==enable\nGET /api/v2/cmdb/firewall/address\n/?filter=type==fqdn,type==\nipmask&filter=visibility==enable\nReturn all addresses with type fqdn or ipmask\nwhich has visibility enabled\nBody data\nBeside URL parameters, some POST/PUT requests also require body data, which must be included in the HTTP body.\nFor example, to create/edit firewall address object, user needs to specify the new/edit object data.\nGET/DELETE requests do not accept body data.\nRequest\nBody data\nDescription\nPOST /api/v2/cmdb/firewall/address?vdom=root\n{'name':\"address1\", 'type':\n\"ipmask\", 'subnet': \"1.1.1.0\n255.255.255.0\"}\ncreate new firewall address\nwith the specified data\nPUT\n/api/v2/cmdb/firewall/address/address1?vdom=root\n{'subnet': \"2.2.2.0\n255.255.255.0\"}\nedit firewall address with the\nspecified data\nLimitation\nIf the body data has the same name as some reserved URL parameters, such as name, path, or action, the request\nwould fail due to the conflict. For example, firewall policy has 'name' and 'action' attribute which conflict with the\nreserved URL parameter 'name' and 'action'. POST/PUT with normal method would fail with 405 error. A workaround is\nto enclosed all object data in a 'json' keyword so the API can correctly identify object data. For example:\nRequest\nBody data\nDescripti\non\nPOST\n/api/v2/cmdb/firewall/policy?vdom\n=root\n{'name':\"test_policy\", 'srcintf': [{\"name\":\"port1\"}], 'dstintf':\n[{\"name\":\"port2\"}],'srcaddr': [{\"name\":\"all\"}],'dstaddr':\n[{\"name\":\"all\"}],'action':\"accept\",'status':\"enable\",'schedule':\"always\n\",'service':[{'name':\"ALL\"}],'nat':\"disable\"}\nThis\nwould fail\nwith 405\nerror\nPOST\n/api/v2/cmdb/firewall/policy?vdom\n=root\n{'json':{'name':\"test_policy\", 'srcintf': [{\"name\":\"port1\"}], 'dstintf':\n[{\"name\":\"port2\"}],'srcaddr': [{\"name\":\"all\"}],'dstaddr':\n[{\"name\":\"all\"}],'action':\"accept\",'status':\"enable\",'schedule':\"always\n\",'service':[{'name':\"ALL\"}],'nat':\"disable\"}}\nThis\nwould\nwork\nCMDB API\n16\nFortiOS REST API Reference\nFortinet Inc.\nFilter with multiple key/value pairs\nFiltering multiple key/value pairs are also supported for all CMDB retrieval requests via 'filter' URL parameter.\nFilter Syntax\nFilters are defined in the following syntax: key operator pattern\nKey\nOperator\nPattern\nFull Request\nDescription\nschedule\n==\nalways\nGET\n/api/v2/cmdb/firewall/policy/?filter=\nschedule==always\nOnly return firewall policy with\nschedule 'always'\nschedule\n!=\nalways\nGET\n/api/v2/cmdb/firewall/policy/?filter=\nschedule!=always\nReturn all firewall policy with\nschedule other than 'always'\nFilter Operators\nOperator\nDescription\n==\nCase insensitive match with pattern.\n!=\nDoes not match with pattern (case insensitive).\n=@\nPattern found in object value (case insensitive).\n!@\nPattern not found in object value (case insensitive).\n<=\nValue must be less than or equal to pattern.\n<\nValue must be less than pattern.\n>=\nValue must be greater than or equal to pattern.\n>\nValue must be greater than pattern.\nCombining Filters\nFilters can be combined to create complex queries.\nCombination\nDescription\nExample\nLogical OR\nSeparate filters using commas \",\". The\nfollowing example returns all policies\nusing the always schedule or the once\nschedule.\nGET /api/v2/cmdb/firewall/policy?filter=\nschedule==always,schedule==once\nCMDB API\n17\nFortiOS REST API Reference\nFortinet Inc.\nCombination\nDescription\nExample\nLogical AND\nFilter strings can be combined to create\nlogical AND queries by including multiple\nfilters in the request. This example\nincludes all policies using schedule\nalways AND action accept.\nGET /api/v2/cmdb/firewall/policy/?filter=\nschedule==always&filter=action==accept\nCombining\nAND and OR\nYou can combine AND and OR filters\ntogether to create more complex filters.\nThis example includes all policies using\nschedule always AND action accept OR\naction deny.\nGET /api/v2/cmdb/firewall/policy/?filter=\nschedule==always&filter=action==accept,action==deny\nReserved Characters\nThe following characters need to be escaped if they are part of a filter pattern.\nCharacter\nEscaped Value\n,\n\\,\n\\\n\\\\\nList of Methods\nType\nHTTP Method\nAction\nSummary\ncollection\nGET\nSelect all entries in a CLI table.\nresource\nGET\ndefault\nReturn the CLI default values for this object type.\nresource\nGET\ndefault\nReturn the CLI default values for entire CLI tree.\nresource\nGET\nschema\nReturn the CLI schema for this object type.\nresource\nGET\nschema\nReturn schema for entire CLI tree.\ncollection\nDELETE\nDelete all objects in this table.\ncollection\nPOST\nCreate an object in this table.\nresource\nGET\nSelect a specific entry from a CLI table.\nresource\nPUT\nUpdate this specific resource.\nresource\nPUT\nmove\nMove this specific resource.\nresource\nPOST\nclone\nClone this specific resource.\nCMDB API\n18\nFortiOS REST API Reference\nFortinet Inc.\nType\nHTTP Method\nAction\nSummary\nresource\nDELETE\nDelete this specific resource.\nresource\nGET\nBuild API directory.\ncollection\nGET\nSummary\nSelect all entries in a CLI table.\nHTTP Method\nGET\nETag Caching\nEnabled\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\ndatasource\nboolean\nEnable to include datasource information for each linked object.\nNo\nstart\nint\nStarting entry index.\nNo\ncount\nint\nMaximum number of entries to return.\nNo\nwith_meta\nboolean\nEnable to include meta information about each object (type id,\nreferences, etc).\nNo\nskip\nboolean\nEnable to call CLI skip operator to hide skipped properties.\nNo\nformat\nstring\nList of property names to include in results, separated by | (i.e.\npolicyid|srcintf).\nNo\nfilter\nstring\nComma separated list of key value pairs to filter on. Filters will be\nlogically OR'd together.\nNo\nkey\nstring\nIf present, objects will be filtered on property with this name.\nNo\npattern\nstring\nIf present, objects will be filtered on property with this value.\nNo\nresource\nGET: default\nSummary\nReturn the CLI default values for this object type.\nCMDB API\n19\nFortiOS REST API Reference\nFortinet Inc.\nHTTP Method\nGET\nETag Caching\nEnabled\nResponse Type\nobject\nGET: default\nSummary\nReturn the CLI default values for entire CLI tree.\nHTTP Method\nGET\nResponse Type\nobject\nGET: schema\nSummary\nReturn the CLI schema for this object type.\nHTTP Method\nGET\nETag Caching\nEnabled\nResponse Type\nobject\nGET: schema\nSummary\nReturn schema for entire CLI tree.\nHTTP Method\nGET\nResponse Type\nobject\nDELETE\nSummary\nDelete all objects in this table.\nHTTP Method\nDELETE\nPOST\nSummary\nCreate an object in this table.\nHTTP Method\nPOST\nCMDB API\n20\nFortiOS REST API Reference\nFortinet Inc.\nGET\nSummary\nSelect a specific entry from a CLI table.\nHTTP Method\nGET\nETag Caching\nEnabled\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\ndatasource\nboolean\nEnable to include datasource information for each linked object.\nNo\nwith_meta\nboolean\nEnable to include meta information about each object (type id,\nreferences, etc).\nNo\nskip\nboolean\nEnable to call CLI skip operator to hide skipped properties.\nNo\nformat\nstring\nList of property names to include in results, separated by | (i.e.\npolicyid|srcintf).\nNo\nPUT\nSummary\nUpdate this specific resource.\nHTTP Method\nPUT\nPUT: move\nSummary\nMove this specific resource.\nHTTP Method\nPUT\nExtra parameters\nName\nType\nSummary\nRequired\nbefore\nstring\nThe ID of the resource that this resource will be moved before.\nNo\nafter\nstring\nThe ID of the resource that this resource will be moved after.\nNo\nCMDB API\n21\nFortiOS REST API Reference\nFortinet Inc.\nPOST: clone\nSummary\nClone this specific resource.\nHTTP Method\nPOST\nExtra parameters\nName\nType\nSummary\nRequired\nnkey\nstring\nThe ID for the new resouce to be created.\nNo\nDELETE\nSummary\nDelete this specific resource.\nHTTP Method\nDELETE\nGET\nSummary\nBuild API directory.\nHTTP Method\nGET\nExamples\nRetrieve table\nMethod\nURL\nURL Parameters\nBody\nData\nDescription\nGET\n/api/v2/cmdb/\nfirewall/address\n?vdom=root\nRetrieve all IPv4 firewall addresses,\nvdom root\nGET\n/api/v2/cmdb/\nfirewall/address\n?vdom=root&start=\n0&count=10&skip=1\nRetrieve the first 10 firewall\naddresses, skip inapplicable\nattributes, vdom root\nGET\n/api/v2/cmdb/\nfirewall/address\n?vdom=root&format=name|type\nRetrieve all firewall addresses but only\nshow name and type, vdom root\nGET\n/api/v2/cmdb/\nfirewall/address\n?vdom=root&key=\ntype&pattern=fqdn\nRetrieve all fqdn firewall addresses,\nvdom root\nCMDB API\n22\nFortiOS REST API Reference\nFortinet Inc.\nMethod\nURL\nURL Parameters\nBody\nData\nDescription\nGET\n/api/v2/cmdb/\nfirewall/address\n?vdom=root&filter=type==fqdn\nRetrieve all fqdn firewall addresses,\nvdom root\nGET\n/api/v2/cmdb/\nfirewall/address\n?vdom=root&filter=\ntype==fqdn,type==iprange\nRetrieve all fqdn or iprange firewall\naddresses, vdom root\nGET\n/api/v2/cmdb/\nfirewall/address\n?vdom=root&filter=type==\nfqdn&filter=associated-\ninterface==lan\nRetrieve all fqdn firewall addresses\nthat belong to lan interface, vdom root\nRetrieve table schema\nMethod\nURL\nURL Parameters\nBody\nData\nDescription\nGET\n/api/v2/cmdb/firewall/address\n?action=schema\nRetrieve firewall address object's schema\nRetrieve table default\nMethod\nURL\nURL Parameters\nBody\nData\nDescription\nGET\n/api/v2/cmdb/firewall/address\n?action=default\nRetrieve firewall address object's default\nPurge table\nMethod\nURL\nURL Parameters\nBody\nData\nDescription\nDELETE\n/api/v2/cmdb/firewall/address\n?vdom=root\nPurge all firewall addresses, vdom root\nRetrieve object\nMethod\nURL\nURL Parameters\nBody\nData\nDescription\nGET\n/api/v2/cmdb/\nfirewall/address/address1\n?action=select&vdom=root\nRetrieve only firewall address\n'address1', vdom root\nCMDB API\n23\nFortiOS REST API Reference\nFortinet Inc.\nCreate object\nMethod\nURL\nURL\nParameters\nBody Data\nDescription\nPOST\n/api/v2/cmdb/firewall/address\n?vdom=root\n{\"name\":\"address1\"}\nCreate firewall address\n'address1', root vdom\nPOST\n/api/v2/cmdb/application/list\n?vdom=root\n{\"name\":\"profile1\"}\nCreate application list profile1,\nvdom root\nEdit object\nMethod\nURL\nURL Parameters\nBody Data\nDescription\nPUT\n/api/v2/cmdb/firewall/\naddress/address1\n?vdom=root\n{\"name\":\"address2\"}\nRename 'address1' to 'address2',\nvdom root\nPUT\n/api/v2/cmdb/firewall/\naddress/address1\n?vdom=root\n{\"comment\":\"test\ncomment\"}\nEdit 'address1' to update comment\n'test comment', vdom root\nPUT\n/api/v2/cmdb/\napplication/list/profile1\n?vdom=root\n{\"entries\":[{\"id\":1,\n\"application\":\n[{\"id\":31236},\n{\"id\":31237}]}]}\nEdit profile1 to add child object '1'\nwhich has child table 'applications',\nvdom root\nDelete object\nMethod\nURL\nURL\nParameters\nBody\nData\nDescription\nDELETE\n/api/v2/cmdb/firewall/address/address1\n?vdom=root\nDelete firewall address 'address1',\nroot vdom\nClone object\nMethod\nURL\nURL Parameters\nBody\nData\nDescription\nPOST\n/api/v2/cmdb/\nfirewall/address/address1\n?vdom=root&action=\nclone&nkey=address1_\nclone\nClone 'address1' to 'address1_clone', root\nvdom\nCMDB API\n24\nFortiOS REST API Reference\nFortinet Inc.\nMove object\nMethod\nURL\nURL Parameters\nBody\nData\nDescription\nPUT\n/api/v2/cmdb/\nfirewall/policy/1\n?vdom=root&action=\nmove&after=2\nMove policy 1 to after policy 2, root vdom\nAppend child object\nMethod\nURL\nURL Parameters\nBody\nData\nDescription\nPOST\n/api/v2/cmdb/application\n/list/profile1/entries\n?vdom=root\n{\"id\":3}\nAdd 3 to application profile1 child table\nentries, vdom root\nEdit child object\nMethod\nURL\nURL Parameters\nBody Data\nDescription\nPUT\n/api/v2/cmdb/application\n/list/profile1/entries/3\n?vdom=root\n{\"application\":\n[{\"id\":31236},\n{\"id\":31237}]}\nEdit child entry 3 to update child\napplication list, vdom root\nDelete child object\nMethod\nURL\nURL Parameters\nBody\nData\nDescription\nDELETE\n/api/v2/cmdb/application\n/list/profile1/entries/3\n?vdom=root\nDelete 3 from application profile1 child\ntable entries, vdom root\nPurge child table\nMethod\nURL\nURL Parameters\nBody\nData\nDescription\nDELETE\n/api/v2/cmdb/application\n/list/profile1/entries\n?vdom=root\nPurge application profile1 child table\nentries, vdom root\nCMDB API\n25\nFortiOS REST API Reference\nFortinet Inc.\nRetrieve complex table\nMethod\nURL\nURL Parameters\nBody\nData\nDescription\nGET\n/api/v2/cmdb/vpn.ssl/settings\n?action=select\nRetrieve vpn ssl settings object\nEdit complex table\nMethod URL\nURL Parameters\nBody Data\nDescription\nPUT\n/api/v2/cmdb/\nvpn.ssl/settings\n?vdom=root\n{\"authentication-\nrule\":[{\"id\":\"1\"},\n{\"id\":\"2\"}]}\nEdit complex object vpn.ssl.settings to\ncreate/modify child table, vdom root\nGlobal requests (apply to all accessible vdoms)\nMethod\nURL\nURL Parameters\nBody Data\nDescription\nGET\n/api/v2/cmdb/\nfirewall/address\n?global=1\nRetrieve all IPv4 firewall addresses,\nall accessible vdoms\nPOST\n/api/v2/cmdb/\nfirewall/address\n?global=1\n{\"name\":\"address1\"}\nCreate firewall address 'address1' for\nall accessible vdoms\nDELETE\n/api/v2/cmdb/firewall/\naddress/address1\n?global=1\nDelete firewall address 'address1' for\nall accessible vdoms\nCMDB API\n26\nFortiOS REST API Reference\nFortinet Inc.\nMonitor API\nMonitor API is used to perform specific actions on endpoint resources. For example, retrieve/close firewall sessions,\nrestart/shutdown FGT, backup/restore config file.\nURL path\nAll Monitor API requests start with '/api/v2/monitor/'. Below is the format of Monitor URL path:\n/api/v2/monitor/<uri>/\nEach Monitor endpoint has a specific URI, which are provided by the URI field of each endpoint.\nURI\nFull URL\nDescription\n/firewall/policy/\nGET\n/api/v2/monitor/firewall/policy/\nList traffic statistics for all IPv4 policies\n/firewall/policy/reset\nPOST\n/api/v2/monitor/firewall/policy/reset\nReset traffic statistics for all IPv4 policies\nURL parameters\nIn addition to the URL path, user can specify URL parameters which are appended to the URL path.\nGeneric parameters\nThe following URL parameters are generic to all Monitor requests.\nURL parameter\nExample\nDescription\nvdom=root\nGET /api/v2/monitor/\nfirewall/policy/?vdom=root\nReturn result/apply changes on the specified vdom. If\nvdom parameter is not provided, use current vdom\ninstead. If admin does not have access to the vdom,\nreturn permission error.\nglobal=1\nGET /api/v2/monitor/\nfirewall/policy/?global=1\nReturn a list of results/apply changes on all provisioned\nvdoms. The request is only applicable to vdoms that the\nadmin has access to.\nFortiOS REST API Reference\nFortinet Inc.\nSpecific parameters\nEach Monitor endpoint may require extra URL parameters which are unique to the endpoint. Those extra parameters\nare documented in the \"Extra Parameters\" section of each endpoint.\nRequired parameters are marked with \"required: true\" flag.\nBelow are some examples.\nURL parameter\nExample\nDescription\ncount=-1\nGET\n/api/v2/monitor/firewall/session?count=1\nReturn all ipv4 firewall sessions\nip_version=ipv6&count=10\nGET /api/v2/monitor/firewall/\nsession?ip_version=ipv6&count=10\nReturn the first 10 ipv6 firewall\nsessions\nBody data\nBeside URL parameters, some POST requests also require body data, which must be included in the HTTP body. The\nextra body data are documented in \"Extra Parameters\" section of each endpoint.\nGET requests do not accept body data.\nRequired body data are marked with \"required: true\" flag.\nBelow are some examples.\nRequest\nBody Data\nDescription\nPOST /api/v2/monitor/firewall/\nsession/close?vdom=root\n{'pro': \"udp\", 'saddr': \"192.168.100.110\",\n'daddr': \"96.45.33.73\", 'sport': 55933,\n'dport': 8888}\nClose the specific ipv4 firewall\nsessions\nFile upload\nFile upload is supported for some endpoints. For example, upload VM license, restore config file. The upload file must\nbe stored in the HTTP body. There are two different methods to do so: via JSON data or multi-part file.\nFile upload via JSON data\nThe upload file can be encoded directly into the HTTP body as JSON data using the 'file_content' field.\nThe JSON data must be encoded in base64 format.\nFor instance, below is how you can upload/restore config file via JSON data using Python Requests module.\nself.session.post(url='/api/v2/monitor/system/config/restore',\nparams={\"vdom\": \"vdom1\"},\nMonitor API\n28\nFortiOS REST API Reference\nFortinet Inc.\ndata={\"source\": \"upload\",\n\"scope\": \"vdom\",\n\"file_content\": b64encode(open(\"vd1.conf.txt\", \"r\").read())})\nFile upload via multi-part file\nAnother way to store upload file in HTTP body is to include it as a multi-part file.\nThe multi-part file does not need to be encoded in base64 format.\nFor instance, below is how you can upload/restore config file via multi-part file using Python Requests module.\nself.session.post(url='/api/v2/monitor/system/config/restore',\nparams={\"vdom\": \"vdom1\"},\ndata={\"source\": \"upload\",\n\"scope\": \"vdom\"},\nfiles=[('random_name',\n('random_conf.conf', open(\"vd1.conf.txt\", \"r\"), 'text/plain'))])\nFile download\nFile download is also supported in some endpoints. For example, download CA certificate, backup config file.\nThe downloaded file is stored in the response's raw content, not JSON data.\nFor example, here is the request to download global certificate name Fortinet_Factory, type local, scope\nglobal:\nGET /api/v2/monitor/system/certificate/download?mkey=Fortinet_Factory&type=local&scope=global\nFile download via browser\nWhen sending file download request via a browser, the browser automatically checks the response's header for\n'Content-Disposition': attachment. If present, the browser will download the file to local directory using the\nname.\nFile download via script\nWhen sending file download request via a script, the script will need to manually perform the above steps to convert the\nresponse's content into a file. For example, the script needs to check the response header for 'Content-\nDisposition': attachment, and write the content into a local file with the given name.\nMonitor API\n29\nFortiOS REST API Reference\nFortinet Inc.\nList of Methods\nURI\nHTTP Method\nSummary\nendpoint-control/profile/xml/\nGET\nList XML representation for each endpoint-control profile.\nendpoint-control/registration-\npassword/check/\nPOST\nCheck if provided registration password is valid for current\nVDOM.\nendpoint-control/record-\nlist/select/\nGET\nList endpoint records.\nendpoint-\ncontrol/registration/summary/\nGET\nSummary of FortiClient registrations.\nendpoint-\ncontrol/registration/quarantine/\nPOST\nQuarantine endpoint by FortiClient UID or MAC.\nendpoint-\ncontrol/registration/unquarantine/\nPOST\nUnquarantine endpoint by FortiClient UID or MAC.\nendpoint-\ncontrol/registration/block/\nPOST\nBlock endpoint by FortiClient UID or MAC.\nendpoint-\ncontrol/registration/unblock/\nPOST\nUnblock endpoint by FortiClient UID or MAC.\nendpoint-\ncontrol/registration/deregister/\nPOST\nDeregister endpoint by FortiClient UID or MAC.\nendpoint-control/installer/select/\nGET\nList available FortiClient installers.\nendpoint-\ncontrol/installer/download/\nGET\nDownload a FortiClient installer via FortiGuard.\nendpoint-\ncontrol/avatar/download/\nGET\nDownload an endpoint avatar image.\nfirewall/health/select/\nGET\nList configured load balance server health monitors.\nfirewall/local-in/select/\nGET\nList implicit and explicit local-in firewall policies.\nfirewall/acl/select/\nGET\nList counters for all IPv4 ACL.\nfirewall/acl/clear_counters/\nPOST\nReset counters for one or more IPv4 ACLs by policy ID.\nfirewall/acl6/select/\nGET\nList counters for all IPv6 ACL.\nfirewall/acl6/clear_counters/\nPOST\nReset counters for one or more IPv6 ACLs by policy ID.\nfirewall/internet-service-\nmatch/select/\nGET\nList internet services that exist at a given IP or Subnet.\nfirewall/policy/select/\nGET\nList traffic statistics for all IPv4 policies.\nMonitor API\n30\nFortiOS REST API Reference\nFortinet Inc.\nURI\nHTTP Method\nSummary\nfirewall/policy/reset/\nPOST\nReset traffic statistics for all IPv4 policies.\nfirewall/policy/clear_counters/\nPOST\nReset traffic statistics for one or more IPv4 policies by policy\nID.\nfirewall/policy6/select/\nGET\nList traffic statistics for all IPv6 policies.\nfirewall/policy6/reset/\nPOST\nReset traffic statistics for all IPv6 policies.\nfirewall/policy6/clear_counters/\nPOST\nReset traffic statistics for one or more IPv6 policies by policy\nID.\nfirewall/proxy-policy/select/\nGET\nList traffic statistics for all explicit proxy policies.\nfirewall/proxy-policy/clear_\ncounters/\nPOST\nReset traffic statistics for one or more explicit proxy policies by\npolicy ID.\nfirewall/policy-lookup/select/\nGET\nPerforms a policy lookup by creating a dummy packet and\nasking the kernel which policy would be hit.\nfirewall/session/select/\nGET\nList all active firewall sessions (optionally filtered).\nfirewall/session/clear_all/\nPOST\nImmediately clear all active IPv4 and IPv6 sessions and IPS\nsessions of current VDOM.\nfirewall/session/close/\nPOST\nClose a specific firewall session that matches all provided\ncriteria.\nfirewall/session-top/select/\nGET\nList of top sessions by specified grouping criteria.\nfirewall/shaper/select/\nGET\nList of statistics for configured firewall shapers.\nfirewall/shaper/reset/\nPOST\nReset statistics for all configured traffic shapers.\nfirewall/load-balance/select/\nGET\nList all firewall load balance servers.\nfirewall/address-fqdns/select/\nGET\nList of FQDN address objects and the IPs they resolved to.\nfirewall/ippool/select/\nGET\nList IPv4 pool statistics.\nfirewall/address-dynamic/select/\nGET\nList of Dynamic SDN address objects and the IPs they resolve\nto.\nfirewall/address6-dynamic/select/\nGET\nList of IPv6 Dynamic SDN address objects and the IPs they\nresolve to.\nfortiview/statistics/select/\nGET\nRetrieve drill-down and summary data for FortiView (both\nrealtime and historical).\nfortiview/session/cancel/\nPOST\nCancel a FortiView request session.\nfortiview/sandbox-file-\ndetails/select/\nGET\nRetrieve FortiSandbox analysis details for a specific file\nchecksum.\nMonitor API\n31\nFortiOS REST API Reference\nFortinet Inc.\nURI\nHTTP Method\nSummary\ngeoip/geoip-query/select/\nGET\nRetrieve location details for IPs queried against FortiGuard's\ngeoip service.\nips/rate-based/select/\nGET\nReturns a list of rate-based signatures in IPS package.\nlicense/status/select/\nGET\nGet current license & registration status.\nlicense/database/upgrade/\nPOST\nUpgrade IPS database on this device using uploaded file.\nlicense/forticare-resellers/select/\nGET\nGet current FortiCare resellers for the requested country.\nlicense/forticare-org-list/select/\nGET\nGet FortiCare organization size and industry lists.\nlog/current-disk-usage/select/\nGET\nReturn current used, free and total disk bytes.\nlog/device/state/\nGET\nRetrieve information on state of log devices.\nlog/forticloud/select/\nGET\nReturn FortiCloud log status.\nlog/fortianalyzer/select/\nGET\nReturn FortiAnalyzer/FortiManager log status.\nlog/fortianalyzer-queue/select/\nGET\nRetrieve information on FortiAnalyzer's queue state. Note:-\nFortiAnalyzer logs are queued only if upload-option is realtime.\nlog/hourly-disk-usage/select/\nGET\nReturn historic hourly disk usage in bytes.\nlog/historic-daily-remote-\nlogs/select/\nGET\nReturns the amount of logs in bytes sent daily to a remote\nlogging service (FortiCloud or FortiAnalyzer).\nlog/stats/select/\nGET\nReturn number of logs sent by category per day for a specific\nlog device.\nlog/stats/reset/\nPOST\nReset logging statistics for all log devices.\nlog/forticloud-report/download/\nGET\nDownload PDF report from FortiCloud.\nlog/ips-archive/download/\nGET\nDownload IPS/application control packet capture files. Uses\nconfigured log display device.\nlog/policy-archive/download/\nGET\nDownload policy-based packet capture archive.\nlog/av-archive/download/\nGET\nDownload file quarantined by AntiVirus.\nrouter/ipv4/select/\nGET\nList all active IPv4 routing table entries.\nrouter/ipv6/select/\nGET\nList all active IPv6 routing table entries.\nrouter/statistics/select/\nGET\nRetrieve routing table statistics, including number of matched\nroutes.\nrouter/lookup/select/\nGET\nPerforms a route lookup by querying the routing table.\nrouter/policy/select/\nGET\nRetrieve a list of active IPv4 policy routes.\nrouter/policy6/select/\nGET\nRetrieve a list of active IPv6 policy routes.\nMonitor API\n32\nFortiOS REST API Reference\nFortinet Inc.\nURI\nHTTP Method\nSummary\nsystem/admin/toggle-vdom-\nmode/\nPOST\nToggles VDOM mode on/off. Enables or disables VDOM mode\nif it is disabled or enabled respectively.\nsystem/api-user/generate-key/\nPOST\nGenerate a new api-key for the specified api-key-auth admin.\nThe old api-key will be replaced. The response contains the\nonly chance to read the new api-key plaintext in the api_key\nfield.\nsystem/config-revision/select/\nGET\nReturns a list of system configuration revisions.\nsystem/config-revision/update-\ncomments/\nPOST\nUpdates comments for a system configuration file.\nsystem/config-revision/delete/\nPOST\nDeletes one or more system configuration revisions.\nsystem/config-revision/file/\nGET\nDownload a specific configuration revision.\nsystem/config-revision/info/\nGET\nRetrieve meta information for a specific configuration revision.\nsystem/config-revision/save/\nPOST\nCreate a new config revision checkpoint.\nsystem/current-admins/select/\nGET\nReturn a list of currently logged in administrators.\nsystem/disconnect-\nadmins/select/\nPOST\nDisconnects logged in administrators.\nsystem/time/set/\nPOST\nSets current system time stamp.\nsystem/time/select/\nGET\nGets current system time stamp.\nsystem/os/reboot/\nPOST\nImmediately reboot this device.\nsystem/os/shutdown/\nPOST\nImmediately shutdown this device.\nsystem/global-resources/select/\nGET\nRetrieve current usage of global resources as well as both the\ndefault and user configured maximum values.\nsystem/vdom-resource/select/\nGET\nRetrieve VDOM resource information, including CPU and\nmemory usage.\nsystem/dhcp/select/\nGET\nReturns a list of all DHCP IPv4 and IPv6 DHCP leases.\nsystem/dhcp/revoke/\nPOST\nRevoke IPv4 DHCP leases.\nsystem/dhcp6/revoke/\nPOST\nRevoke IPv6 DHCP leases.\nsystem/firmware/select/\nGET\nRetrieve a list of firmware images available to use for upgrade\non this device.\nsystem/firmware/upgrade/\nPOST\nUpgrade firmware image on this device using uploaded file.\nsystem/firmware/upgrade-paths/\nGET\nRetrieve a list of supported firmware upgrade paths.\nMonitor API\n33\nFortiOS REST API Reference\nFortinet Inc.\nURI\nHTTP Method\nSummary\nsystem/fsck/start/\nPOST\nSet file system check flag so that it will be executed on next\ndevice reboot.\nsystem/storage/select/\nGET\nRetrieve information for the non-boot disk.\nsystem/change-password/select/\nPOST\nSave admin and guest-admin passwords.\nsystem/password-policy-\nconform/select/\nPOST\nCheck whether password conforms to the password policy.\nsystem/csf/select/\nGET\nRetrieve a full tree of downstream FortiGates registered to the\nSecurity Fabric.\nsystem/modem/select/\nGET\nRetrieve statistics for internal/external configured modem.\nsystem/modem/reset/\nPOST\nReset statistics for internal/external configured modem.\nsystem/modem/connect/\nPOST\nTrigger a connect for the configured modem.\nsystem/modem/disconnect/\nPOST\nTrigger a disconnect for the configured modem.\nsystem/modem/update/\nPOST\nUpdate supported modem list from FortiGuard.\nsystem/3g-modem/select/\nGET\nList all 3G modems available via FortiGuard.\nsystem/resource/usage/\nGET\nRetreive current and historical usage data for a provided\nresource.\nsystem/sniffer/select/\nGET\nReturn a list of all configured packet captures.\nsystem/sniffer/restart/\nPOST\nRestart specified packet capture.\nsystem/sniffer/start/\nPOST\nStart specified packet capture.\nsystem/sniffer/stop/\nPOST\nStop specified packet capture.\nsystem/sniffer/download/\nGET\nDownload a stored packet capture.\nsystem/fsw/select/\nGET\nRetrieve statistics for configured FortiSwitches\nsystem/fsw/update/\nPOST\nUpdate administrative state for a given FortiSwitch (enable or\ndisable authorization).\nsystem/fsw/restart/\nPOST\nRestart a given FortiSwitch.\nsystem/fsw/upgrade/\nPOST\nUpgrade firmware image on the given FortiSwitch using\nuploaded file.\nsystem/fsw/poe-reset/\nPOST\nReset PoE on a given FortiSwitch's port.\nsystem/fsw-firmware/select/\nGET\nRetrieve a list of recommended firmware for managed\nFortiSwitches.\nswitch-controller/managed-\nswitch/dhcp-snooping/\nGET\nRetrieve DHCP servers monitored by FortiSwitches.\nMonitor API\n34\nFortiOS REST API Reference\nFortinet Inc.\nURI\nHTTP Method\nSummary\nswitch-controller/managed-\nswitch/faceplate-xml/\nGET\nRetrieve XML for rendering FortiSwitch faceplate widget.\nswitch-controller/validate-switch-\nprefix/select/\nGET\nValidate a FortiSwitch serial number prefix.\nsystem/interface/select/\nGET\nRetrieve statistics for all system interfaces.\nsystem/available-\ninterfaces/select/\nGET\nRetrieve a list of all interfaces along with some meta\ninformation regarding their availability.\nsystem/acquired-dns/select/\nGET\nRetrieve a list of interfaces and their acquired DNS servers.\nsystem/resolve-fqdn/select/\nGET\nResolves the provided FQDNs to FQDN -> IP mappings.\nsystem/usb-log/select/\nGET\nRetrieve information about connected USB drives, including\nestimated log sizes.\nsystem/usb-log/start/\nPOST\nStart backup of logs from current VDOM to USB drive.\nsystem/usb-log/stop/\nPOST\nStop backup of logs to USB drive.\nsystem/ipconf/select/\nGET\nDetermine if there is an IP conflict for a specific IP using ARP.\nsystem/fortiguard/update/\nPOST\nImmediately update status for FortiGuard services.\nsystem/fortiguard/clear-cache/\nPOST\nImmediately clear all FortiGuard statistics.\nsystem/fortiguard/test-\navailability/\nPOST\nTest availability of FortiGuard services.\nsystem/fortiguard/server-info/\nGET\nGet FortiGuard server list and information.\nsystem/fortimanager/status/\nGET\nGet FortiManager status.\nsystem/fortimanager/config/\nPOST\nConfigure FortiManager address.\nsystem/available-\ncertificates/select/\nGET\nGet available certificates.\nsystem/certificate/download/\nGET\nDownload certificate.\nsystem/debug/select/\nPOST\nLog debug messages to the console (if enabled).\nsystem/debug/download/\nGET\nDownload debug report for technical support.\nsystem/com-log/dump/\nPOST\nDump system com-log to file.\nsystem/com-log/update/\nGET\nFetch system com-log file dump progress.\nsystem/com-log/download/\nGET\nDownload com-log file (after file dump is complete).\nsystem/botnet/stat/\nGET\nRetrieve statistics for FortiGuard botnet database.\nMonitor API\n35\nFortiOS REST API Reference\nFortinet Inc.\nURI\nHTTP Method\nSummary\nsystem/botnet/select/\nGET\nList all known IP-based botnet entries in FortiGuard botnet\ndatabase.\nsystem/botnet-domains/select/\nGET\nList all known domain-based botnet entries in FortiGuard\nbotnet database.\nsystem/botnet-domains/stat/\nGET\nList statistics on domain-based botnet entries in FortiGuard\nbotnet database.\nsystem/ha-statistics/select/\nGET\nList of statistics for members of HA cluster.\nsystem/ha-history/select/\nGET\nGet HA cluster historical logs.\nsystem/ha-checksums/select/\nGET\nList of checksums for members of HA cluster.\nsystem/ha-peer/select/\nGET\nGet configuration of peer(s) in HA cluster. Uptime is expressed\nin seconds.\nsystem/ha-peer/update/\nPOST\nUpdate configuration of peer in HA cluster.\nsystem/ha-peer/disconnect/\nPOST\nUpdate configuration of peer in HA cluster.\nsystem/link-monitor/select/\nGET\nRetrieve per-interface statistics for active link monitors.\nsystem/compliance/run/\nPOST\nImmediately run compliance checks for the selected VDOM.\nsystem/config/restore/\nPOST\nRestore system configuration from uploaded file or from USB.\nsystem/config/backup/\nGET\nBackup system config\nsystem/config/usb-filelist/\nGET\nList configuration files available on connected USB drive.\nsystem/sandbox/status/\nGET\nRetrieve sandbox status.\nsystem/sandbox/stats/\nGET\nRetrieve sandbox statistics.\nsystem/object/usage/\nGET\nRetrieve all objects that are currently using as well as objects\nthat can use the given object.\nsystem/timezone/select/\nGET\nGet world timezone and daylight saving time.\nsystem/vmlicense/upload/\nPOST\nUpdate VM license using uploaded file. Reboots immediately if\nsuccessful.\nsystem/sensor-info/select/\nGET\nRetrieve system sensor status.\nsystem/audit/select/\nGET\nRetrieve Security Fabric audit results.\nsystem/fortiguard-\nblacklist/select/\nGET\nRetrieve blacklist information for a specified IP.\nvpn-certificate/ca/import/\nPOST\nImport CA certificate.\nvpn-certificate/crl/import/\nPOST\nImport certificate revocation lists (CRL) from file content.\nMonitor API\n36\nFortiOS REST API Reference\nFortinet Inc.\nURI\nHTTP Method\nSummary\nvpn-certificate/local/import/\nPOST\nImport local certificate.\nvpn-certificate/remote/import/\nPOST\nImport remote certificate.\nvpn-certificate/csr/generate/\nPOST\nGenerate a certificate signing request (CSR) and a private key.\nThe CSR can be retrieved / downloaded from CLI, GUI and\nREST API.\nsystem/check-port-\navailability/select/\nGET\nCheck whether a list of TCP port ranges is available for a\ncertain service.\nextender-\ncontroller/extender/select/\nGET\nRetrieve statistics for specific configured FortiExtender units.\nextender-\ncontroller/extender/reset/\nPOST\nReset a specific FortiExtender unit.\nuser/firewall/select/\nGET\nList authenticated firewall users.\nuser/firewall/deauth/\nPOST\nDeauthenticate single, multiple, or all firewall users.\nuser/banned/select/\nGET\nReturn a list of all banned users by IP.\nuser/banned/clear_users/\nPOST\nImmediately clear a list of specific banned users by IP.\nuser/banned/add_users/\nPOST\nImmediately add one or more users to the banned list.\nuser/banned/clear_all/\nPOST\nImmediately clear all banned users.\nuser/fortitoken/select/\nGET\nRetrieve a map of FortiTokens and their status.\nuser/fortitoken/activate/\nPOST\nActivate a set of FortiTokens by serial number.\nuser/device/select/\nGET\nRetrieve a list of detected devices.\nuser/fortitoken/refresh/\nPOST\nRefresh a set of FortiTokens by serial number.\nuser/fortitoken/provision/\nPOST\nProvision a set of FortiTokens by serial number.\nuser/fortitoken/send-activation/\nPOST\nSend a FortiToken activation code to a user via SMS or Email.\nuser/fsso/refresh-server/\nPOST\nRefresh remote agent group list for all fsso agents.\nuser/fsso/select/\nGET\nGet a list of fsso and fsso polling status.\nutm/rating-lookup/select/\nGET\nLookup FortiGuard rating for a specific URL.\nutm/app-lookup/select/\nGET\nQuery remote FortiFlow database to resolve hosts to\napplication control entries.\nutm/application-\ncategories/select/\nGET\nRetrieve a list of application control categories.\nutm/antivirus/stats/\nGET\nRetrieve antivirus scanning statistics.\nMonitor API\n37\nFortiOS REST API Reference\nFortinet Inc.\nURI\nHTTP Method\nSummary\nvirtual-wan/health-check/select/\nGET\nRetrieve health-check statistics for each SD-WAN link.\nvirtual-wan/members/select/\nGET\nRetrieve interface statistics for each SD-WAN link.\nwebfilter/override/select/\nGET\nList all administrative and user initiated webfilter overrides.\nwebfilter/override/delete/\nPOST\nDelete a configured webfilter override.\nwebfilter/malicious-urls/select/\nGET\nList all URLs in FortiSandbox malicious URL database.\nwebfilter/malicious-urls/stat/\nGET\nRetrieve statistics for the FortiSandbox malicious URL\ndatabase.\nwebfilter/category-quota/select/\nGET\nRetrieve quota usage statistics for webfilter categories.\nwebfilter/category-quota/reset/\nPOST\nReset webfilter quota for user or IP.\nwebfilter/fortiguard-\ncategories/select/\nGET\nReturn FortiGuard web filter categories.\nwebfilter/trusted-urls/select/\nGET\nList all URLs in FortiGuard trusted URL database.\nvpn/ipsec/select/\nGET\nReturn an array of active IPsec VPNs.\nvpn/ipsec/tunnel_up/\nPOST\nBring up a specific IPsec VPN tunnel.\nvpn/ipsec/tunnel_down/\nPOST\nBring down a specific IPsec VPN tunnel.\nvpn/ipsec/tunnel_reset_stats/\nPOST\nReset statistics for a specific IPsec VPN tunnel.\nvpn/ssl/select/\nGET\nRetrieve a list of all SSL-VPN sessions and sub-sessions.\nvpn/ssl/clear_tunnel/\nPOST\nRemove all active tunnel sessions in current virtual domain.\nvpn/ssl/delete/\nPOST\nTerminate the provided SSL-VPN session.\nvpn/ssl/stats/\nGET\nReturn statistics about the SSL-VPN.\nwanopt/history/select/\nGET\nRetrieve WAN opt. statistics history.\nwanopt/history/reset/\nPOST\nReset WAN opt. statistics.\nwanopt/webcache/select/\nGET\nRetrieve webcache statistics history.\nwanopt/webcache/reset/\nPOST\nReset webcache statistics.\nwanopt/peer_stats/select/\nGET\nRetrieve a list of WAN opt peer statistics.\nwanopt/peer_stats/reset/\nPOST\nReset WAN opt peer statistics.\nwebproxy/pacfile/download/\nGET\nDownload webproxy PAC file.\nwebcache/stats/select/\nGET\nRetrieve webcache statistics.\nwebcache/stats/reset/\nPOST\nReset all webcache statistics.\nwifi/client/select/\nGET\nRetrieve a list of connected WiFi clients.\nMonitor API\n38\nFortiOS REST API Reference\nFortinet Inc.\nURI\nHTTP Method\nSummary\nwifi/managed_ap/select/\nGET\nRetrieve a list of managed FortiAPs.\nwifi/managed_ap/set_status/\nPOST\nUpdate administrative state for a given FortiAP (enable or\ndisable authorization).\nwifi/firmware/select/\nGET\nRetrieve a list of current and recommended firmware for\nFortiAPs in use.\nwifi/managed_ap/restart/\nPOST\nRestart a given FortiAP.\nwifi/managed_ap/upgrade/\nPOST\nUpgrade firmware image on the given FortiAP using uploaded\nfile.\nwifi/ap_status/select/\nGET\nRetrieve statistics for all managed FortiAPs.\nwifi/interfering_ap/select/\nGET\nRetrieve a list of interfering APs for one FortiAP radio.\nwifi/euclid/select/\nGET\nRetrieve presence analytics statistics.\nwifi/euclid/reset/\nPOST\nReset presence analytics statistics.\nwifi/rogue_ap/select/\nGET\nRetrieve a list of detected rogue APs.\nwifi/rogue_ap/clear_all/\nPOST\nClear all detected rogue APs.\nwifi/rogue_ap/set_status/\nPOST\nMark detected APs as rogue APs.\nwifi/spectrum/select/\nGET\nRetrieve spectrum analysis information for a specific FortiAP.\ncoverage/download/select/\nGET\nDownload code coverage.\nendpoint-control\nprofile: xml\nSummary\nList XML representation for each endpoint-control profile.\nURI\nendpoint-control/profile/xml/\nHTTP Method\nGET\nAction\nxml\nAccess Group\nendpoint-control-grp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nstring\nName of endpoint-control profile.\nNo\nMonitor API\n39\nFortiOS REST API Reference\nFortinet Inc.\nregistration-password: check\nSummary\nCheck if provided registration password is valid for current VDOM.\nURI\nendpoint-control/registration-password/check/\nHTTP Method\nPOST\nAction\ncheck\nAccess Group\nendpoint-control-grp\nResponse Type\nboolean\nExtra parameters\nName\nType\nSummary\nRequired\npassword\nstring\nRegistration password to test.\nYes\nrecord-list: select\nSummary\nList endpoint records.\nURI\nendpoint-control/record-list/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nendpoint-control-grp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nintf_name\nstring\nFilter: Name of interface where the endpoint was detected.\nNo\nregistration: summary\nSummary\nSummary of FortiClient registrations.\nURI\nendpoint-control/registration/summary/\nHTTP Method\nGET\nAction\nsummary\nAccess Group\nendpoint-control-grp\nMonitor API\n40\nFortiOS REST API Reference\nFortinet Inc.\nregistration: quarantine\nSummary\nQuarantine endpoint by FortiClient UID or MAC.\nURI\nendpoint-control/registration/quarantine/\nHTTP Method\nPOST\nAction\nquarantine\nAccess Group\nendpoint-control-grp\nExtra parameters\nName\nType\nSummary\nRequired\nuid\narray\nArray of FortiClient UIDs to quarantine.\nNo\nuid\nstring\nSingle FortiClient UID to quarantine.\nNo\nmac\narray\nArray of MACs to quarantine.\nNo\nmac\nstring\nSingle MAC to quarantine.\nNo\nregistration: unquarantine\nSummary\nUnquarantine endpoint by FortiClient UID or MAC.\nURI\nendpoint-control/registration/unquarantine/\nHTTP Method\nPOST\nAction\nunquarantine\nAccess Group\nendpoint-control-grp\nExtra parameters\nName\nType\nSummary\nRequired\nuid\narray\nArray of FortiClient UIDs to unquarantine.\nNo\nuid\nstring\nSingle FortiClient UID to unquarantine.\nNo\nmac\narray\nArray of MACs to unquarantine.\nNo\nmac\nstring\nSingle MAC to unquarantine.\nNo\nMonitor API\n41\nFortiOS REST API Reference\nFortinet Inc.\nregistration: block\nSummary\nBlock endpoint by FortiClient UID or MAC.\nURI\nendpoint-control/registration/block/\nHTTP Method\nPOST\nAction\nblock\nAccess Group\nendpoint-control-grp\nExtra parameters\nName\nType\nSummary\nRequired\nuid\narray\nArray of FortiClient UIDs to block.\nNo\nuid\nstring\nSingle FortiClient UID to block.\nNo\nmac\narray\nArray of MACs to block.\nNo\nmac\nstring\nSingle MAC to block.\nNo\nregistration: unblock\nSummary\nUnblock endpoint by FortiClient UID or MAC.\nURI\nendpoint-control/registration/unblock/\nHTTP Method\nPOST\nAction\nunblock\nAccess Group\nendpoint-control-grp\nExtra parameters\nName\nType\nSummary\nRequired\nuid\narray\nArray of FortiClient UIDs to unblock.\nNo\nuid\nstring\nSingle FortiClient UID to unblock.\nNo\nmac\narray\nArray of MACs to unblock.\nNo\nmac\nstring\nSingle MAC to unblock.\nNo\nMonitor API\n42\nFortiOS REST API Reference\nFortinet Inc.\nregistration: deregister\nSummary\nDeregister endpoint by FortiClient UID or MAC.\nURI\nendpoint-control/registration/deregister/\nHTTP Method\nPOST\nAction\nderegister\nAccess Group\nendpoint-control-grp\nExtra parameters\nName\nType\nSummary\nRequired\nuid\narray\nArray of FortiClient UIDs to deregister.\nNo\nuid\nstring\nSingle FortiClient UID to deregister.\nNo\nmac\narray\nArray of MACs to deregister.\nNo\nmac\nstring\nSingle MAC to deregister.\nNo\ninstaller: select\nSummary\nList available FortiClient installers.\nURI\nendpoint-control/installer/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nendpoint-control-grp\nExtra parameters\nName\nType\nSummary\nRequired\nmin_version\nstring\nFilter: Minimum installer version. (String of the format n[.n[.n]]).\nNo\ninstaller: download\nSummary\nDownload a FortiClient installer via FortiGuard.\nURI\nendpoint-control/installer/download/\nHTTP Method\nGET\nMonitor API\n43\nFortiOS REST API Reference\nFortinet Inc.\nAction\ndownload\nAccess Group\nendpoint-control-grp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nstring\nName of installer (image_id).\nYes\navatar: download\nSummary\nDownload an endpoint avatar image.\nURI\nendpoint-control/avatar/download/\nHTTP Method\nGET\nAction\ndownload\nAccess Group\nendpoint-control-grp\nETag Caching\nEnabled\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nuid\nstring\nSingle FortiClient UID.\nNo\nuser\nstring\nUser name of the endpoint.\nNo\nalias\nstring\nAlias of the device. Used to lookup device avatar when endpoint\navatar is not available.\nNo\ndefault\nstring\nDefault avatar name ['authuser'|'unauthuser'|'authuser_\n72'|'unauthuser_72']. Default avatar when endpoint / device avatar is\nnot available. If default is not set, Not found 404 is returned.\nNo\nfirewall\nhealth: select\nSummary\nList configured load balance server health monitors.\nMonitor API\n44\nFortiOS REST API Reference\nFortinet Inc.\nURI\nfirewall/health/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\narray\nlocal-in: select\nSummary\nList implicit and explicit local-in firewall policies.\nURI\nfirewall/local-in/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.policy\nResponse Type\narray\nacl: select\nSummary\nList counters for all IPv4 ACL.\nURI\nfirewall/acl/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.policy\nacl: clear_counters\nSummary\nReset counters for one or more IPv4 ACLs by policy ID.\nURI\nfirewall/acl/clear_counters/\nHTTP Method\nPOST\nAction\nclear_counters\nAccess Group\nfwgrp.policy\nMonitor API\n45\nFortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\npolicy\narray\nArray of policy IDs to reset.\nNo\npolicy\nint\nSingle policy ID to reset.\nNo\nacl6: select\nSummary\nList counters for all IPv6 ACL.\nURI\nfirewall/acl6/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.policy\nacl6: clear_counters\nSummary\nReset counters for one or more IPv6 ACLs by policy ID.\nURI\nfirewall/acl6/clear_counters/\nHTTP Method\nPOST\nAction\nclear_counters\nAccess Group\nfwgrp.policy\nExtra parameters\nName\nType\nSummary\nRequired\npolicy\narray\nArray of policy IDs to reset.\nNo\npolicy\nint\nSingle policy ID to reset.\nNo\ninternet-service-match: select\nSummary\nList internet services that exist at a given IP or Subnet.\nURI\nfirewall/internet-service-match/select/\nHTTP Method\nGET\nAction\nselect\nMonitor API\n46\nFortiOS REST API Reference\nFortinet Inc.\nAccess Group\nfwgrp.address\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nip\nstring\nIP (in dot-decimal notation).\nYes\nmask\nstring\nIP Mask (in dot-decimal notation).\nYes\npolicy: select\nSummary\nList traffic statistics for all IPv4 policies.\nURI\nfirewall/policy/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.policy\npolicy: reset\nSummary\nReset traffic statistics for all IPv4 policies.\nURI\nfirewall/policy/reset/\nHTTP Method\nPOST\nAction\nreset\nAccess Group\nfwgrp.policy\npolicy: clear_counters\nSummary\nReset traffic statistics for one or more IPv4 policies by policy ID.\nURI\nfirewall/policy/clear_counters/\nHTTP Method\nPOST\nAction\nclear_counters\nAccess Group\nfwgrp.policy\nMonitor API\n47\nFortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\npolicy\narray\nArray of policy IDs to reset.\nNo\npolicy\nint\nSingle policy ID to reset.\nNo\npolicy6: select\nSummary\nList traffic statistics for all IPv6 policies.\nURI\nfirewall/policy6/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.policy\npolicy6: reset\nSummary\nReset traffic statistics for all IPv6 policies.\nURI\nfirewall/policy6/reset/\nHTTP Method\nPOST\nAction\nreset\nAccess Group\nfwgrp.policy\npolicy6: clear_counters\nSummary\nReset traffic statistics for one or more IPv6 policies by policy ID.\nURI\nfirewall/policy6/clear_counters/\nHTTP Method\nPOST\nAction\nclear_counters\nAccess Group\nfwgrp.policy\nExtra parameters\nName\nType\nSummary\nRequired\npolicy\narray\nArray of policy IDs to reset.\nNo\npolicy\nint\nSingle policy ID to reset.\nNo\nMonitor API\n48\nFortiOS REST API Reference\nFortinet Inc.\nproxy-policy: select\nSummary\nList traffic statistics for all explicit proxy policies.\nURI\nfirewall/proxy-policy/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.policy\nproxy-policy: clear_counters\nSummary\nReset traffic statistics for one or more explicit proxy policies by policy ID.\nURI\nfirewall/proxy-policy/clear_counters/\nHTTP Method\nPOST\nAction\nclear_counters\nAccess Group\nfwgrp.policy\nExtra parameters\nName\nType\nSummary\nRequired\npolicy\narray\nArray of policy IDs to reset.\nNo\npolicy\nint\nSingle policy ID to reset.\nNo\npolicy-lookup: select\nSummary\nPerforms a policy lookup by creating a dummy packet and asking the kernel which\npolicy would be hit.\nURI\nfirewall/policy-lookup/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.policy\nResponse Type\nobject\nMonitor API\n49\nFortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nipv6\nboolean\nPerform an IPv6 lookup?\nNo\nsrcintf\nstring\nSource interface.\nYes\nsourceport\nint\nSource port.\nNo\nsourceip\nstring\nSource IP.\nNo\nprotocol\nstring\nProtocol.\nYes\ndest\nstring\nDestination IP/FQDN.\nYes\ndestport\nint\nDestination port.\nYes\nicmptype\nint\nICMP type.\nNo\nicmpcode\nint\nICMP code.\nNo\nsession: select\nSummary\nList all active firewall sessions (optionally filtered).\nURI\nfirewall/session/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nip_version\nstring\nIP version [*ipv4 | ipv6 | ipboth].\nNo\nstart\nint\nStarting entry index.\nNo\ncount\nint\nMaximum number of entries to return.\nYes\nsummary\nboolean\nEnable/disable inclusion of session summary (setup rate, total\nsessions, etc).\nNo\nsourceport\nint\nFilter: Source port.\nNo\npolicyid\nint\nFilter: Policy ID.\nNo\napplication\nint\nFilter: Application ID.\nNo\nMonitor API\n50\nFortiOS REST API Reference\nFortinet Inc.\nName\nType\nSummary\nRequired\napplication\nstring\nFilter: Application PROTO/PORT. (e.g. \"TCP/443\")\nNo\nprotocol\nint\nFilter: Protocol name [all|igmp|tcp|udp|icmp|etc].\nNo\ndestport\nint\nFilter: Destination port.\nNo\nsrcintf\nstring\nFilter: Source interface name.\nNo\ndstintf\nstring\nFilter: Destination interface name.\nNo\nsrcintfrole\narray\nFilter: Source interface roles.\nNo\ndstintfrole\narray\nFilter: Destination interface roles.\nNo\nsource\nstring\nFilter: Source IP address.\nNo\ndestination\nstring\nFilter: Destination IP address.\nNo\nusername\nstring\nFilter: Authenticated username.\nNo\nshaper\nstring\nFilter: Forward traffic shaper name.\nNo\ncountry\nstring\nFilter: Destination country name.\nNo\nnatsourceaddress\nstring\nFilter: NAT source address.\nNo\nnatsourceport\nstring\nFilter: NAT source port.\nNo\nsession: clear_all\nSummary\nImmediately clear all active IPv4 and IPv6 sessions and IPS sessions of current\nVDOM.\nURI\nfirewall/session/clear_all/\nHTTP Method\nPOST\nAction\nclear_all\nAccess Group\nsysgrp\nResponse Type\nint\nsession: close\nSummary\nClose a specific firewall session that matches all provided criteria.\nURI\nfirewall/session/close/\nHTTP Method\nPOST\nMonitor API\n51\nFortiOS REST API Reference\nFortinet Inc.\nAction\nclose\nAccess Group\nsysgrp\nExtra parameters\nName\nType\nSummary\nRequired\npro\nstring\nProtocol name [tcp|udp|icmp|...].\nYes\nsaddr\nstring\nSource address.\nYes\ndaddr\nstring\nDestination address.\nYes\nsport\nstring\nSource port.\nYes\ndport\nstring\nDestination port.\nYes\nsession-top: select\nSummary\nList of top sessions by specified grouping criteria.\nURI\nfirewall/session-top/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nreport_by\nstring\nCriteria to group results by [source*|destination|application|web-\ncategory|web-domain|srcintf|dstintf|policy|country].\nNo\nsort_by\nstring\nCriteria to sort results by [bytes|msg-counts].\nNo\ncount\nint\nMaximum number of entries to return.\nNo\nfilter\nobject\nA map of filter keys to string values. The key(s) may be srcintf,\nsource, dstintf, srcintfrole, dstintfrole, destination, policyid,\napplication, web_category_id, web_domain, country.\nNo\nshaper: select\nSummary\nList of statistics for configured firewall shapers.\nMonitor API\n52\nFortiOS REST API Reference\nFortinet Inc.\nURI\nfirewall/shaper/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.others\nResponse Type\narray\nshaper: reset\nSummary\nReset statistics for all configured traffic shapers.\nURI\nfirewall/shaper/reset/\nHTTP Method\nPOST\nAction\nreset\nAccess Group\nfwgrp.others\nload-balance: select\nSummary\nList all firewall load balance servers.\nURI\nfirewall/load-balance/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.others\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nstart\nint\nStarting entry index.\nNo\ncount\nint\nMaximum number of entries to return.\nYes\naddress-fqdns: select\nSummary\nList of FQDN address objects and the IPs they resolved to.\nURI\nfirewall/address-fqdns/select/\nMonitor API\n53\nFortiOS REST API Reference\nFortinet Inc.\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.address\nResponse Type\nobject\nippool: select\nSummary\nList IPv4 pool statistics.\nURI\nfirewall/ippool/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.policy\nResponse Type\nobject\naddress-dynamic: select\nSummary\nList of Dynamic SDN address objects and the IPs they resolve to.\nURI\nfirewall/address-dynamic/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.address\nResponse Type\nobject\naddress6-dynamic: select\nSummary\nList of IPv6 Dynamic SDN address objects and the IPs they resolve to.\nURI\nfirewall/address6-dynamic/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.address\nResponse Type\nobject\nMonitor API\n54\nFortiOS REST API Reference\nFortinet Inc.\nfortiview\nstatistics: select\nSummary\nRetrieve drill-down and summary data for FortiView (both realtime and historical).\nURI\nfortiview/statistics/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nrealtime\nboolean\nSet to true to retrieve realtime results (from kernel).\nNo\nfilter\nobject\nA map of filter keys to arrays of values.\nNo\nsession: cancel\nSummary\nCancel a FortiView request session.\nURI\nfortiview/session/cancel/\nHTTP Method\nPOST\nAction\ncancel\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nsessionid\nint\nSession ID to cancel.\nNo\ndevice\nint\nFortiView request session's device. [disk|faz]\nNo\nreport_by\nint\nReport by field.\nNo\nview_level\nint\nFortiView View level.\nNo\nMonitor API\n55\nFortiOS REST API Reference\nFortinet Inc.\nsandbox-file-details: select\nSummary\nRetrieve FortiSandbox analysis details for a specific file checksum.\nURI\nfortiview/sandbox-file-details/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nchecksum\nstring\nChecksum of a specific file that has been analyzed by the connected\nFortiSandbox.\nYes\ngeoip\ngeoip-query: select\nSummary\nRetrieve location details for IPs queried against FortiGuard's geoip service.\nURI\ngeoip/geoip-query/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nip_addresses\nstring\nOne or more IP address strings to query for location details.\nYes\nMonitor API\n56\nFortiOS REST API Reference\nFortinet Inc.\nips\nrate-based: select\nSummary\nReturns a list of rate-based signatures in IPS package.\nURI\nips/rate-based/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nutmgrp.ips\nResponse Type\narray\nlicense\nstatus: select\nSummary\nGet current license & registration status.\nURI\nlicense/status/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\ndatabase: upgrade\nSummary\nUpgrade IPS database on this device using uploaded file.\nURI\nlicense/database/upgrade/\nHTTP Method\nPOST\nAction\nupgrade\nAccess Group\nupdategrp\nResponse Type\nobject\nMonitor API\n57\nFortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\ndb_name\nstring\nSecurity service database name [ips_appctrl|antivirus|...]\nNo\nfile_content\nstring\nProvided when uploading a file: base64 encoded file data. Must not\ncontain whitespace or other invalid base64 characters. Must be\nincluded in HTTP body.\nNo\nforticare-resellers: select\nSummary\nGet current FortiCare resellers for the requested country.\nURI\nlicense/forticare-resellers/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\ncountry_code\nint\nFortiGuard country code\nNo\nforticare-org-list: select\nSummary\nGet FortiCare organization size and industry lists.\nURI\nlicense/forticare-org-list/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\nMonitor API\n58\nFortiOS REST API Reference\nFortinet Inc.\nlog\ncurrent-disk-usage: select\nSummary\nReturn current used, free and total disk bytes.\nURI\nlog/current-disk-usage/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nloggrp.data-access\ndevice: state\nSummary\nRetrieve information on state of log devices.\nURI\nlog/device/state/\nHTTP Method\nGET\nAction\nstate\nAccess Group\nloggrp.data-access\nResponse Type\nobject\nforticloud: select\nSummary\nReturn FortiCloud log status.\nURI\nlog/forticloud/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nloggrp.config\nfortianalyzer: select\nSummary\nReturn FortiAnalyzer/FortiManager log status.\nURI\nlog/fortianalyzer/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nloggrp.config\nMonitor API\n59\nFortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nserver\nstring\nFortiAnalyzer/FortiManager address.\nNo\nfortianalyzer-queue: select\nSummary\nRetrieve information on FortiAnalyzer's queue state. Note:- FortiAnalyzer logs are\nqueued only if upload-option is realtime.\nURI\nlog/fortianalyzer-queue/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nloggrp.config\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nscope\nstring\nScope from which to retrieve FortiAnalyzer's queue state\n[vdom*|global].\nNo\nhourly-disk-usage: select\nSummary\nReturn historic hourly disk usage in bytes.\nURI\nlog/hourly-disk-usage/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nloggrp.data-access\nhistoric-daily-remote-logs: select\nSummary\nReturns the amount of logs in bytes sent daily to a remote logging service\n(FortiCloud or FortiAnalyzer).\nURI\nlog/historic-daily-remote-logs/select/\nHTTP Method\nGET\nMonitor API\n60\nFortiOS REST API Reference\nFortinet Inc.\nAction\nselect\nAccess Group\nloggrp.data-access\nstats: select\nSummary\nReturn number of logs sent by category per day for a specific log device.\nURI\nlog/stats/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nloggrp.data-access\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\ndev\nstring\nLog device [*memory | disk | fortianalyzer | forticloud].\nNo\nstats: reset\nSummary\nReset logging statistics for all log devices.\nURI\nlog/stats/reset/\nHTTP Method\nPOST\nAction\nreset\nAccess Group\nloggrp.data-access\nforticloud-report: download\nSummary\nDownload PDF report from FortiCloud.\nURI\nlog/forticloud-report/download/\nHTTP Method\nGET\nAction\ndownload\nAccess Group\nloggrp.data-access\nResponse Type\nobject\nMonitor API\n61\nFortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nint\nFortiCloud Report ID.\nYes\ninline\nint\nSet to 1 to download the report inline.\nNo\nips-archive: download\nSummary\nDownload IPS/application control packet capture files. Uses configured log\ndisplay device.\nURI\nlog/ips-archive/download/\nHTTP Method\nGET\nAction\ndownload\nAccess Group\nloggrp.data-access\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nint\nIPS archive ID.\nYes\npcap_no\nint\nPacket capture roll number (required when log device is 'disk')\nNo\npcap_category\nint\nPacket capture category (required when log device is 'disk')\nNo\npolicy-archive: download\nSummary\nDownload policy-based packet capture archive.\nURI\nlog/policy-archive/download/\nHTTP Method\nGET\nAction\ndownload\nAccess Group\nloggrp.data-access\nResponse Type\nobject\nMonitor API\n62\nFortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nint\nSession ID (from traffic log).\nYes\nsrcip\nstring\nSource IP.\nYes\ndstip\nstring\nDestination IP.\nYes\nav-archive: download\nSummary\nDownload file quarantined by AntiVirus.\nURI\nlog/av-archive/download/\nHTTP Method\nGET\nAction\ndownload\nAccess Group\nloggrp.data-access\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nstring\nChecksum for quarantined file.\nYes\nrouter\nipv4: select\nSummary\nList all active IPv4 routing table entries.\nURI\nrouter/ipv4/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\narray\nMonitor API\n63\nFortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nstart\nint\nStarting entry index.\nNo\ncount\nint\nMaximum number of entries to return (Default for all routes).\nNo\nip_mask\nstring\nFilter: IP/netmask.\nNo\ngateway\nstring\nFilter: gateway.\nNo\ntype\nstring\nFilter: route type.\nNo\ninterface\nstring\nFilter: interface name.\nNo\nipv6: select\nSummary\nList all active IPv6 routing table entries.\nURI\nrouter/ipv6/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nstart\nint\nStarting entry index.\nNo\ncount\nint\nMaximum number of entries to return (Default for all routes).\nNo\nip_mask\nstring\nFilter: IP/netmask.\nNo\ngateway\nstring\nFilter: gateway.\nNo\ntype\nstring\nFilter: route type.\nNo\ninterface\nstring\nFilter: interface name.\nNo\nstatistics: select\nSummary\nRetrieve routing table statistics, including number of matched routes.\nURI\nrouter/statistics/select/\nMonitor API\n64\nFortiOS REST API Reference\nFortinet Inc.\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nip_version\nint\nIP version (4|6). If not present, IPv4 and IPv6 will be returned.\nNo\nip_mask\nstring\nFilter: IP/netmask.\nNo\ngateway\nstring\nFilter: gateway.\nNo\ntype\nstring\nFilter: route type.\nNo\ninterface\nstring\nFilter: interface name.\nNo\nlookup: select\nSummary\nPerforms a route lookup by querying the routing table.\nURI\nrouter/lookup/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nroutegrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nipv6\nboolean\nPerform an IPv6 lookup?\nNo\ndestination\nstring\nDestination IP/FQDN\nYes\npolicy: select\nSummary\nRetrieve a list of active IPv4 policy routes.\nURI\nrouter/policy/select/\nMonitor API\n65\nFortiOS REST API Reference\nFortinet Inc.\nHTTP Method\nGET\nAction\nselect\nAccess Group\nroutegrp\nResponse Type\narray\npolicy6: select\nSummary\nRetrieve a list of active IPv6 policy routes.\nURI\nrouter/policy6/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nroutegrp\nResponse Type\narray\nsystem\nadmin: toggle-vdom-mode\nSummary\nToggles VDOM mode on/off. Enables or disables VDOM mode if it is disabled or\nenabled respectively.\nURI\nsystem/admin/toggle-vdom-mode/\nHTTP Method\nPOST\nAction\ntoggle-vdom-mode\nAccess Group\nsysgrp\nResponse Type\nobject\napi-user: generate-key\nSummary\nGenerate a new api-key for the specified api-key-auth admin. The old api-key will\nbe replaced. The response contains the only chance to read the new api-key\nplaintext in the api_key field.\nURI\nsystem/api-user/generate-key/\nHTTP Method\nPOST\nMonitor API\n66\nFortiOS REST API Reference\nFortinet Inc.\nAction\ngenerate-key\nAccess Group\nadmingrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\napi-user\nstring\nGenerate a new token for this api-user.\nYes\nconfig-revision: select\nSummary\nReturns a list of system configuration revisions.\nURI\nsystem/config-revision/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\nconfig-revision: update-comments\nSummary\nUpdates comments for a system configuration file.\nURI\nsystem/config-revision/update-comments/\nHTTP Method\nPOST\nAction\nupdate-comments\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nconfig_id\nint\nConfiguration id.\nNo\ncomments\nstring\nConfiguration comments.\nNo\nMonitor API\n67\nFortiOS REST API Reference\nFortinet Inc.\nconfig-revision: delete\nSummary\nDeletes one or more system configuration revisions.\nURI\nsystem/config-revision/delete/\nHTTP Method\nPOST\nAction\ndelete\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nconfig_ids\narray\nList of configuration ids.\nYes\nconfig-revision: file\nSummary\nDownload a specific configuration revision.\nURI\nsystem/config-revision/file/\nHTTP Method\nGET\nAction\nfile\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nconfig_id\nint\nConfiguration id.\nNo\nconfig-revision: info\nSummary\nRetrieve meta information for a specific configuration revision.\nURI\nsystem/config-revision/info/\nHTTP Method\nGET\nAction\ninfo\nMonitor API\n68\nFortiOS REST API Reference\nFortinet Inc.\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nconfig_id\nint\nConfiguration id.\nNo\nconfig-revision: save\nSummary\nCreate a new config revision checkpoint.\nURI\nsystem/config-revision/save/\nHTTP Method\nPOST\nAction\nsave\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\ncomments\nstring\nOptional revision comments\nNo\ncurrent-admins: select\nSummary\nReturn a list of currently logged in administrators.\nURI\nsystem/current-admins/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\narray\ndisconnect-admins: select\nSummary\nDisconnects logged in administrators.\nMonitor API\n69\nFortiOS REST API Reference\nFortinet Inc.\nURI\nsystem/disconnect-admins/select/\nHTTP Method\nPOST\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nid\nint\nAdmin ID\nNo\nmethod\nstring\nLogin method used to connect admin to FortiGate.\nNo\nadmins\narray\nList of objects with admin id and method.\nNo\ntime: set\nSummary\nSets current system time stamp.\nURI\nsystem/time/set/\nHTTP Method\nPOST\nAction\nset\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nyear\nint\nSpecifies the year for setting/updating time manually.\nYes\nmonth\nint\nSpecifies the month (0 - 11) for setting/updating time manually.\nYes\nday\nint\nSpecifies the day for setting/updating time manually.\nYes\nhour\nint\nSpecifies the hour (0 - 23) for setting/updating time manually.\nYes\nminute\nint\nSpecifies the minute (0 - 59) for setting/updating time manually.\nYes\nsecond\nint\nSpecifies the second (0 - 59) for setting/updating time manually.\nYes\nMonitor API\n70\nFortiOS REST API Reference\nFortinet Inc.\ntime: select\nSummary\nGets current system time stamp.\nURI\nsystem/time/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\nos: reboot\nSummary\nImmediately reboot this device.\nURI\nsystem/os/reboot/\nHTTP Method\nPOST\nAction\nreboot\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nevent_log_\nmessage\nstring\nMessage to be logged in event log.\nNo\nos: shutdown\nSummary\nImmediately shutdown this device.\nURI\nsystem/os/shutdown/\nHTTP Method\nPOST\nAction\nshutdown\nAccess Group\nsysgrp\nResponse Type\nobject\nMonitor API\n71\nFortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nevent_log_\nmessage\nstring\nMessage to be logged in event log.\nNo\nglobal-resources: select\nSummary\nRetrieve current usage of global resources as well as both the default and user\nconfigured maximum values.\nURI\nsystem/global-resources/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nvdom-resource: select\nSummary\nRetrieve VDOM resource information, including CPU and memory usage.\nURI\nsystem/vdom-resource/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\ndhcp: select\nSummary\nReturns a list of all DHCP IPv4 and IPv6 DHCP leases.\nURI\nsystem/dhcp/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nnetgrp\nResponse Type\narray\nMonitor API\n72\nFortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nscope\nstring\nScope from which to retrieve DHCP leases [vdom*|global]. Global\nscope is only accessible for global administrators.\nNo\nipv6\nboolean\nInclude IPv6 addresses in the response.\nNo\ndhcp: revoke\nSummary\nRevoke IPv4 DHCP leases.\nURI\nsystem/dhcp/revoke/\nHTTP Method\nPOST\nAction\nrevoke\nAccess Group\nnetgrp\nExtra parameters\nName\nType\nSummary\nRequired\nip\narray\nOptional list of addresses to revoke. Defaults to all addresses if not\nprovided.\nNo\ndhcp6: revoke\nSummary\nRevoke IPv6 DHCP leases.\nURI\nsystem/dhcp6/revoke/\nHTTP Method\nPOST\nAction\nrevoke\nAccess Group\nnetgrp\nExtra parameters\nName\nType\nSummary\nRequired\nip\narray\nOptional list of addresses to revoke. Defaults to all addresses if not\nprovided.\nNo\nMonitor API\n73\nFortiOS REST API Reference\nFortinet Inc.\nfirmware: select\nSummary\nRetrieve a list of firmware images available to use for upgrade on this device.\nURI\nsystem/firmware/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nmntgrp\nfirmware: upgrade\nSummary\nUpgrade firmware image on this device using uploaded file.\nURI\nsystem/firmware/upgrade/\nHTTP Method\nPOST\nAction\nupgrade\nAccess Group\nmntgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nsource\nstring\nFirmware file data source [upload|usb|fortiguard].\nYes\nfilename\nstring\nName of file on fortiguard or USB disk to upgrade to.\nNo\nformat_\npartition\nboolean\nSet to true to format boot partition before upgrade.\nNo\nfile_content\nstring\nProvided when uploading a file: base64 encoded file data. Must not\ncontain whitespace or other invalid base64 characters. Must be\nincluded in HTTP body.\nNo\nfirmware: upgrade-paths\nSummary\nRetrieve a list of supported firmware upgrade paths.\nURI\nsystem/firmware/upgrade-paths/\nHTTP Method\nGET\nAction\nupgrade-paths\nAccess Group\nmntgrp\nMonitor API\n74\nFortiOS REST API Reference\nFortinet Inc.\nfsck: start\nSummary\nSet file system check flag so that it will be executed on next device reboot.\nURI\nsystem/fsck/start/\nHTTP Method\nPOST\nAction\nstart\nAccess Group\nsysgrp\nstorage: select\nSummary\nRetrieve information for the non-boot disk.\nURI\nsystem/storage/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nchange-password: select\nSummary\nSave admin and guest-admin passwords.\nURI\nsystem/change-password/select/\nHTTP Method\nPOST\nAction\nselect\nAccess Group\nany\npassword-policy-conform: select\nSummary\nCheck whether password conforms to the password policy.\nURI\nsystem/password-policy-conform/select/\nHTTP Method\nPOST\nAction\nselect\nAccess Group\nany\nMonitor API\n75\nFortiOS REST API Reference\nFortinet Inc.\ncsf: select\nSummary\nRetrieve a full tree of downstream FortiGates registered to the Security Fabric.\nURI\nsystem/csf/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nETag Caching\nEnabled\nResponse Type\nobject\nmodem: select\nSummary\nRetrieve statistics for internal/external configured modem.\nURI\nsystem/modem/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nmodem: reset\nSummary\nReset statistics for internal/external configured modem.\nURI\nsystem/modem/reset/\nHTTP Method\nPOST\nAction\nreset\nAccess Group\nsysgrp\nmodem: connect\nSummary\nTrigger a connect for the configured modem.\nURI\nsystem/modem/connect/\nHTTP Method\nPOST\nAction\nconnect\nAccess Group\nsysgrp\nMonitor API\n76\nFortiOS REST API Reference\nFortinet Inc.\nmodem: disconnect\nSummary\nTrigger a disconnect for the configured modem.\nURI\nsystem/modem/disconnect/\nHTTP Method\nPOST\nAction\ndisconnect\nAccess Group\nsysgrp\nmodem: update\nSummary\nUpdate supported modem list from FortiGuard.\nURI\nsystem/modem/update/\nHTTP Method\nPOST\nAction\nupdate\nAccess Group\nsysgrp\n3g-modem: select\nSummary\nList all 3G modems available via FortiGuard.\nURI\nsystem/3g-modem/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nresource: usage\nSummary\nRetreive current and historical usage data for a provided resource.\nURI\nsystem/resource/usage/\nHTTP Method\nGET\nAction\nusage\nAccess Group\nsysgrp\nResponse Type\nobject\nMonitor API\n77\nFortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nresource\nstring\nResource to get usage data for [cpu|memory|disk|sessions|lograte].\nDefaults to all resources if not provided.\nNo\ninterval\nstring\nTime interval of resource usage [1-min|10-min|30-min|1-hour|12-\nhour|24-hour]. Defaults to all intervals if not provided.\nNo\nsniffer: select\nSummary\nReturn a list of all configured packet captures.\nURI\nsystem/sniffer/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.packet-capture\nResponse Type\narray\nsniffer: restart\nSummary\nRestart specified packet capture.\nURI\nsystem/sniffer/restart/\nHTTP Method\nPOST\nAction\nrestart\nAccess Group\nfwgrp.packet-capture\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nint\nID of packet capture entry.\nYes\nsniffer: start\nSummary\nStart specified packet capture.\nURI\nsystem/sniffer/start/\nMonitor API\n78\nFortiOS REST API Reference\nFortinet Inc.\nHTTP Method\nPOST\nAction\nstart\nAccess Group\nfwgrp.packet-capture\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nint\nID of packet capture entry.\nYes\nsniffer: stop\nSummary\nStop specified packet capture.\nURI\nsystem/sniffer/stop/\nHTTP Method\nPOST\nAction\nstop\nAccess Group\nfwgrp.packet-capture\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nint\nID of packet capture entry.\nYes\nsniffer: download\nSummary\nDownload a stored packet capture.\nURI\nsystem/sniffer/download/\nHTTP Method\nGET\nAction\ndownload\nAccess Group\nfwgrp.packet-capture\nResponse Type\nobject\nMonitor API\n79\nFortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nint\nID of packet capture entry.\nYes\nfsw: select\nSummary\nRetrieve statistics for configured FortiSwitches\nURI\nsystem/fsw/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nfsw_id\nstring\nFilter: FortiSwitch ID.\nNo\npoe\nboolean\nFilter: Retrieve PoE statistics for ports of configured FortiSwitches.\nPort power usage is in Watt units.\nNo\nport_stats\nboolean\nFilter: Retrieve tx/rx statistics for ports of configured FortiSwitches.\nNo\nfsw: update\nSummary\nUpdate administrative state for a given FortiSwitch (enable or disable\nauthorization).\nURI\nsystem/fsw/update/\nHTTP Method\nPOST\nAction\nupdate\nAccess Group\nsysgrp\nExtra parameters\nName\nType\nSummary\nRequired\nfswname\nstring\nFortiSwitch name.\nNo\nadmin\nstring\nNew FortiSwitch administrative state [enable|disable|discovered].\nNo\nMonitor API\n80\nFortiOS REST API Reference\nFortinet Inc.\nfsw: restart\nSummary\nRestart a given FortiSwitch.\nURI\nsystem/fsw/restart/\nHTTP Method\nPOST\nAction\nrestart\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nstring\nName of managed FortiSwitch.\nYes\nfsw: upgrade\nSummary\nUpgrade firmware image on the given FortiSwitch using uploaded file.\nURI\nsystem/fsw/upgrade/\nHTTP Method\nPOST\nAction\nupgrade\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nstring\nName of managed FortiSwitch.\nYes\nsource\nstring\nFirmware file data source [upload|fortiguard].\nYes\nfile_content\nstring\nProvided when uploading a file: base64 encoded file data. Must not\ncontain whitespace or other invalid base64 characters. Must be\nincluded in HTTP body.\nNo\nfsw: poe-reset\nSummary\nReset PoE on a given FortiSwitch's port.\nMonitor API\n81\nFortiOS REST API Reference\nFortinet Inc.\nURI\nsystem/fsw/poe-reset/\nHTTP Method\nPOST\nAction\npoe-reset\nAccess Group\nsysgrp\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nstring\nName of managed FortiSwitch.\nYes\nport\nstring\nName of port to reset PoE on.\nYes\nfsw-firmware: select\nSummary\nRetrieve a list of recommended firmware for managed FortiSwitches.\nURI\nsystem/fsw-firmware/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nstring\nFilter: FortiSwitch ID.\nNo\ntimeout\nstring\nFortiGuard connection timeout (defaults to 3 seconds).\nNo\nswitch-controller\nmanaged-switch: dhcp-snooping\nSummary\nRetrieve DHCP servers monitored by FortiSwitches.\nURI\nswitch-controller/managed-switch/dhcp-snooping/\nHTTP Method\nGET\nAction\ndhcp-snooping\nMonitor API\n82\nFortiOS REST API Reference\nFortinet Inc.\nAccess Group\nwifi\nResponse Type\narray\nmanaged-switch: faceplate-xml\nSummary\nRetrieve XML for rendering FortiSwitch faceplate widget.\nURI\nswitch-controller/managed-switch/faceplate-xml/\nHTTP Method\nGET\nAction\nfaceplate-xml\nAccess Group\nwifi\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nstring\nName of managed FortiSwitch.\nNo\nvalidate-switch-prefix: select\nSummary\nValidate a FortiSwitch serial number prefix.\nURI\nswitch-controller/validate-switch-prefix/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nwifi\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nprefix\nstring\nPrefix of FortiSwitch serial number.\nNo\ninterface: select\nSummary\nRetrieve statistics for all system interfaces.\nMonitor API\n83\nFortiOS REST API Reference\nFortinet Inc.\nURI\nsystem/interface/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nnetgrp\nETag Caching\nEnabled\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\ninterface_\nname\nstring\nFilter: interface name.\nNo\ninclude_vlan\nboolean\nEnable to include VLANs in result list.\nNo\navailable-interfaces: select\nSummary\nRetrieve a list of all interfaces along with some meta information regarding their\navailability.\nURI\nsystem/available-interfaces/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nany\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nview_type\nstring\nOptionally include additional information for interfaces. This\nparameter can be repeated multiple times. 'poe': Includes PoE\ninformation for supported ports. 'ha': Includes extra meta information\nuseful when dealing with interfaces related to HA configuration.\nInterfaces that are used by an HA cluster as management interfaces\nare also included in this view. 'zone': Includes extra meta information\nfor determining zone membership eligibility.\nNo\nMonitor API\n84\nFortiOS REST API Reference\nFortinet Inc.\nacquired-dns: select\nSummary\nRetrieve a list of interfaces and their acquired DNS servers.\nURI\nsystem/acquired-dns/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nany\nResponse Type\narray\nresolve-fqdn: select\nSummary\nResolves the provided FQDNs to FQDN -> IP mappings.\nURI\nsystem/resolve-fqdn/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nany\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nipv6\nboolean\nResolve for the AAAA record?\nNo\nfqdn\nstring\nFQDN\nYes\nfqdn\narray\nList of FQDNs to be resolved\nNo\nusb-log: select\nSummary\nRetrieve information about connected USB drives, including estimated log sizes.\nURI\nsystem/usb-log/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nMonitor API\n85\nFortiOS REST API Reference\nFortinet Inc.\nusb-log: start\nSummary\nStart backup of logs from current VDOM to USB drive.\nURI\nsystem/usb-log/start/\nHTTP Method\nPOST\nAction\nstart\nAccess Group\nsysgrp\nusb-log: stop\nSummary\nStop backup of logs to USB drive.\nURI\nsystem/usb-log/stop/\nHTTP Method\nPOST\nAction\nstop\nAccess Group\nsysgrp\nipconf: select\nSummary\nDetermine if there is an IP conflict for a specific IP using ARP.\nURI\nsystem/ipconf/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nnetgrp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\ndev\nobject\nList of interfaces to check for conflict.\nNo\nipaddr\nstring\nIPv4 address to check for conflict.\nNo\nfortiguard: update\nSummary\nImmediately update status for FortiGuard services.\nMonitor API\n86\nFortiOS REST API Reference\nFortinet Inc.\nURI\nsystem/fortiguard/update/\nHTTP Method\nPOST\nAction\nupdate\nAccess Group\nsysgrp\nfortiguard: clear-cache\nSummary\nImmediately clear all FortiGuard statistics.\nURI\nsystem/fortiguard/clear-cache/\nHTTP Method\nPOST\nAction\nclear-cache\nAccess Group\nsysgrp\nfortiguard: test-availability\nSummary\nTest availability of FortiGuard services.\nURI\nsystem/fortiguard/test-availability/\nHTTP Method\nPOST\nAction\ntest-availability\nAccess Group\nsysgrp\nfortiguard: server-info\nSummary\nGet FortiGuard server list and information.\nURI\nsystem/fortiguard/server-info/\nHTTP Method\nGET\nAction\nserver-info\nAccess Group\nsysgrp\nfortimanager: status\nSummary\nGet FortiManager status.\nMonitor API\n87\nFortiOS REST API Reference\nFortinet Inc.\nURI\nsystem/fortimanager/status/\nHTTP Method\nGET\nAction\nstatus\nAccess Group\nsysgrp\nfortimanager: config\nSummary\nConfigure FortiManager address.\nURI\nsystem/fortimanager/config/\nHTTP Method\nPOST\nAction\nconfig\nAccess Group\nsysgrp\navailable-certificates: select\nSummary\nGet available certificates.\nURI\nsystem/available-certificates/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nany\nExtra parameters\nName\nType\nSummary\nRequired\nscope\nstring\nScope of certificate [vdom*|global].\nNo\ncertificate: download\nSummary\nDownload certificate.\nURI\nsystem/certificate/download/\nHTTP Method\nGET\nAction\ndownload\nAccess Group\nvpngrp\nResponse Type\nobject\nMonitor API\n88\nFortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nstring\nName of certificate.\nYes\ntype\nstring\nType of certificate [local|csr|remote|ca|crl].\nYes\nscope\nstring\nScope of certificate [vdom*|global].\nNo\ndebug: select\nSummary\nLog debug messages to the console (if enabled).\nURI\nsystem/debug/select/\nHTTP Method\nPOST\nAction\nselect\nAccess Group\nany\nExtra parameters\nName\nType\nSummary\nRequired\ntype\nstring\nType of message.\nYes\nmsg\nstring\nMessage content.\nYes\nfile\nstring\nFile name generating message.\nYes\nline\nstring\nLine number in file.\nYes\ndebug: download\nSummary\nDownload debug report for technical support.\nURI\nsystem/debug/download/\nHTTP Method\nGET\nAction\ndownload\nAccess Group\nmntgrp\nResponse Type\nobject\nMonitor API\n89\nFortiOS REST API Reference\nFortinet Inc.\ncom-log: dump\nSummary\nDump system com-log to file.\nURI\nsystem/com-log/dump/\nHTTP Method\nPOST\nAction\ndump\nAccess Group\nsysgrp\ncom-log: update\nSummary\nFetch system com-log file dump progress.\nURI\nsystem/com-log/update/\nHTTP Method\nGET\nAction\nupdate\nAccess Group\nsysgrp\ncom-log: download\nSummary\nDownload com-log file (after file dump is complete).\nURI\nsystem/com-log/download/\nHTTP Method\nGET\nAction\ndownload\nAccess Group\nsysgrp\nResponse Type\nobject\nbotnet: stat\nSummary\nRetrieve statistics for FortiGuard botnet database.\nURI\nsystem/botnet/stat/\nHTTP Method\nGET\nAction\nstat\nAccess Group\nsysgrp\nMonitor API\n90\nFortiOS REST API Reference\nFortinet Inc.\nETag Caching\nEnabled\nResponse Type\nobject\nbotnet: select\nSummary\nList all known IP-based botnet entries in FortiGuard botnet database.\nURI\nsystem/botnet/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nETag Caching\nEnabled\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nstart\nint\nStarting entry index.\nNo\ncount\nint\nMaximum number of entries to return.\nNo\nbotnet-domains: select\nSummary\nList all known domain-based botnet entries in FortiGuard botnet database.\nURI\nsystem/botnet-domains/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nETag Caching\nEnabled\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nstart\nint\nStarting entry index.\nNo\ncount\nint\nMaximum number of entries to return.\nNo\nMonitor API\n91\nFortiOS REST API Reference\nFortinet Inc.\nbotnet-domains: stat\nSummary\nList statistics on domain-based botnet entries in FortiGuard botnet database.\nURI\nsystem/botnet-domains/stat/\nHTTP Method\nGET\nAction\nstat\nAccess Group\nsysgrp\nETag Caching\nEnabled\nResponse Type\nobject\nha-statistics: select\nSummary\nList of statistics for members of HA cluster.\nURI\nsystem/ha-statistics/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\narray\nha-history: select\nSummary\nGet HA cluster historical logs.\nURI\nsystem/ha-history/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nETag Caching\nEnabled\nResponse Type\nobject\nha-checksums: select\nSummary\nList of checksums for members of HA cluster.\nMonitor API\n92\nFortiOS REST API Reference\nFortinet Inc.\nURI\nsystem/ha-checksums/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nETag Caching\nEnabled\nResponse Type\narray\nha-peer: select\nSummary\nGet configuration of peer(s) in HA cluster. Uptime is expressed in seconds.\nURI\nsystem/ha-peer/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nserial_no\nstring\nSerial number of the HA member. If not specified, fetch information\nfor all HA members\nNo\nvcluster_id\nint\nVirtual cluster number. If not specified, fetch information for all active\nvclusters\nNo\nha-peer: update\nSummary\nUpdate configuration of peer in HA cluster.\nURI\nsystem/ha-peer/update/\nHTTP Method\nPOST\nAction\nupdate\nAccess Group\nsysgrp\nResponse Type\nobject\nMonitor API\n93\nFortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nserial_no\nstring\nSerial number of the HA member.\nYes\nvcluster_id\nint\nVirtual cluster number.\nNo\npriority\nint\nPriority to assign to HA member.\nNo\nhostname\nstring\nName to assign the HA member.\nNo\nha-peer: disconnect\nSummary\nUpdate configuration of peer in HA cluster.\nURI\nsystem/ha-peer/disconnect/\nHTTP Method\nPOST\nAction\ndisconnect\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nserial_no\nstring\nSerial number of the HA member.\nYes\ninterface\nstring\nName of the interface which should be assigned for management.\nYes\nip\nstring\nIP to assign to the selected interface.\nYes\nmask\nstring\nFull network mask to assign to the selected interface.\nYes\nlink-monitor: select\nSummary\nRetrieve per-interface statistics for active link monitors.\nURI\nsystem/link-monitor/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nMonitor API\n94\nFortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nstring\nName of link monitor.\nNo\ncompliance: run\nSummary\nImmediately run compliance checks for the selected VDOM.\nURI\nsystem/compliance/run/\nHTTP Method\nPOST\nAction\nrun\nAccess Group\nsysgrp\nconfig: restore\nSummary\nRestore system configuration from uploaded file or from USB.\nURI\nsystem/config/restore/\nHTTP Method\nPOST\nAction\nrestore\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nsource\nstring\nConfiguration file data source [upload | usb | revision].\nYes\nusb_filename\nstring\nWhen using 'usb' source: the filename to restore from the connected\nUSB device.\nNo\nconfig_id\nint\nWhen using 'revision' source: valid ID of configuration stored on disk\nto revert to.\nNo\npassword\nstring\nPassword to decrypt configuration data.\nNo\nscope\nstring\nSpecify global or VDOM only restore [global | vdom].\nYes\nvdom\nstring\nIf 'vdom' scope specified, the name of the VDOM to restore\nconfiguration.\nNo\nMonitor API\n95\nFortiOS REST API Reference\nFortinet Inc.\nName\nType\nSummary\nRequired\nfile_content\nstring\nProvided when uploading a file: base64 encoded file data. Must not\ncontain whitespace or other invalid base64 characters. Must be\nincluded in HTTP body.\nNo\nconfig: backup\nSummary\nBackup system config\nURI\nsystem/config/backup/\nHTTP Method\nGET\nAction\nbackup\nAccess Group\nmntgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\ndestination\nstring\nConfiguration file destination [file* | usb]\nNo\nusb_filename\nstring\nWhen using 'usb' destination: the filename to save to on the\nconnected USB device\nNo\npassword\nstring\nPassword to encrypt configuration data.\nNo\nscope\nstring\nSpecify global or VDOM only backup [global | vdom].\nYes\nvdom\nstring\nIf 'vdom' scope specified, the name of the VDOM to backup\nconfiguration.\nNo\nconfig: usb-filelist\nSummary\nList configuration files available on connected USB drive.\nURI\nsystem/config/usb-filelist/\nHTTP Method\nGET\nAction\nusb-filelist\nAccess Group\nsysgrp\nResponse Type\narray\nMonitor API\n96\nFortiOS REST API Reference\nFortinet Inc.\nsandbox: status\nSummary\nRetrieve sandbox status.\nURI\nsystem/sandbox/status/\nHTTP Method\nGET\nAction\nstatus\nAccess Group\nsysgrp\nResponse Type\nobject\nsandbox: stats\nSummary\nRetrieve sandbox statistics.\nURI\nsystem/sandbox/stats/\nHTTP Method\nGET\nAction\nstats\nAccess Group\nsysgrp\nResponse Type\nobject\nobject: usage\nSummary\nRetrieve all objects that are currently using as well as objects that can use the\ngiven object.\nURI\nsystem/object/usage/\nHTTP Method\nGET\nAction\nusage\nAccess Group\nany\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\npath\nstring\nThe CMDB table's path\nNo\nname\nstring\nThe CMDB table's name\nNo\nMonitor API\n97\nFortiOS REST API Reference\nFortinet Inc.\nName\nType\nSummary\nRequired\nqtypes\narray\nList of CMDB table qTypes\nNo\nmkey\nstring\nThe mkey for the object\nYes\ntimezone: select\nSummary\nGet world timezone and daylight saving time.\nURI\nsystem/timezone/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nany\nResponse Type\narray\nvmlicense: upload\nSummary\nUpdate VM license using uploaded file. Reboots immediately if successful.\nURI\nsystem/vmlicense/upload/\nHTTP Method\nPOST\nAction\nupload\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nfile_content\nstring\nProvided when uploading a file: base64 encoded file data. Must not\ncontain whitespace or other invalid base64 characters. Must be\nincluded in HTTP body.\nNo\nsensor-info: select\nSummary\nRetrieve system sensor status.\nURI\nsystem/sensor-info/select/\nHTTP Method\nGET\nMonitor API\n98\nFortiOS REST API Reference\nFortinet Inc.\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\naudit: select\nSummary\nRetrieve Security Fabric audit results.\nURI\nsystem/audit/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\nfortiguard-blacklist: select\nSummary\nRetrieve blacklist information for a specified IP.\nURI\nsystem/fortiguard-blacklist/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nip\nstring\nIPv4 address to check against.\nYes\ntimeout\nint\nTimeout period in seconds (defaults to 5).\nNo\nvpn-certificate\nca: import\nSummary\nImport CA certificate.\nMonitor API\n99\nFortiOS REST API Reference\nFortinet Inc.\nURI\nvpn-certificate/ca/import/\nHTTP Method\nPOST\nAction\nimport\nAccess Group\nvpngrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nimport_method\nstring\nMethod of importing CA certificate.[file|scep]\nYes\nscep_url\nstring\nSCEP server URL. Required for import via SCEP\nNo\nscep_ca_id\nstring\nSCEP server CA identifier for import via SCEP.\nNo\nscope\nstring\nScope of CA certificate [vdom*|global]. Global scope is only\naccessible for global administrators\nNo\nfile_content\nstring\nProvided when uploading a file: base64 encoded file data. Must not\ncontain whitespace or other invalid base64 characters. Must be\nincluded in HTTP body.\nNo\ncrl: import\nSummary\nImport certificate revocation lists (CRL) from file content.\nURI\nvpn-certificate/crl/import/\nHTTP Method\nPOST\nAction\nimport\nAccess Group\nvpngrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nscope\nstring\nScope of CRL [vdom*|global]. Global scope is only accessible for\nglobal administrators\nNo\nfile_content\nstring\nProvided when uploading a file: base64 encoded file data. Must not\ncontain whitespace or other invalid base64 characters. Must be\nincluded in HTTP body.\nNo\nMonitor API\n100\nFortiOS REST API Reference\nFortinet Inc.\nlocal: import\nSummary\nImport local certificate.\nURI\nvpn-certificate/local/import/\nHTTP Method\nPOST\nAction\nimport\nAccess Group\nvpngrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\ntype\nstring\nType of certificate.[local|pkcs12|regular]\nYes\ncertname\nstring\nCertificate name for pkcs12 and regular certificate types.\nNo\npassword\nstring\nOptional password for pkcs12 and regular certificate types.\nNo\nkey_file_\ncontent\nstring\nKey content encoded in BASE64 for regular certificate type.\nNo\nscope\nstring\nScope of local certificate [vdom*|global]. Global scope is only\naccessible for global administrators\nNo\nfile_content\nstring\nProvided when uploading a file: base64 encoded file data. Must not\ncontain whitespace or other invalid base64 characters. Must be\nincluded in HTTP body.\nNo\nremote: import\nSummary\nImport remote certificate.\nURI\nvpn-certificate/remote/import/\nHTTP Method\nPOST\nAction\nimport\nAccess Group\nvpngrp\nResponse Type\nobject\nMonitor API\n101\nFortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nfile_content\nstring\nProvided when uploading a file: base64 encoded file data. Must not\ncontain whitespace or other invalid base64 characters. Must be\nincluded in HTTP body.\nNo\ncsr: generate\nSummary\nGenerate a certificate signing request (CSR) and a private key. The CSR can be\nretrieved / downloaded from CLI, GUI and REST API.\nURI\nvpn-certificate/csr/generate/\nHTTP Method\nPOST\nAction\ngenerate\nAccess Group\nvpngrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\ncertname\nstring\nCerticate name. Used to retrieve / download the CSR. Not included\nin CSR and key content.\nYes\nsubject\nstring\nSubject (Host IP/Domain Name/E-Mail). Common Name (CN) of the\ncertificate subject.\nYes\nkeytype\nstring\nGenerate a RSA or an elliptic curve certificate request [rsa|ec]. The\nElliptic Curve option is unavailable if the FortiGate is a Low\nEncryption Device (LENC)\nYes\nkeysize\nint\nKey size.[1024|1536|2048|4096]. 512 only if the FortiGate is a Low\nEncryption Device (LENC). Required when keytype is RSA.\nNo\ncurvename\nstring\nElliptic curve name. [secp256r1|secp384r1|secp521r1]. Unavailable if\nthe FortiGate is a Low Encryption Device (LENC). Required when\nkeytype is ec.\nNo\norgunits\narray\nList of organization units. Organization Units (OU) of the certificate\nsubject.\nNo\norg\nstring\nOrganization (O) of the certificate subject.\nNo\ncity\nstring\nLocality (L) of the certificate subject.\nNo\nstate\nstring\nState (ST) of the certificate subject.\nNo\nMonitor API\n102\nFortiOS REST API Reference\nFortinet Inc.\nName\nType\nSummary\nRequired\ncountrycode\nstring\nCountry (C) of the certificate subject.\nNo\nemail\nstring\nEmail of the certificate subject.\nNo\nsub_alt_name\nstring\nSubject alternative name (SAN) of the certificate.\nNo\npassword\nstring\nPassword / pass phrase for the private key. If not provided, FortiGate\ngenerates a random one.\nNo\nscep_url\nstring\nSCEP server URL. If provided, use the url to enroll the csr through\nSCEP.\nNo\nscep_password\nstring\nSCEP challenge password. Some SCEP servers may require\nchallege password. Provide it when SCEP server requires.\nNo\nscope\nstring\nScope of CSR [vdom*|global]. Global scope is only accessible for\nglobal administrators\nNo\ncheck-port-availability: select\nSummary\nCheck whether a list of TCP port ranges is available for a certain service.\nURI\nsystem/check-port-availability/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nport_ranges\narray\nList of TCP port range objects to check against.\nYes\nservice\nstring\nThe service in which the ports could be available. 'service' options are\n[reserved | sysglobal | webproxy | ftpproxy | sslvpn | slaprobe | fsso |\nftm_push]. If 'service' is not specified, the port ranges availablity is\nchecked against all services.\nNo\nextender-controller\nextender: select\nSummary\nRetrieve statistics for specific configured FortiExtender units.\nMonitor API\n103\nFortiOS REST API Reference\nFortinet Inc.\nURI\nextender-controller/extender/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nnetgrp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nid\narray\nList of FortiExtender IDs to query.\nYes\nextender: reset\nSummary\nReset a specific FortiExtender unit.\nURI\nextender-controller/extender/reset/\nHTTP Method\nPOST\nAction\nreset\nAccess Group\nnetgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nid\nstring\nFortiExtender ID to reset.\nYes\nuser\nfirewall: select\nSummary\nList authenticated firewall users.\nURI\nuser/firewall/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nauthgrp\nMonitor API\n104\nFortiOS REST API Reference\nFortinet Inc.\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nstart\nint\nStarting entry index.\nNo\ncount\nint\nMaximum number of entries to return.\nNo\nipv4\nboolean\nInclude IPv4 user (default=true).\nNo\nipv6\nboolean\nInclude IPv6 users.\nNo\nfirewall: deauth\nSummary\nDeauthenticate single, multiple, or all firewall users.\nURI\nuser/firewall/deauth/\nHTTP Method\nPOST\nAction\ndeauth\nAccess Group\nauthgrp\nExtra parameters\nName\nType\nSummary\nRequired\nuser_type\nstring\nUser type [proxy|firewall]. Required for both proxy and firewall users.\nNo\nid\nint\nUser ID. Required for both proxy and firewall users.\nNo\nip\nstring\nUser IP address. Required for both proxy and firewall users.\nNo\nip_version\nstring\nIP version [ip4|ip6]. Only required if user_type is firewall.\nNo\nmethod\nstring\nAuthentication method [fsso|rsso|ntlm|firewall|wsso|fsso_citrix|sso_\nguest]. Only required if user_type is firewall.\nNo\nall\nboolean\nSet to true to deauthenticate all users. Other parameters will be\nignored.\nNo\nusers\narray\nArray of user objects to deauthenticate. Use this to deauthenticate\nmultiple users at once. Each object should include the above\nproperties.\nNo\nMonitor API\n105\nFortiOS REST API Reference\nFortinet Inc.\nbanned: select\nSummary\nReturn a list of all banned users by IP.\nURI\nuser/banned/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nauthgrp\nbanned: clear_users\nSummary\nImmediately clear a list of specific banned users by IP.\nURI\nuser/banned/clear_users/\nHTTP Method\nPOST\nAction\nclear_users\nAccess Group\nauthgrp\nExtra parameters\nName\nType\nSummary\nRequired\nip_addresses\narray\nList of banned user IPs to clear. IPv4 and IPv6 addresses are\nallowed.\nYes\nbanned: add_users\nSummary\nImmediately add one or more users to the banned list.\nURI\nuser/banned/add_users/\nHTTP Method\nPOST\nAction\nadd_users\nAccess Group\nauthgrp\nExtra parameters\nName\nType\nSummary\nRequired\nip_addresses\narray\nList of IP Addresses to ban. IPv4 and IPv6 addresses are allowed.\nYes\nexpiry\nint\nTime until expiry in seconds. 0 for indefinite ban.\nNo\nMonitor API\n106\nFortiOS REST API Reference\nFortinet Inc.\nbanned: clear_all\nSummary\nImmediately clear all banned users.\nURI\nuser/banned/clear_all/\nHTTP Method\nPOST\nAction\nclear_all\nAccess Group\nauthgrp\nfortitoken: select\nSummary\nRetrieve a map of FortiTokens and their status.\nURI\nuser/fortitoken/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nauthgrp\nResponse Type\nobject\nfortitoken: activate\nSummary\nActivate a set of FortiTokens by serial number.\nURI\nuser/fortitoken/activate/\nHTTP Method\nPOST\nAction\nactivate\nAccess Group\nauthgrp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\ntokens\narray\nList of FortiToken serial numbers to activate. If omitted, all tokens\nwill be used.\nNo\nMonitor API\n107\nFortiOS REST API Reference\nFortinet Inc.\ndevice: select\nSummary\nRetrieve a list of detected devices.\nURI\nuser/device/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nETag Caching\nEnabled\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nmaster_only\nboolean\nList of master device only.\nNo\nfortilink_\nvisibility\nboolean\nAdd port and switch info for devices behind a managed FortiSwitch.\nNo\ncompliance_\nvisibility\nboolean\nAdd compliance status to indicate if a device is 'exempt' or 'non-\ncompliant' by interface's FortiClient host check.\nNo\nintf_name\nstring\nFilter: Name of interface where the device was detected. Only\navailable when compliance_visibility is true.\nNo\nmaster_mac\nstring\nFilter: Master MAC of a device. Multiple entries could be returned.\nNo\nfortitoken: refresh\nSummary\nRefresh a set of FortiTokens by serial number.\nURI\nuser/fortitoken/refresh/\nHTTP Method\nPOST\nAction\nrefresh\nAccess Group\nauthgrp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\ntokens\narray\nList of FortiToken serial numbers to refresh. If omitted, all tokens will\nbe used.\nNo\nMonitor API\n108\nFortiOS REST API Reference\nFortinet Inc.\nfortitoken: provision\nSummary\nProvision a set of FortiTokens by serial number.\nURI\nuser/fortitoken/provision/\nHTTP Method\nPOST\nAction\nprovision\nAccess Group\nauthgrp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\ntokens\narray\nList of FortiToken serial numbers to provision. If omitted, all tokens\nwill be used.\nNo\nfortitoken: send-activation\nSummary\nSend a FortiToken activation code to a user via SMS or Email.\nURI\nuser/fortitoken/send-activation/\nHTTP Method\nPOST\nAction\nsend-activation\nAccess Group\nauthgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\ntoken\nstring\nFortiToken serial number. The token must be assigned to a\nuser/admin.\nYes\nmethod\nstring\nMethod to send activation code [email|sms].\nYes\nemail\nstring\nOverride email address.\nNo\nsms_phone\nstring\nOverride SMS phone number. SMS provider must be set in the\nassigned user/admin.\nNo\nMonitor API\n109\nFortiOS REST API Reference\nFortinet Inc.\nfsso: refresh-server\nSummary\nRefresh remote agent group list for all fsso agents.\nURI\nuser/fsso/refresh-server/\nHTTP Method\nPOST\nAction\nrefresh-server\nAccess Group\nauthgrp\nfsso: select\nSummary\nGet a list of fsso and fsso polling status.\nURI\nuser/fsso/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nauthgrp\nutm\nrating-lookup: select\nSummary\nLookup FortiGuard rating for a specific URL.\nURI\nutm/rating-lookup/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nutmgrp.webfilter\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nurl\nstring\nURL to query.\nYes\nurl\narray\nList of URLs to query.\nNo\nMonitor API\n110\nFortiOS REST API Reference\nFortinet Inc.\napp-lookup: select\nSummary\nQuery remote FortiFlow database to resolve hosts to application control entries.\nURI\nutm/app-lookup/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nany\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nhosts\narray\nList of hosts to resolve.\nNo\naddress\nstring\nDestination IP for one host entry.\nNo\ndst_port\nint\nDestination port for one host entry.\nNo\nprotocol\nint\nProtocol for one host entry.\nNo\napplication-categories: select\nSummary\nRetrieve a list of application control categories.\nURI\nutm/application-categories/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nany\nResponse Type\narray\nantivirus: stats\nSummary\nRetrieve antivirus scanning statistics.\nURI\nutm/antivirus/stats/\nHTTP Method\nGET\nAction\nstats\nAccess Group\nutmgrp.antivirus\nResponse Type\nobject\nMonitor API\n111\nFortiOS REST API Reference\nFortinet Inc.\nvirtual-wan\nhealth-check: select\nSummary\nRetrieve health-check statistics for each SD-WAN link.\nURI\nvirtual-wan/health-check/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nmembers: select\nSummary\nRetrieve interface statistics for each SD-WAN link.\nURI\nvirtual-wan/members/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nwebfilter\noverride: select\nSummary\nList all administrative and user initiated webfilter overrides.\nURI\nwebfilter/override/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nutmgrp.webfilter\noverride: delete\nSummary\nDelete a configured webfilter override.\nURI\nwebfilter/override/delete/\nHTTP Method\nPOST\nMonitor API\n112\nFortiOS REST API Reference\nFortinet Inc.\nAction\ndelete\nAccess Group\nutmgrp.webfilter\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nstring\nID of webfilter override to delete.\nNo\nmalicious-urls: select\nSummary\nList all URLs in FortiSandbox malicious URL database.\nURI\nwebfilter/malicious-urls/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nutmgrp.webfilter\nETag Caching\nEnabled\nResponse Type\nobject\nmalicious-urls: stat\nSummary\nRetrieve statistics for the FortiSandbox malicious URL database.\nURI\nwebfilter/malicious-urls/stat/\nHTTP Method\nGET\nAction\nstat\nAccess Group\nutmgrp.webfilter\nETag Caching\nEnabled\nResponse Type\nobject\ncategory-quota: select\nSummary\nRetrieve quota usage statistics for webfilter categories.\nURI\nwebfilter/category-quota/select/\nHTTP Method\nGET\nMonitor API\n113\nFortiOS REST API Reference\nFortinet Inc.\nAction\nselect\nAccess Group\nutmgrp.webfilter\nExtra parameters\nName\nType\nSummary\nRequired\nprofile\nstring\nWebfilter profile.\nNo\nuser\nstring\nUser or IP (required if profile specified).\nNo\ncategory-quota: reset\nSummary\nReset webfilter quota for user or IP.\nURI\nwebfilter/category-quota/reset/\nHTTP Method\nPOST\nAction\nreset\nAccess Group\nutmgrp.webfilter\nExtra parameters\nName\nType\nSummary\nRequired\nprofile\nstring\nWebfilter profile to reset.\nNo\nuser\nstring\nUser or IP to reset with.\nNo\nfortiguard-categories: select\nSummary\nReturn FortiGuard web filter categories.\nURI\nwebfilter/fortiguard-categories/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nany\nResponse Type\narray\nMonitor API\n114\nFortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\ninclude_\nunrated\nboolean\nInclude Unrated category in result list.\nNo\nconvert_\nunrated_id\nboolean\nConvert Unrated category id to the one for CLI use.\nNo\ntrusted-urls: select\nSummary\nList all URLs in FortiGuard trusted URL database.\nURI\nwebfilter/trusted-urls/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nutmgrp.webfilter\nETag Caching\nEnabled\nResponse Type\nobject\nvpn\nipsec: select\nSummary\nReturn an array of active IPsec VPNs.\nURI\nvpn/ipsec/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nvpngrp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\ntunnel\nstring\nFilter for a specific IPsec tunnel name.\nNo\nstart\nint\nStarting entry index.\nNo\ncount\nint\nMaximum number of entries to return.\nNo\nMonitor API\n115\nFortiOS REST API Reference\nFortinet Inc.\nipsec: tunnel_up\nSummary\nBring up a specific IPsec VPN tunnel.\nURI\nvpn/ipsec/tunnel_up/\nHTTP Method\nPOST\nAction\ntunnel_up\nAccess Group\nvpngrp\nExtra parameters\nName\nType\nSummary\nRequired\np1name\nstring\nIPsec phase1 name.\nYes\np2name\nstring\nIPsec phase2 name.\nYes\np2serial\nstring\nIPsec phase2 serial.\nNo\nipsec: tunnel_down\nSummary\nBring down a specific IPsec VPN tunnel.\nURI\nvpn/ipsec/tunnel_down/\nHTTP Method\nPOST\nAction\ntunnel_down\nAccess Group\nvpngrp\nExtra parameters\nName\nType\nSummary\nRequired\np1name\nstring\nIPsec phase1 name.\nYes\np2name\nstring\nIPsec phase2 name.\nYes\np2serial\nstring\nIPsec phase2 serial.\nNo\nipsec: tunnel_reset_stats\nSummary\nReset statistics for a specific IPsec VPN tunnel.\nURI\nvpn/ipsec/tunnel_reset_stats/\nMonitor API\n116\nFortiOS REST API Reference\nFortinet Inc.\nHTTP Method\nPOST\nAction\ntunnel_reset_stats\nAccess Group\nvpngrp\nExtra parameters\nName\nType\nSummary\nRequired\np1name\nstring\nIPsec phase1 name.\nYes\nssl: select\nSummary\nRetrieve a list of all SSL-VPN sessions and sub-sessions.\nURI\nvpn/ssl/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nvpngrp\nssl: clear_tunnel\nSummary\nRemove all active tunnel sessions in current virtual domain.\nURI\nvpn/ssl/clear_tunnel/\nHTTP Method\nPOST\nAction\nclear_tunnel\nAccess Group\nvpngrp\nssl: delete\nSummary\nTerminate the provided SSL-VPN session.\nURI\nvpn/ssl/delete/\nHTTP Method\nPOST\nAction\ndelete\nAccess Group\nvpngrp\nMonitor API\n117\nFortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\ntype\nstring\nThe session type [websession|subsession].\nYes\nindex\nint\nThe session index.\nYes\nssl: stats\nSummary\nReturn statistics about the SSL-VPN.\nURI\nvpn/ssl/stats/\nHTTP Method\nGET\nAction\nstats\nAccess Group\nvpngrp\nwanopt\nhistory: select\nSummary\nRetrieve WAN opt. statistics history.\nURI\nwanopt/history/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nwanoptgrp\nExtra parameters\nName\nType\nSummary\nRequired\nperiod\nstring\nStatistics period [10-min*|hour|day|week|30-day].\nNo\nhistory: reset\nSummary\nReset WAN opt. statistics.\nURI\nwanopt/history/reset/\nHTTP Method\nPOST\nMonitor API\n118\nFortiOS REST API Reference\nFortinet Inc.\nAction\nreset\nAccess Group\nwanoptgrp\nwebcache: select\nSummary\nRetrieve webcache statistics history.\nURI\nwanopt/webcache/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nwanoptgrp\nExtra parameters\nName\nType\nSummary\nRequired\nperiod\nstring\nStatistics period [10-min*|hour|day|week|30-day].\nNo\nwebcache: reset\nSummary\nReset webcache statistics.\nURI\nwanopt/webcache/reset/\nHTTP Method\nPOST\nAction\nreset\nAccess Group\nwanoptgrp\npeer_stats: select\nSummary\nRetrieve a list of WAN opt peer statistics.\nURI\nwanopt/peer_stats/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nwanoptgrp\nMonitor API\n119\nFortiOS REST API Reference\nFortinet Inc.\npeer_stats: reset\nSummary\nReset WAN opt peer statistics.\nURI\nwanopt/peer_stats/reset/\nHTTP Method\nPOST\nAction\nreset\nAccess Group\nwanoptgrp\nwebproxy\npacfile: download\nSummary\nDownload webproxy PAC file.\nURI\nwebproxy/pacfile/download/\nHTTP Method\nGET\nAction\ndownload\nAccess Group\nnetgrp\nResponse Type\nobject\nwebcache\nstats: select\nSummary\nRetrieve webcache statistics.\nURI\nwebcache/stats/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nwanoptgrp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nperiod\nstring\nStatistics period [10min|hour|day|month].\nNo\nMonitor API\n120\nFortiOS REST API Reference\nFortinet Inc.\nstats: reset\nSummary\nReset all webcache statistics.\nURI\nwebcache/stats/reset/\nHTTP Method\nPOST\nAction\nreset\nAccess Group\nwanoptgrp\nwifi\nclient: select\nSummary\nRetrieve a list of connected WiFi clients.\nURI\nwifi/client/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nwifi\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nstart\nint\nStarting entry index.\nNo\ncount\nint\nMaximum number of entries to return.\nNo\ntype\nstring\nRequest type [all*|fail-login].\nNo\nmanaged_ap: select\nSummary\nRetrieve a list of managed FortiAPs.\nURI\nwifi/managed_ap/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nwifi\nResponse Type\narray\nMonitor API\n121\nFortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nwtp_id\nstring\nFilter: single managed FortiAP by ID.\nNo\nincl_local\nboolean\nEnable to include the local FortiWiFi device in the results.\nNo\nmanaged_ap: set_status\nSummary\nUpdate administrative state for a given FortiAP (enable or disable authorization).\nURI\nwifi/managed_ap/set_status/\nHTTP Method\nPOST\nAction\nset_status\nAccess Group\nwifi\nExtra parameters\nName\nType\nSummary\nRequired\nwtpname\nstring\nFortiAP name.\nNo\nadmin\nstring\nNew FortiAP administrative state [enable|disable|discovered].\nNo\nfirmware: select\nSummary\nRetrieve a list of current and recommended firmware for FortiAPs in use.\nURI\nwifi/firmware/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nwifi\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\ntimeout\nstring\nFortiGuard connection timeout (defaults to 2 seconds).\nNo\nMonitor API\n122\nFortiOS REST API Reference\nFortinet Inc.\nmanaged_ap: restart\nSummary\nRestart a given FortiAP.\nURI\nwifi/managed_ap/restart/\nHTTP Method\nPOST\nAction\nrestart\nAccess Group\nwifi\nExtra parameters\nName\nType\nSummary\nRequired\nwtpname\nstring\nFortiAP name.\nNo\nmanaged_ap: upgrade\nSummary\nUpgrade firmware image on the given FortiAP using uploaded file.\nURI\nwifi/managed_ap/upgrade/\nHTTP Method\nPOST\nAction\nupgrade\nAccess Group\nwifi\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nstring\nSerial number of FortiAP to upgrade.\nYes\nsource\nstring\nFirmware file data source [upload|fortiguard].\nYes\nfilename\nstring\nFirmware image file for when 'source' is 'upload'.\nNo\nimage_id\nstring\nFortiguard image file ID for when 'source' is 'fortiguard'.\nNo\nfile_content\nstring\nProvided when uploading a file: base64 encoded file data. Must not\ncontain whitespace or other invalid base64 characters. Must be\nincluded in HTTP body.\nNo\nMonitor API\n123\nFortiOS REST API Reference\nFortinet Inc.\nap_status: select\nSummary\nRetrieve statistics for all managed FortiAPs.\nURI\nwifi/ap_status/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nwifi\ninterfering_ap: select\nSummary\nRetrieve a list of interfering APs for one FortiAP radio.\nURI\nwifi/interfering_ap/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nwifi\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nwtp\nstring\nFortiAP ID to query.\nNo\nradio\nint\nRadio ID.\nNo\nstart\nint\nStarting entry index.\nNo\ncount\nint\nMaximum number of entries to return.\nNo\neuclid: select\nSummary\nRetrieve presence analytics statistics.\nURI\nwifi/euclid/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nwifi\nMonitor API\n124\nFortiOS REST API Reference\nFortinet Inc.\neuclid: reset\nSummary\nReset presence analytics statistics.\nURI\nwifi/euclid/reset/\nHTTP Method\nPOST\nAction\nreset\nAccess Group\nwifi\nrogue_ap: select\nSummary\nRetrieve a list of detected rogue APs.\nURI\nwifi/rogue_ap/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nwifi\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nstart\nint\nStarting entry index.\nNo\ncount\nint\nMaximum number of entries to return.\nNo\nrogue_ap: clear_all\nSummary\nClear all detected rogue APs.\nURI\nwifi/rogue_ap/clear_all/\nHTTP Method\nPOST\nAction\nclear_all\nAccess Group\nwifi\nrogue_ap: set_status\nSummary\nMark detected APs as rogue APs.\nMonitor API\n125\nFortiOS REST API Reference\nFortinet Inc.\nURI\nwifi/rogue_ap/set_status/\nHTTP Method\nPOST\nAction\nset_status\nAccess Group\nwifi\nExtra parameters\nName\nType\nSummary\nRequired\nbssid\narray\nList of rogue AP MAC addresses.\nNo\nssid\narray\nCorresponding list of rogue AP SSIDs.\nNo\nstatus\nstring\nStatus to assign matching APs\n[unclassified|rogue|accepted|suppressed].\nNo\nspectrum: select\nSummary\nRetrieve spectrum analysis information for a specific FortiAP.\nURI\nwifi/spectrum/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nwifi\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nwtp_id\nstring\nFortiAP ID to query.\nYes\ncoverage\ndownload: select\nSummary\nDownload code coverage.\nURI\ncoverage/download/select/\nHTTP Method\nGET\nMonitor API\n126\nFortiOS REST API Reference\nFortinet Inc.\nAction\nselect\nAccess Group\nany\nResponse Type\nobject\nExamples\nMethod\nURL\nURL Parameters\nBody Data\nAccess\nGroup\nDescription\nGET\n/api/v2/monitor/\nfirewall/policy\n?vdom=root\nfwgrp.policy\nList traffic statistics for all\nIPv4 policies, vdom root\nGET\n/api/v2/monitor/\nfirewall/policy\n?global=1\nfwgrp.policy\nList traffic statistics for all\nIPv4 policies, all\naccessible vdoms\nPOST\n/api/v2/monitor/\nfirewall/policy/reset\n?vdom=root\nfwgrp.policy\nReset traffic statistics for\nall IPv4 policies, vdom root\nPOST\n/api/v2/monitor/\nfirewall/policy/reset\n?global=1\nfwgrp.policy\nReset traffic statistics for\nall IPv4 policies, all\naccessible vdoms\nPOST\n/api/v2/monitor/\nfirewall/policy6/\nclear_counters\n?vdom=root\n{'policy': 1,}\nfwgrp.policy\nReset traffic statistics for\nsingle IPv4 policy, vdom\nroot\nPOST\n/api/v2/monitor/\nfirewall/policy6/\nclear_counters\n?vdom=root\n{'policy': [1, 2]}\nfwgrp.policy\nReset traffic statistics for\nmultiple IPv4 policies,\nvdom root\nGET\n/api/v2/monitor/\nfirewall/session\n?vdom=root&\nip_version=ipv4&\nstart=0&count=\n1&summary=True\nsysgrp\nList the first active ipv4\nfirewall sessions, vdom\nroot\nPOST\n/api/v2/monitor/\nfirewall/session/\nclear_all\n?vdom=root\nsysgrp\nImmediately clear all\nactive IPv4 and IPv6\nsessions, vdom root\nPOST\n/api/v2/monitor/\nfirewall/session/\nclose\n?vdom=root\n{'pro': \"udp\", 'saddr':\n\"192.168.100.110\",\n'daddr': \"96.45.33.\n73\", 'sport': 55933,\n'dport': 8888}\nsysgrp\nImmediately close specific\nsession matched with the\nfilter, vdom root\nMonitor API\n127\nFortiOS REST API Reference\nFortinet Inc.\nMethod\nURL\nURL Parameters\nBody Data\nAccess\nGroup\nDescription\nPOST\n/api/v2/monitor/\nsystem/os/\nreboot\nsysgrp\nImmediately reboot this\ndevice\nPOST\n/api/v2/monitor/\nsystem/os/\nshutdown\nsysgrp\nImmediately shutdown this\ndevice\nMonitor API\n128\nCopyright© 2019 Fortinet, Inc. All rights reserved. Fortinet®, FortiGate®, FortiCare® and FortiGuard®, and certain other marks are registered trademarks of Fortinet, Inc., in\nthe U.S. and other jurisdictions, and other Fortinet names herein may also be registered and/or common law trademarks of Fortinet. All other product or company names may\nbe trademarks of their respective owners. Performance and other metrics contained herein were attained in internal lab tests under ideal conditions, and actual performance\nand other results may vary. Network variables, different network environments and other conditions may affect performance results. Nothing herein represents any binding\ncommitment by Fortinet, and Fortinet disclaims all warranties, whether express or implied, except to the extent Fortinet enters a binding written contract, signed by Fortinet’s\nGeneral Counsel, with a purchaser that expressly warrants that the identified product will perform according to certain expressly-identified performance metrics and, in such\nevent, only the specific performance metrics expressly identified in such binding written contract shall be binding on Fortinet. For absolute clarity, any such warranty will be\nlimited to performance in the same ideal conditions as in Fortinet’s internal lab tests. In no event does Fortinet make any commitment related to future deliverables, features\nor development, and circumstances may change such that any forward-looking statements herein are not accurate. Fortinet disclaims in full any covenants, representations,\nand guarantees pursuant hereto, whether express or implied. Fortinet reserves the right to change, modify, transfer, or otherwise revise this publication without notice, and\nthe most current version of the publication shall be applicable.",
      "page_count": 129,
      "pages": [
        {
          "page": 1,
          "text": "FortiOS - REST API Reference\nVersion 5.6.11\n\nFi RTINET.\nFortiOS - REST API Reference\nVersion 5.6.11",
          "char_count": 100,
          "ocr_used": true,
          "original_char_count": 44
        },
        {
          "page": 2,
          "text": "FORTINET DOCUMENT LIBRARY\nhttps://docs.fortinet.com\nFORTINET VIDEO GUIDE\nhttps://video.fortinet.com\nFORTINET BLOG\nhttps://blog.fortinet.com\nCUSTOMER SERVICE & SUPPORT\nhttps://support.fortinet.com\nFORTINET TRAINING & CERTIFICATION PROGRAM\nhttps://www.fortinet.com/support-and-training/training.html\nNSE INSTITUTE\nhttps://training.fortinet.com\nFORTIGUARD CENTER\nhttps://fortiguard.com/\nEND USER LICENSE AGREEMENT\nhttps://www.fortinet.com/doc/legal/EULA.pdf\nFEEDBACK\nEmail: techdoc@fortinet.com\nAugust 15, 2019\nFortiOS 5.6.11 REST API Reference\n01-5611-414177-20190815",
          "char_count": 566,
          "ocr_used": false
        },
        {
          "page": 3,
          "text": "TABLE OF CONTENTS\nChange Log\n5\nIntroduction\n6\nWhat's New in the REST API\n6\nAuthentication\n6\nSession-based authentication\n6\nToken-based authentication\n8\nAuthorization\n11\nSupported HTTP methods\n11\nResponse codes\n11\nDebugging\n12\nCMDB API\n14\nURL path\n14\nURL parameters\n14\nGeneric parameters\n15\nSpecific parameters\n15\nBody data\n16\nLimitation\n16\nFilter with multiple key/value pairs\n17\nFilter Syntax\n17\nFilter Operators\n17\nCombining Filters\n17\nReserved Characters\n18\nList of Methods\n18\ncollection\n19\nresource\n19\nExamples\n22\nRetrieve table\n22\nRetrieve table schema\n23\nRetrieve table default\n23\nPurge table\n23\nRetrieve object\n23\nCreate object\n24\nEdit object\n24\nDelete object\n24\nClone object\n24\nMove object\n25\nAppend child object\n25\nEdit child object\n25\nDelete child object\n25\nPurge child table\n25\nRetrieve complex table\n26\nFortiOS REST API Reference\nFortinet Inc.",
          "char_count": 856,
          "ocr_used": false
        },
        {
          "page": 4,
          "text": "Edit complex table\n26\nGlobal requests (apply to all accessible vdoms)\n26\nMonitor API\n27\nURL path\n27\nURL parameters\n27\nGeneric parameters\n27\nSpecific parameters\n28\nBody data\n28\nFile upload\n28\nFile upload via JSON data\n28\nFile upload via multi-part file\n29\nFile download\n29\nFile download via browser\n29\nFile download via script\n29\nList of Methods\n30\nendpoint-control\n39\nfirewall\n44\nfortiview\n55\ngeoip\n56\nips\n57\nlicense\n57\nlog\n59\nrouter\n63\nsystem\n66\nswitch-controller\n82\nvpn-certificate\n99\nextender-controller\n103\nuser\n104\nutm\n110\nvirtual-wan\n112\nwebfilter\n112\nvpn\n115\nwanopt\n118\nwebproxy\n120\nwebcache\n120\nwifi\n121\ncoverage\n126\nExamples\n127\nFortiOS REST API Reference\nFortinet Inc.\n4",
          "char_count": 681,
          "ocr_used": false
        },
        {
          "page": 5,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nChange Log\nDate\nChange Description\n2019-08-15\nInitial release.",
          "char_count": 104,
          "ocr_used": false
        },
        {
          "page": 6,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nIntroduction\nThis document provides the REST API information supported in FortiOS 5.6.11. This document covers a reference of\nthe REST API supported by the FortiOS GUI.\nFortiOS 5.6.11 supports the following REST APIs:\nl\nCMDB API\nl\nRetrieve object meta data (default, schema)\nl\nRetrieve object/table (with filter, format, start, count, other flags)\nl\nCreate object\nl\nModify object\nl\nDelete object\nl\nClone object\nl\nMove object\nl\nMonitor API\nl\nRetrieve/Reset endpoint stats (with filter, start, count)\nl\nPerform endpoint operations\nl\nUpload/Download file\nl\nRestore/Backup config\nl\nUpgrade/Downgrade firmware\nl\nRestart/Shutdown FGT\nWhat's New in the REST API\nNo major change to the REST API endpoints.\nAuthentication\nStarting in FortiOS 5.6.1, there are two ways that user can authenticate against the API:\nl\nSession-based authentication (legacy)\nl\nToken-based authentication (5.6.1 and newer)\nSession-based authentication\nAs the name suggests, the authentication is valid per login session. The user needs to send a login request to obtain a\nauthentication cookie and CSRF token to be used for subsequent requests. The user then needs to send a logout\nrequest to invalidate the authentication cookie and CSRF token.",
          "char_count": 1254,
          "ocr_used": false
        },
        {
          "page": 7,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nAuthentication Cookie\nAuthentication cookie (APSCOOKIE) is provided by the API after a successful login request. All subsequent requests\nmust include this cookie to be authorized by the API. Any request without the cookie or with mismatched cookie will be\ndenied access to the API (HTTP 401 error code).\nCSRF Tokens\nCross-Site Request Forgery (CSRF) Tokens are alphanumeric values that are passed back-and-forth between client and\nserver to ensure that a user's form submission does not originate from an offsite document.\nThe CSRF token is available in the session ccsrftoken cookie, which must be included in the request header under\nX-CSRFTOKEN. See test script sample for how to handle CSRF token.\nOnly write requests (HTTP POST/PUT/DELETE) need CSRF tokens. Read requests (HTTP\nGET) do not require CSRF tokens.\nSetting Up an Authenticated Session\nTo authenticate with the FortiGate and request a session, send a POST request to the log in request handler with your\nusername and password.\nLogin URL\n/logincheck\nThe request body must contain the following keys in URL form encoding:\nKey\nType\nDescription\nusername\nString\nThe admin username.\nsecretkey\nString\nThe password for that admin.\najax\nInt (1)\nRequired: Format the response for easier parsing. Enable using 1.\nExample:\nPOST /logincheck\nusername=AdminUser&secretkey=AdminPassword&ajax=1\nEnsure that you're using the correct protocol. By default, a FortiGate? will redirect HTTP requests to HTTPS and your\nlogin requests may fail. As well, FortiGate? 's will use a self-signed server certificate by default. Refer to the\ndocumentation for the specific library or framework that you're using to validate the certificate manually.\nThe response to this request will be in the following format:\n<status_code><javascript>\nA successful login response would be:\n1document.location=\"/ng/prompt?viewOnly&redir=%2Fng%2F\";\nFor most uses, you only need to read the first character of the response body to get the response status code.\nIntroduction\n7",
          "char_count": 2035,
          "ocr_used": false
        },
        {
          "page": 8,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nCode\nDescription\n0\nLog in failure. Most likely an incorrect username/password combo.\n1\nSuccessful log in*\n2\nAdmin is now locked out\n3\nTwo-factor Authentication is needed**\n* In some cases users may receive a successful login status but not be completely authenticated, such as when there is\na post-login-banner configured.\n** For Two-Factor log in, make another POST request with the same username and password, but include the token_\ncode parameter with the value of the one-time-password.\nOnce you've received a successful login status, read each Set-Cookie header and retain the following Cookies:\nName\nDescription\nAPSCOOKIE_<NUMBER>\nThis cookie authenticates you with the FortiGate . You must present this cookie\nwith every subsequent request you make after logging in.\nccsrftoken\nThis is the (c)CSRF token. As described in Authorization on page 11, you must\nprovide the value of this cookie as a X-CSRFTOKEN header. *\n*There may be two ccsrftoken cookies, one with a number suffix that matches the APSCOOKIE. For simplicity, you\ndon't need to locate that cookie and can rely on the ccsrftoken cookie.\nLogging out of an Authenticated Session\nAuthenticated sessions will remain active with the device until any of the following occurs:\nl\nThe admin logs out\nl\nThe session remains inactive for longer than the timeout specified by the admintimeout setting in config\nsystem global\nl\nThe admin is disconnected by another admin\nThere are a limited number of admins that can have active sessions on the device, therefore it's recommended that you\nlog out when you're finished using the device.\nTo log out, a POST request to the /logout URL will remove the current session.\nLogout URL\n/logout\nBody data\nnone needed\nToken-based authentication\nThe authentication is done via a single API token. This token is only generated once when creating an API admin. The\nuser must store this token in a safe place because it cannot be retrieved again. The user can however regenerate the\ntoken at any time. Each API request must include the token in order to be authenticated as the associated API admin.\nIntroduction\n8",
          "char_count": 2145,
          "ocr_used": false
        },
        {
          "page": 9,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nOnly HTTPS access is allowed with token-based authentication to ensure maximum security.\nCreate API admin\nIn order to use the token-based authentication, user must first create a special API admin. The user can assign vdom\nprovision and admin profile to this API admin which defines the admin's privileges.\nOnly Super admin can create or modify API admin.\nconfig system api-user\nedit \"api-admin\"\nset comments \"admin for API access only\"\nset api-key ENC SH23sQt? +/9D9/mKb1oQoDvlP32ggn/cpQeGcY/VGUe5S5WIr5nqU20xcNMYDQE=\nset accprofile \"API profile\"\nset vdom \"root\"\nconfig trusthost\nedit 1\nset ipv4-trusthost 192.168.10.0 255.255.255.0\nnext\nend\nnext\nend\nGUI does not allow user to pick super admin or prof_admin profile for API admin to\nencourage user to use a special profile.\nTrusted host\nAt least one trusted host must be configured for the API admin. The user can define multiple trusted host/subnet. IPv6\nhosts are also supported.\nPKI Certificate\nToken-based also supports certificate matching as an extra layer of security (set PKI group in api-user). Both client\ncertificate and token must match to be granted access to the API. PKI option is enabled by default.\nCORS permission\nToken-based also supports Cross Origin Resource Sharing (CORS) allowing third-party web apps to make API requests\nto FGT using the token. CORS is disabled by default.\nIntroduction\n9",
          "char_count": 1407,
          "ocr_used": false
        },
        {
          "page": 10,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nGenerate API token\nAfter creating the api-user, user can generate new token via CLI command, GUI, or REST API. The token is only shown\nonce and cannot be retrieved after. The user needs to generate new token if they forget.\nCLI command:\nexecute api-user generate-key [API user name]\nNew API key: fccys3cfbhyhqbqghkyzm1QGNnm31r\nThe bearer of this API key will be granted all access privileges assigned to the api-user api-\nadmin.\nREST API\nRequest\nBody data\nDescription\nPOST /api/v2/monitor/system/api-\nuser/generate-key?vdom=root\n{'api-user':\"api-\nadmin\"}\n\"Generate a new api-key for the specified\napi-key-auth admin. The old api-key will be\nreplaced. The response contains the only\nchance to read the new api-key plaintext in\nthe access_token field.\"\nUse the API token\nThe API token can be included in any REST API request via either request header or URL parameter\nPassing API token via request header\nThe user needs to explicitly add the following field to the request header: 'Authorization': 'Bearer ' +\n[api_token]\nAuthorization: Bearer fccys3cfbhyhqbqghkyzm1QGNnm31r\nPassing API token via request URL parameter\nThe user needs to explicitly include the following field in the request URL parameter: access_token=[api_token]\nMethod\nURL\nBody\ndata\nDescription\nGET\n/api/v2/cmdb/firewall/address\n?vdom=root&access_\ntoken=fccys3cfbhyhqbqghkyzm1QGNnm31r\nRetrieve all IPv4\nfirewall\naddresses,\nvdom root\nIntroduction\n10",
          "char_count": 1457,
          "ocr_used": false
        },
        {
          "page": 11,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nAuthorization\nAfter the request is authenticated, the API will check if the associated admin has the permission to perform the\noperation. Each admin or API admin has an admin profile and vdom scope which define the privileges of the admin. For\nexample, if the admin has vdom scope set to \"vdom1\" and a profile that only has read-only permission to Firewall\nobjects access group, the admin can only access vdom1 resource, and cannot make change to Firewall objects (policy,\naddress, etc).\nEach endpoint requires specific group permission defined in 'Access Group' of the endpoint summary table. Request to\nthe endpoint will be checked against this access group to ensure the admin has proper permission to access the\nresource. Make sure the administrative account you login with has the permissions required to perform the intended\nactions.\nAdmin with read-only permission to the resource can only send read requests (HTTP GET) to the resource. Admin with\nwrite permission to the resource can send read/write requests (HTTP GET/POST/PUT/DELETE) to the resource. Admin\nwith no permission to the resource cannot access the resource.\nRequest with insufficient profile permission will return 403 error.\nSupported HTTP methods\nFortiOS REST APIs support the following HTTP methods:\nHTTP Method\nDescription\nGET\nRetrieve a resource or collection of resources.\nPOST\nCreate a resource or execute actions.\nPUT\nUpdate a resource.\nDELETE\nDelete a resource or collection of resources.\nFor any action other than GET, you must provide the X-CSRFTOKEN header in the request. The value of this header is\nthe value of the ccsrftoken cookie that is provided by the FortiGate when you log in.\nIf the request is submitted using HTTP POST , the HTTP method can also be overridden using the \"X-HTTP-Method-\nOverride\" HTTP header.\nResponse codes\nFortiOS APIs use well-defined HTTP status codes to indicate the results of queries to the API.\nThe following table shows how some of the HTTP status codes are used in the context of FortiOS REST APIs.\nIntroduction\n11",
          "char_count": 2078,
          "ocr_used": false
        },
        {
          "page": 12,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nHTTP Response Code\nDescription\n200 - OK\nRequest returns successful.\n400 - Bad Request\nRequest cannot be processed by the API.\n401 - Not Authorized\nRequest without successful login session.\n403 - Forbidden\nRequest is missing CSRF token or administrator is missing access profile\npermissions.\n404 - Resource Not Found\nUnable to find the specified resource.\n405 - Method Not Allowed\nSpecified HTTP method is not allowed for this resource.\n413 - Request Entity Too Large\nRequest cannot be processed due to large entity.\n424 - Failed Dependency\nFail dependency can be duplicate resource, missing required parameter, missing\nrequired attribute, invalid attribute value.\n429 - Too many requests\nThe request is actively blocked by FGT due to a rate limit. For example, if an\nadmin uses invalid credentials too many times, there will be a timeout before\nthey can try again.\n500 - Internal Server Error\nInternal error when processing the request.\nDebugging\nVerbose debug output can be enabled in the FortiGate CLI with the following commands:\ndiagnose debug enable\ndiagnose debug application httpsd -1\nThis will produce the following output when the REST API for IPv4 policy statistics is queried:\n[httpsd 228 - 1418751787] http_config.c[558] ap_invoke_handler -- new request (handler='api_\nmonitor_v2-handler', uri='/api/v2/monitor/firewall/policy', method='GET')\n[httpsd 228 - 1418751787] http_config.c[562] ap_invoke_handler -- User-Agent: Mozilla/5.0\n(Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko)\nChrome/39.0.2171.71 Safari/537.36\n[httpsd 228 - 1418751787] http_config.c[565] ap_invoke_handler -- Source: 192.168.1.100:56256\nDestination: 192.168.1.99:443\n[httpsd 228 - 1418751787] api_monitor.c[1427] api_monitor_v2_handler -- received api_monitor_\nv2_request from '192.168.1.100'\n[httpsd 228 - 1418751787] aps_access.c[3652] aps_chk_rolebased_perm -- truncated URI\n(/api/v2/monitor/firewall/policy) to (/api/v2/monitor) for permission check\n[httpsd 228 - 1418751787] api_monitor.c[1265] handle_req_v2_vdom -- attempting to change from\nvdom \"root\" to vdom \"root\"\n[httpsd 228 - 1418751787] api_monitor.c[1280] handle_req_v2_vdom -- new API request\n(action='select',path='firewall',name='policy',vdom='root',user='admin')\n[httpsd 228 - 1418751787] api_monitor.c[1286] handle_req_v2_vdom -- returning to original vdom\n\"root\"\n[httpsd 228 - 1418751787] http_config.c[581] ap_invoke_handler -- request completed\n(handler='api_monitor_v2-handler' result==0)\nIntroduction\n12",
          "char_count": 2531,
          "ocr_used": false
        },
        {
          "page": 13,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nThis debug will also include all requests to/from the FortiOS web interface, in addition to\nREST API requests.\nIntroduction\n13",
          "char_count": 168,
          "ocr_used": false
        },
        {
          "page": 14,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nCMDB API\nCMDB API is used to retrieve and modify CLI configurations. For example, create/edit/delete firewall policy.\nURL path\nAll CMDB requests start with '/api/v2/cmdb/'. Below is the format of CMDB URL path.\n/api/v2/cmdb/<path>/<name>/<mkey>(optional)/<child_name>(optional)/<child_mkey>(optional)/\nCMDB URL path follows CLI commands syntax with an exception of vdom configuration.\nCLI\nCommand\npath\nname\nmkey\nchild_\nname\nchild_\nmkey\nFull URL\nconfigure vdom\nsystem\nvdom\n/api/v2/cmdb/system/vdom/\nconfigure vdom, edit vdom1\nsystem\nvdom\nvdom1\n/api/v2/cmdb/system/\nvdom/vdom1/\nconfigure firewall schedule\nrecurring\nfirewall.\nschedule\nrecurring\n/api/v2/cmdb/\nfirewall.schedule/recurring/\nconfigure firewall policy\nfirewall\npolicy\n/api/v2/cmdb/firewall/policy/\nconfigure firewall policy,\nedit 1\nfirewall\npolicy\n1\n/api/v2/cmdb/firewall/policy/1/\nconfigure firewall policy,\nedit 1, set srcintf\nfirewall\npolicy\n1\nsrcintf\n/api/v2/cmdb/firewall/\npolicy/1/srcintf/\nconfigure firewall policy,\nedit 1, delete srcintf lan\nfirewall\npolicy\n1\nsrcintf\nlan\n/api/v2/cmdb/firewall/\npolicy/1/srcintf/lan/\nFor operations on the entire table, mkey is not needed. For instance, add new entry, get all entries, purge table.\nFor operations on a specific resource, mkey is required. For example, edit/delete/clone/move a firewall policy.\nFor operations on the child table, child_name is required. For example, retrieve child table, purge child table, add new\nentry to child table.\nFor operations on the child table entry, child_mkey is required. For example, delete/move child object.\nURL parameters\nIn addition to the URL path, user can specify URL parameters which are appended to the URL path.",
          "char_count": 1712,
          "ocr_used": false
        },
        {
          "page": 15,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nGeneric parameters\nThe following URL parameters are generic to all CMDB requests.\nURL parameter\nExample\nDescription\nvdom=root\nGET\n/api/v2/cmdb/firewall/address/?vdom=root\nReturn result/apply changes on the specified\nvdom. If vdom parameter is not provided, use\ncurrent vdom instead. If admin does not have\naccess to the vdom, return permission error.\nglobal=1\nGET\n/api/v2/cmdb/firewall/address/?global=1\nReturn a list of results/apply changes on all\nprovisioned vdoms. The request is only\napplicable to vdoms that the admin has access\nto.\nSpecific parameters\nEach CMDB method may require extra URL parameters which are unique to the method. Those extra parameters are\ndocumented in the \"Extra Parameters\" section of each CMDB method.\nBelow are some examples.\nURL parameter\nExample\nDescription\naction=schema\nGET /api/v2/cmdb/firewall/policy\n/?action=schema\nReturn schema of the resource table\naction=default\nGET /api/v2/cmdb/firewall/policy\n/?action=default\nReturn default attributes of the resource\naction=move\nPUT /api/v2/cmdb/firewall/policy/1\n/?action=move&after=2\nMove policy 1 to after policy 2\naction=clone\nPOST\n/api/v2/cmdb/firewall/address/address1\n/?action=clone&nkey=address1_clone\nClone 'address1' to 'address1_clone'\nskip=1\nGET\n/api/v2/cmdb/firewall/policy/?skip=1\nReturn a list of all firewall policy but only show\nrelevant attributes\nskip=1\nGET\n/api/v2/cmdb/firewall/policy/1/?skip=1\nReturn firewall policy 1 but only show relevant\nattributes\nformat=policyid|action\nGET /api/v2/cmdb/firewall/policy\n/?format=policyid|action\nReturn a list of all firewall policy, however,\nonly show policyid and action for each policy\nformat=policyid|action\nGET /api/v2/cmdb/firewall/policy\n/1?format=policyid|action\nReturn firewall policy 1, however, only show\npolicyid and action\nstart=0&count=10\nGET /api/v2/cmdb/firewall/address\n/?start=0&count=10\nReturn the first 10 firewall addresses\nCMDB API\n15",
          "char_count": 1940,
          "ocr_used": false
        },
        {
          "page": 16,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nURL parameter\nExample\nDescription\nkey=type&pattern=fqdn\nGET /api/v2/cmdb/firewall/address\n/?key=type&pattern=fqdn\nReturn all addresses with type fqdn\nfilter=type==fqdn\nGET /api/v2/cmdb/firewall/address\n/?filter=type==fqdn\nReturn all addresses with type fqdn\nfilter=type==\nfqdn,type==ipmask&filter=\nvisibility==enable\nGET /api/v2/cmdb/firewall/address\n/?filter=type==fqdn,type==\nipmask&filter=visibility==enable\nReturn all addresses with type fqdn or ipmask\nwhich has visibility enabled\nBody data\nBeside URL parameters, some POST/PUT requests also require body data, which must be included in the HTTP body.\nFor example, to create/edit firewall address object, user needs to specify the new/edit object data.\nGET/DELETE requests do not accept body data.\nRequest\nBody data\nDescription\nPOST /api/v2/cmdb/firewall/address?vdom=root\n{'name':\"address1\", 'type':\n\"ipmask\", 'subnet': \"1.1.1.0\n255.255.255.0\"}\ncreate new firewall address\nwith the specified data\nPUT\n/api/v2/cmdb/firewall/address/address1?vdom=root\n{'subnet': \"2.2.2.0\n255.255.255.0\"}\nedit firewall address with the\nspecified data\nLimitation\nIf the body data has the same name as some reserved URL parameters, such as name, path, or action, the request\nwould fail due to the conflict. For example, firewall policy has 'name' and 'action' attribute which conflict with the\nreserved URL parameter 'name' and 'action'. POST/PUT with normal method would fail with 405 error. A workaround is\nto enclosed all object data in a 'json' keyword so the API can correctly identify object data. For example:\nRequest\nBody data\nDescripti\non\nPOST\n/api/v2/cmdb/firewall/policy?vdom\n=root\n{'name':\"test_policy\", 'srcintf': [{\"name\":\"port1\"}], 'dstintf':\n[{\"name\":\"port2\"}],'srcaddr': [{\"name\":\"all\"}],'dstaddr':\n[{\"name\":\"all\"}],'action':\"accept\",'status':\"enable\",'schedule':\"always\n\",'service':[{'name':\"ALL\"}],'nat':\"disable\"}\nThis\nwould fail\nwith 405\nerror\nPOST\n/api/v2/cmdb/firewall/policy?vdom\n=root\n{'json':{'name':\"test_policy\", 'srcintf': [{\"name\":\"port1\"}], 'dstintf':\n[{\"name\":\"port2\"}],'srcaddr': [{\"name\":\"all\"}],'dstaddr':\n[{\"name\":\"all\"}],'action':\"accept\",'status':\"enable\",'schedule':\"always\n\",'service':[{'name':\"ALL\"}],'nat':\"disable\"}}\nThis\nwould\nwork\nCMDB API\n16",
          "char_count": 2264,
          "ocr_used": false
        },
        {
          "page": 17,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nFilter with multiple key/value pairs\nFiltering multiple key/value pairs are also supported for all CMDB retrieval requests via 'filter' URL parameter.\nFilter Syntax\nFilters are defined in the following syntax: key operator pattern\nKey\nOperator\nPattern\nFull Request\nDescription\nschedule\n==\nalways\nGET\n/api/v2/cmdb/firewall/policy/?filter=\nschedule==always\nOnly return firewall policy with\nschedule 'always'\nschedule\n!=\nalways\nGET\n/api/v2/cmdb/firewall/policy/?filter=\nschedule!=always\nReturn all firewall policy with\nschedule other than 'always'\nFilter Operators\nOperator\nDescription\n==\nCase insensitive match with pattern.\n!=\nDoes not match with pattern (case insensitive).\n=@\nPattern found in object value (case insensitive).\n!@\nPattern not found in object value (case insensitive).\n<=\nValue must be less than or equal to pattern.\n<\nValue must be less than pattern.\n>=\nValue must be greater than or equal to pattern.\n>\nValue must be greater than pattern.\nCombining Filters\nFilters can be combined to create complex queries.\nCombination\nDescription\nExample\nLogical OR\nSeparate filters using commas \",\". The\nfollowing example returns all policies\nusing the always schedule or the once\nschedule.\nGET /api/v2/cmdb/firewall/policy?filter=\nschedule==always,schedule==once\nCMDB API\n17",
          "char_count": 1320,
          "ocr_used": false
        },
        {
          "page": 18,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nCombination\nDescription\nExample\nLogical AND\nFilter strings can be combined to create\nlogical AND queries by including multiple\nfilters in the request. This example\nincludes all policies using schedule\nalways AND action accept.\nGET /api/v2/cmdb/firewall/policy/?filter=\nschedule==always&filter=action==accept\nCombining\nAND and OR\nYou can combine AND and OR filters\ntogether to create more complex filters.\nThis example includes all policies using\nschedule always AND action accept OR\naction deny.\nGET /api/v2/cmdb/firewall/policy/?filter=\nschedule==always&filter=action==accept,action==deny\nReserved Characters\nThe following characters need to be escaped if they are part of a filter pattern.\nCharacter\nEscaped Value\n,\n\\,\n\\\n\\\\\nList of Methods\nType\nHTTP Method\nAction\nSummary\ncollection\nGET\nSelect all entries in a CLI table.\nresource\nGET\ndefault\nReturn the CLI default values for this object type.\nresource\nGET\ndefault\nReturn the CLI default values for entire CLI tree.\nresource\nGET\nschema\nReturn the CLI schema for this object type.\nresource\nGET\nschema\nReturn schema for entire CLI tree.\ncollection\nDELETE\nDelete all objects in this table.\ncollection\nPOST\nCreate an object in this table.\nresource\nGET\nSelect a specific entry from a CLI table.\nresource\nPUT\nUpdate this specific resource.\nresource\nPUT\nmove\nMove this specific resource.\nresource\nPOST\nclone\nClone this specific resource.\nCMDB API\n18",
          "char_count": 1437,
          "ocr_used": false
        },
        {
          "page": 19,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nType\nHTTP Method\nAction\nSummary\nresource\nDELETE\nDelete this specific resource.\nresource\nGET\nBuild API directory.\ncollection\nGET\nSummary\nSelect all entries in a CLI table.\nHTTP Method\nGET\nETag Caching\nEnabled\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\ndatasource\nboolean\nEnable to include datasource information for each linked object.\nNo\nstart\nint\nStarting entry index.\nNo\ncount\nint\nMaximum number of entries to return.\nNo\nwith_meta\nboolean\nEnable to include meta information about each object (type id,\nreferences, etc).\nNo\nskip\nboolean\nEnable to call CLI skip operator to hide skipped properties.\nNo\nformat\nstring\nList of property names to include in results, separated by | (i.e.\npolicyid|srcintf).\nNo\nfilter\nstring\nComma separated list of key value pairs to filter on. Filters will be\nlogically OR'd together.\nNo\nkey\nstring\nIf present, objects will be filtered on property with this name.\nNo\npattern\nstring\nIf present, objects will be filtered on property with this value.\nNo\nresource\nGET: default\nSummary\nReturn the CLI default values for this object type.\nCMDB API\n19",
          "char_count": 1136,
          "ocr_used": false
        },
        {
          "page": 20,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nHTTP Method\nGET\nETag Caching\nEnabled\nResponse Type\nobject\nGET: default\nSummary\nReturn the CLI default values for entire CLI tree.\nHTTP Method\nGET\nResponse Type\nobject\nGET: schema\nSummary\nReturn the CLI schema for this object type.\nHTTP Method\nGET\nETag Caching\nEnabled\nResponse Type\nobject\nGET: schema\nSummary\nReturn schema for entire CLI tree.\nHTTP Method\nGET\nResponse Type\nobject\nDELETE\nSummary\nDelete all objects in this table.\nHTTP Method\nDELETE\nPOST\nSummary\nCreate an object in this table.\nHTTP Method\nPOST\nCMDB API\n20",
          "char_count": 564,
          "ocr_used": false
        },
        {
          "page": 21,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nGET\nSummary\nSelect a specific entry from a CLI table.\nHTTP Method\nGET\nETag Caching\nEnabled\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\ndatasource\nboolean\nEnable to include datasource information for each linked object.\nNo\nwith_meta\nboolean\nEnable to include meta information about each object (type id,\nreferences, etc).\nNo\nskip\nboolean\nEnable to call CLI skip operator to hide skipped properties.\nNo\nformat\nstring\nList of property names to include in results, separated by | (i.e.\npolicyid|srcintf).\nNo\nPUT\nSummary\nUpdate this specific resource.\nHTTP Method\nPUT\nPUT: move\nSummary\nMove this specific resource.\nHTTP Method\nPUT\nExtra parameters\nName\nType\nSummary\nRequired\nbefore\nstring\nThe ID of the resource that this resource will be moved before.\nNo\nafter\nstring\nThe ID of the resource that this resource will be moved after.\nNo\nCMDB API\n21",
          "char_count": 903,
          "ocr_used": false
        },
        {
          "page": 22,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nPOST: clone\nSummary\nClone this specific resource.\nHTTP Method\nPOST\nExtra parameters\nName\nType\nSummary\nRequired\nnkey\nstring\nThe ID for the new resouce to be created.\nNo\nDELETE\nSummary\nDelete this specific resource.\nHTTP Method\nDELETE\nGET\nSummary\nBuild API directory.\nHTTP Method\nGET\nExamples\nRetrieve table\nMethod\nURL\nURL Parameters\nBody\nData\nDescription\nGET\n/api/v2/cmdb/\nfirewall/address\n?vdom=root\nRetrieve all IPv4 firewall addresses,\nvdom root\nGET\n/api/v2/cmdb/\nfirewall/address\n?vdom=root&start=\n0&count=10&skip=1\nRetrieve the first 10 firewall\naddresses, skip inapplicable\nattributes, vdom root\nGET\n/api/v2/cmdb/\nfirewall/address\n?vdom=root&format=name|type\nRetrieve all firewall addresses but only\nshow name and type, vdom root\nGET\n/api/v2/cmdb/\nfirewall/address\n?vdom=root&key=\ntype&pattern=fqdn\nRetrieve all fqdn firewall addresses,\nvdom root\nCMDB API\n22",
          "char_count": 905,
          "ocr_used": false
        },
        {
          "page": 23,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nMethod\nURL\nURL Parameters\nBody\nData\nDescription\nGET\n/api/v2/cmdb/\nfirewall/address\n?vdom=root&filter=type==fqdn\nRetrieve all fqdn firewall addresses,\nvdom root\nGET\n/api/v2/cmdb/\nfirewall/address\n?vdom=root&filter=\ntype==fqdn,type==iprange\nRetrieve all fqdn or iprange firewall\naddresses, vdom root\nGET\n/api/v2/cmdb/\nfirewall/address\n?vdom=root&filter=type==\nfqdn&filter=associated-\ninterface==lan\nRetrieve all fqdn firewall addresses\nthat belong to lan interface, vdom root\nRetrieve table schema\nMethod\nURL\nURL Parameters\nBody\nData\nDescription\nGET\n/api/v2/cmdb/firewall/address\n?action=schema\nRetrieve firewall address object's schema\nRetrieve table default\nMethod\nURL\nURL Parameters\nBody\nData\nDescription\nGET\n/api/v2/cmdb/firewall/address\n?action=default\nRetrieve firewall address object's default\nPurge table\nMethod\nURL\nURL Parameters\nBody\nData\nDescription\nDELETE\n/api/v2/cmdb/firewall/address\n?vdom=root\nPurge all firewall addresses, vdom root\nRetrieve object\nMethod\nURL\nURL Parameters\nBody\nData\nDescription\nGET\n/api/v2/cmdb/\nfirewall/address/address1\n?action=select&vdom=root\nRetrieve only firewall address\n'address1', vdom root\nCMDB API\n23",
          "char_count": 1186,
          "ocr_used": false
        },
        {
          "page": 24,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nCreate object\nMethod\nURL\nURL\nParameters\nBody Data\nDescription\nPOST\n/api/v2/cmdb/firewall/address\n?vdom=root\n{\"name\":\"address1\"}\nCreate firewall address\n'address1', root vdom\nPOST\n/api/v2/cmdb/application/list\n?vdom=root\n{\"name\":\"profile1\"}\nCreate application list profile1,\nvdom root\nEdit object\nMethod\nURL\nURL Parameters\nBody Data\nDescription\nPUT\n/api/v2/cmdb/firewall/\naddress/address1\n?vdom=root\n{\"name\":\"address2\"}\nRename 'address1' to 'address2',\nvdom root\nPUT\n/api/v2/cmdb/firewall/\naddress/address1\n?vdom=root\n{\"comment\":\"test\ncomment\"}\nEdit 'address1' to update comment\n'test comment', vdom root\nPUT\n/api/v2/cmdb/\napplication/list/profile1\n?vdom=root\n{\"entries\":[{\"id\":1,\n\"application\":\n[{\"id\":31236},\n{\"id\":31237}]}]}\nEdit profile1 to add child object '1'\nwhich has child table 'applications',\nvdom root\nDelete object\nMethod\nURL\nURL\nParameters\nBody\nData\nDescription\nDELETE\n/api/v2/cmdb/firewall/address/address1\n?vdom=root\nDelete firewall address 'address1',\nroot vdom\nClone object\nMethod\nURL\nURL Parameters\nBody\nData\nDescription\nPOST\n/api/v2/cmdb/\nfirewall/address/address1\n?vdom=root&action=\nclone&nkey=address1_\nclone\nClone 'address1' to 'address1_clone', root\nvdom\nCMDB API\n24",
          "char_count": 1231,
          "ocr_used": false
        },
        {
          "page": 25,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nMove object\nMethod\nURL\nURL Parameters\nBody\nData\nDescription\nPUT\n/api/v2/cmdb/\nfirewall/policy/1\n?vdom=root&action=\nmove&after=2\nMove policy 1 to after policy 2, root vdom\nAppend child object\nMethod\nURL\nURL Parameters\nBody\nData\nDescription\nPOST\n/api/v2/cmdb/application\n/list/profile1/entries\n?vdom=root\n{\"id\":3}\nAdd 3 to application profile1 child table\nentries, vdom root\nEdit child object\nMethod\nURL\nURL Parameters\nBody Data\nDescription\nPUT\n/api/v2/cmdb/application\n/list/profile1/entries/3\n?vdom=root\n{\"application\":\n[{\"id\":31236},\n{\"id\":31237}]}\nEdit child entry 3 to update child\napplication list, vdom root\nDelete child object\nMethod\nURL\nURL Parameters\nBody\nData\nDescription\nDELETE\n/api/v2/cmdb/application\n/list/profile1/entries/3\n?vdom=root\nDelete 3 from application profile1 child\ntable entries, vdom root\nPurge child table\nMethod\nURL\nURL Parameters\nBody\nData\nDescription\nDELETE\n/api/v2/cmdb/application\n/list/profile1/entries\n?vdom=root\nPurge application profile1 child table\nentries, vdom root\nCMDB API\n25",
          "char_count": 1058,
          "ocr_used": false
        },
        {
          "page": 26,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nRetrieve complex table\nMethod\nURL\nURL Parameters\nBody\nData\nDescription\nGET\n/api/v2/cmdb/vpn.ssl/settings\n?action=select\nRetrieve vpn ssl settings object\nEdit complex table\nMethod URL\nURL Parameters\nBody Data\nDescription\nPUT\n/api/v2/cmdb/\nvpn.ssl/settings\n?vdom=root\n{\"authentication-\nrule\":[{\"id\":\"1\"},\n{\"id\":\"2\"}]}\nEdit complex object vpn.ssl.settings to\ncreate/modify child table, vdom root\nGlobal requests (apply to all accessible vdoms)\nMethod\nURL\nURL Parameters\nBody Data\nDescription\nGET\n/api/v2/cmdb/\nfirewall/address\n?global=1\nRetrieve all IPv4 firewall addresses,\nall accessible vdoms\nPOST\n/api/v2/cmdb/\nfirewall/address\n?global=1\n{\"name\":\"address1\"}\nCreate firewall address 'address1' for\nall accessible vdoms\nDELETE\n/api/v2/cmdb/firewall/\naddress/address1\n?global=1\nDelete firewall address 'address1' for\nall accessible vdoms\nCMDB API\n26",
          "char_count": 889,
          "ocr_used": false
        },
        {
          "page": 27,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nMonitor API\nMonitor API is used to perform specific actions on endpoint resources. For example, retrieve/close firewall sessions,\nrestart/shutdown FGT, backup/restore config file.\nURL path\nAll Monitor API requests start with '/api/v2/monitor/'. Below is the format of Monitor URL path:\n/api/v2/monitor/<uri>/\nEach Monitor endpoint has a specific URI, which are provided by the URI field of each endpoint.\nURI\nFull URL\nDescription\n/firewall/policy/\nGET\n/api/v2/monitor/firewall/policy/\nList traffic statistics for all IPv4 policies\n/firewall/policy/reset\nPOST\n/api/v2/monitor/firewall/policy/reset\nReset traffic statistics for all IPv4 policies\nURL parameters\nIn addition to the URL path, user can specify URL parameters which are appended to the URL path.\nGeneric parameters\nThe following URL parameters are generic to all Monitor requests.\nURL parameter\nExample\nDescription\nvdom=root\nGET /api/v2/monitor/\nfirewall/policy/?vdom=root\nReturn result/apply changes on the specified vdom. If\nvdom parameter is not provided, use current vdom\ninstead. If admin does not have access to the vdom,\nreturn permission error.\nglobal=1\nGET /api/v2/monitor/\nfirewall/policy/?global=1\nReturn a list of results/apply changes on all provisioned\nvdoms. The request is only applicable to vdoms that the\nadmin has access to.",
          "char_count": 1345,
          "ocr_used": false
        },
        {
          "page": 28,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nSpecific parameters\nEach Monitor endpoint may require extra URL parameters which are unique to the endpoint. Those extra parameters\nare documented in the \"Extra Parameters\" section of each endpoint.\nRequired parameters are marked with \"required: true\" flag.\nBelow are some examples.\nURL parameter\nExample\nDescription\ncount=-1\nGET\n/api/v2/monitor/firewall/session?count=1\nReturn all ipv4 firewall sessions\nip_version=ipv6&count=10\nGET /api/v2/monitor/firewall/\nsession?ip_version=ipv6&count=10\nReturn the first 10 ipv6 firewall\nsessions\nBody data\nBeside URL parameters, some POST requests also require body data, which must be included in the HTTP body. The\nextra body data are documented in \"Extra Parameters\" section of each endpoint.\nGET requests do not accept body data.\nRequired body data are marked with \"required: true\" flag.\nBelow are some examples.\nRequest\nBody Data\nDescription\nPOST /api/v2/monitor/firewall/\nsession/close?vdom=root\n{'pro': \"udp\", 'saddr': \"192.168.100.110\",\n'daddr': \"96.45.33.73\", 'sport': 55933,\n'dport': 8888}\nClose the specific ipv4 firewall\nsessions\nFile upload\nFile upload is supported for some endpoints. For example, upload VM license, restore config file. The upload file must\nbe stored in the HTTP body. There are two different methods to do so: via JSON data or multi-part file.\nFile upload via JSON data\nThe upload file can be encoded directly into the HTTP body as JSON data using the 'file_content' field.\nThe JSON data must be encoded in base64 format.\nFor instance, below is how you can upload/restore config file via JSON data using Python Requests module.\nself.session.post(url='/api/v2/monitor/system/config/restore',\nparams={\"vdom\": \"vdom1\"},\nMonitor API\n28",
          "char_count": 1746,
          "ocr_used": false
        },
        {
          "page": 29,
          "text": "FortiOS REST API Reference\nFortinet Inc.\ndata={\"source\": \"upload\",\n\"scope\": \"vdom\",\n\"file_content\": b64encode(open(\"vd1.conf.txt\", \"r\").read())})\nFile upload via multi-part file\nAnother way to store upload file in HTTP body is to include it as a multi-part file.\nThe multi-part file does not need to be encoded in base64 format.\nFor instance, below is how you can upload/restore config file via multi-part file using Python Requests module.\nself.session.post(url='/api/v2/monitor/system/config/restore',\nparams={\"vdom\": \"vdom1\"},\ndata={\"source\": \"upload\",\n\"scope\": \"vdom\"},\nfiles=[('random_name',\n('random_conf.conf', open(\"vd1.conf.txt\", \"r\"), 'text/plain'))])\nFile download\nFile download is also supported in some endpoints. For example, download CA certificate, backup config file.\nThe downloaded file is stored in the response's raw content, not JSON data.\nFor example, here is the request to download global certificate name Fortinet_Factory, type local, scope\nglobal:\nGET /api/v2/monitor/system/certificate/download?mkey=Fortinet_Factory&type=local&scope=global\nFile download via browser\nWhen sending file download request via a browser, the browser automatically checks the response's header for\n'Content-Disposition': attachment. If present, the browser will download the file to local directory using the\nname.\nFile download via script\nWhen sending file download request via a script, the script will need to manually perform the above steps to convert the\nresponse's content into a file. For example, the script needs to check the response header for 'Content-\nDisposition': attachment, and write the content into a local file with the given name.\nMonitor API\n29",
          "char_count": 1673,
          "ocr_used": false
        },
        {
          "page": 30,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nList of Methods\nURI\nHTTP Method\nSummary\nendpoint-control/profile/xml/\nGET\nList XML representation for each endpoint-control profile.\nendpoint-control/registration-\npassword/check/\nPOST\nCheck if provided registration password is valid for current\nVDOM.\nendpoint-control/record-\nlist/select/\nGET\nList endpoint records.\nendpoint-\ncontrol/registration/summary/\nGET\nSummary of FortiClient registrations.\nendpoint-\ncontrol/registration/quarantine/\nPOST\nQuarantine endpoint by FortiClient UID or MAC.\nendpoint-\ncontrol/registration/unquarantine/\nPOST\nUnquarantine endpoint by FortiClient UID or MAC.\nendpoint-\ncontrol/registration/block/\nPOST\nBlock endpoint by FortiClient UID or MAC.\nendpoint-\ncontrol/registration/unblock/\nPOST\nUnblock endpoint by FortiClient UID or MAC.\nendpoint-\ncontrol/registration/deregister/\nPOST\nDeregister endpoint by FortiClient UID or MAC.\nendpoint-control/installer/select/\nGET\nList available FortiClient installers.\nendpoint-\ncontrol/installer/download/\nGET\nDownload a FortiClient installer via FortiGuard.\nendpoint-\ncontrol/avatar/download/\nGET\nDownload an endpoint avatar image.\nfirewall/health/select/\nGET\nList configured load balance server health monitors.\nfirewall/local-in/select/\nGET\nList implicit and explicit local-in firewall policies.\nfirewall/acl/select/\nGET\nList counters for all IPv4 ACL.\nfirewall/acl/clear_counters/\nPOST\nReset counters for one or more IPv4 ACLs by policy ID.\nfirewall/acl6/select/\nGET\nList counters for all IPv6 ACL.\nfirewall/acl6/clear_counters/\nPOST\nReset counters for one or more IPv6 ACLs by policy ID.\nfirewall/internet-service-\nmatch/select/\nGET\nList internet services that exist at a given IP or Subnet.\nfirewall/policy/select/\nGET\nList traffic statistics for all IPv4 policies.\nMonitor API\n30",
          "char_count": 1800,
          "ocr_used": false
        },
        {
          "page": 31,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nURI\nHTTP Method\nSummary\nfirewall/policy/reset/\nPOST\nReset traffic statistics for all IPv4 policies.\nfirewall/policy/clear_counters/\nPOST\nReset traffic statistics for one or more IPv4 policies by policy\nID.\nfirewall/policy6/select/\nGET\nList traffic statistics for all IPv6 policies.\nfirewall/policy6/reset/\nPOST\nReset traffic statistics for all IPv6 policies.\nfirewall/policy6/clear_counters/\nPOST\nReset traffic statistics for one or more IPv6 policies by policy\nID.\nfirewall/proxy-policy/select/\nGET\nList traffic statistics for all explicit proxy policies.\nfirewall/proxy-policy/clear_\ncounters/\nPOST\nReset traffic statistics for one or more explicit proxy policies by\npolicy ID.\nfirewall/policy-lookup/select/\nGET\nPerforms a policy lookup by creating a dummy packet and\nasking the kernel which policy would be hit.\nfirewall/session/select/\nGET\nList all active firewall sessions (optionally filtered).\nfirewall/session/clear_all/\nPOST\nImmediately clear all active IPv4 and IPv6 sessions and IPS\nsessions of current VDOM.\nfirewall/session/close/\nPOST\nClose a specific firewall session that matches all provided\ncriteria.\nfirewall/session-top/select/\nGET\nList of top sessions by specified grouping criteria.\nfirewall/shaper/select/\nGET\nList of statistics for configured firewall shapers.\nfirewall/shaper/reset/\nPOST\nReset statistics for all configured traffic shapers.\nfirewall/load-balance/select/\nGET\nList all firewall load balance servers.\nfirewall/address-fqdns/select/\nGET\nList of FQDN address objects and the IPs they resolved to.\nfirewall/ippool/select/\nGET\nList IPv4 pool statistics.\nfirewall/address-dynamic/select/\nGET\nList of Dynamic SDN address objects and the IPs they resolve\nto.\nfirewall/address6-dynamic/select/\nGET\nList of IPv6 Dynamic SDN address objects and the IPs they\nresolve to.\nfortiview/statistics/select/\nGET\nRetrieve drill-down and summary data for FortiView (both\nrealtime and historical).\nfortiview/session/cancel/\nPOST\nCancel a FortiView request session.\nfortiview/sandbox-file-\ndetails/select/\nGET\nRetrieve FortiSandbox analysis details for a specific file\nchecksum.\nMonitor API\n31",
          "char_count": 2152,
          "ocr_used": false
        },
        {
          "page": 32,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nURI\nHTTP Method\nSummary\ngeoip/geoip-query/select/\nGET\nRetrieve location details for IPs queried against FortiGuard's\ngeoip service.\nips/rate-based/select/\nGET\nReturns a list of rate-based signatures in IPS package.\nlicense/status/select/\nGET\nGet current license & registration status.\nlicense/database/upgrade/\nPOST\nUpgrade IPS database on this device using uploaded file.\nlicense/forticare-resellers/select/\nGET\nGet current FortiCare resellers for the requested country.\nlicense/forticare-org-list/select/\nGET\nGet FortiCare organization size and industry lists.\nlog/current-disk-usage/select/\nGET\nReturn current used, free and total disk bytes.\nlog/device/state/\nGET\nRetrieve information on state of log devices.\nlog/forticloud/select/\nGET\nReturn FortiCloud log status.\nlog/fortianalyzer/select/\nGET\nReturn FortiAnalyzer/FortiManager log status.\nlog/fortianalyzer-queue/select/\nGET\nRetrieve information on FortiAnalyzer's queue state. Note:-\nFortiAnalyzer logs are queued only if upload-option is realtime.\nlog/hourly-disk-usage/select/\nGET\nReturn historic hourly disk usage in bytes.\nlog/historic-daily-remote-\nlogs/select/\nGET\nReturns the amount of logs in bytes sent daily to a remote\nlogging service (FortiCloud or FortiAnalyzer).\nlog/stats/select/\nGET\nReturn number of logs sent by category per day for a specific\nlog device.\nlog/stats/reset/\nPOST\nReset logging statistics for all log devices.\nlog/forticloud-report/download/\nGET\nDownload PDF report from FortiCloud.\nlog/ips-archive/download/\nGET\nDownload IPS/application control packet capture files. Uses\nconfigured log display device.\nlog/policy-archive/download/\nGET\nDownload policy-based packet capture archive.\nlog/av-archive/download/\nGET\nDownload file quarantined by AntiVirus.\nrouter/ipv4/select/\nGET\nList all active IPv4 routing table entries.\nrouter/ipv6/select/\nGET\nList all active IPv6 routing table entries.\nrouter/statistics/select/\nGET\nRetrieve routing table statistics, including number of matched\nroutes.\nrouter/lookup/select/\nGET\nPerforms a route lookup by querying the routing table.\nrouter/policy/select/\nGET\nRetrieve a list of active IPv4 policy routes.\nrouter/policy6/select/\nGET\nRetrieve a list of active IPv6 policy routes.\nMonitor API\n32",
          "char_count": 2261,
          "ocr_used": false
        },
        {
          "page": 33,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nURI\nHTTP Method\nSummary\nsystem/admin/toggle-vdom-\nmode/\nPOST\nToggles VDOM mode on/off. Enables or disables VDOM mode\nif it is disabled or enabled respectively.\nsystem/api-user/generate-key/\nPOST\nGenerate a new api-key for the specified api-key-auth admin.\nThe old api-key will be replaced. The response contains the\nonly chance to read the new api-key plaintext in the api_key\nfield.\nsystem/config-revision/select/\nGET\nReturns a list of system configuration revisions.\nsystem/config-revision/update-\ncomments/\nPOST\nUpdates comments for a system configuration file.\nsystem/config-revision/delete/\nPOST\nDeletes one or more system configuration revisions.\nsystem/config-revision/file/\nGET\nDownload a specific configuration revision.\nsystem/config-revision/info/\nGET\nRetrieve meta information for a specific configuration revision.\nsystem/config-revision/save/\nPOST\nCreate a new config revision checkpoint.\nsystem/current-admins/select/\nGET\nReturn a list of currently logged in administrators.\nsystem/disconnect-\nadmins/select/\nPOST\nDisconnects logged in administrators.\nsystem/time/set/\nPOST\nSets current system time stamp.\nsystem/time/select/\nGET\nGets current system time stamp.\nsystem/os/reboot/\nPOST\nImmediately reboot this device.\nsystem/os/shutdown/\nPOST\nImmediately shutdown this device.\nsystem/global-resources/select/\nGET\nRetrieve current usage of global resources as well as both the\ndefault and user configured maximum values.\nsystem/vdom-resource/select/\nGET\nRetrieve VDOM resource information, including CPU and\nmemory usage.\nsystem/dhcp/select/\nGET\nReturns a list of all DHCP IPv4 and IPv6 DHCP leases.\nsystem/dhcp/revoke/\nPOST\nRevoke IPv4 DHCP leases.\nsystem/dhcp6/revoke/\nPOST\nRevoke IPv6 DHCP leases.\nsystem/firmware/select/\nGET\nRetrieve a list of firmware images available to use for upgrade\non this device.\nsystem/firmware/upgrade/\nPOST\nUpgrade firmware image on this device using uploaded file.\nsystem/firmware/upgrade-paths/\nGET\nRetrieve a list of supported firmware upgrade paths.\nMonitor API\n33",
          "char_count": 2055,
          "ocr_used": false
        },
        {
          "page": 34,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nURI\nHTTP Method\nSummary\nsystem/fsck/start/\nPOST\nSet file system check flag so that it will be executed on next\ndevice reboot.\nsystem/storage/select/\nGET\nRetrieve information for the non-boot disk.\nsystem/change-password/select/\nPOST\nSave admin and guest-admin passwords.\nsystem/password-policy-\nconform/select/\nPOST\nCheck whether password conforms to the password policy.\nsystem/csf/select/\nGET\nRetrieve a full tree of downstream FortiGates registered to the\nSecurity Fabric.\nsystem/modem/select/\nGET\nRetrieve statistics for internal/external configured modem.\nsystem/modem/reset/\nPOST\nReset statistics for internal/external configured modem.\nsystem/modem/connect/\nPOST\nTrigger a connect for the configured modem.\nsystem/modem/disconnect/\nPOST\nTrigger a disconnect for the configured modem.\nsystem/modem/update/\nPOST\nUpdate supported modem list from FortiGuard.\nsystem/3g-modem/select/\nGET\nList all 3G modems available via FortiGuard.\nsystem/resource/usage/\nGET\nRetreive current and historical usage data for a provided\nresource.\nsystem/sniffer/select/\nGET\nReturn a list of all configured packet captures.\nsystem/sniffer/restart/\nPOST\nRestart specified packet capture.\nsystem/sniffer/start/\nPOST\nStart specified packet capture.\nsystem/sniffer/stop/\nPOST\nStop specified packet capture.\nsystem/sniffer/download/\nGET\nDownload a stored packet capture.\nsystem/fsw/select/\nGET\nRetrieve statistics for configured FortiSwitches\nsystem/fsw/update/\nPOST\nUpdate administrative state for a given FortiSwitch (enable or\ndisable authorization).\nsystem/fsw/restart/\nPOST\nRestart a given FortiSwitch.\nsystem/fsw/upgrade/\nPOST\nUpgrade firmware image on the given FortiSwitch using\nuploaded file.\nsystem/fsw/poe-reset/\nPOST\nReset PoE on a given FortiSwitch's port.\nsystem/fsw-firmware/select/\nGET\nRetrieve a list of recommended firmware for managed\nFortiSwitches.\nswitch-controller/managed-\nswitch/dhcp-snooping/\nGET\nRetrieve DHCP servers monitored by FortiSwitches.\nMonitor API\n34",
          "char_count": 2005,
          "ocr_used": false
        },
        {
          "page": 35,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nURI\nHTTP Method\nSummary\nswitch-controller/managed-\nswitch/faceplate-xml/\nGET\nRetrieve XML for rendering FortiSwitch faceplate widget.\nswitch-controller/validate-switch-\nprefix/select/\nGET\nValidate a FortiSwitch serial number prefix.\nsystem/interface/select/\nGET\nRetrieve statistics for all system interfaces.\nsystem/available-\ninterfaces/select/\nGET\nRetrieve a list of all interfaces along with some meta\ninformation regarding their availability.\nsystem/acquired-dns/select/\nGET\nRetrieve a list of interfaces and their acquired DNS servers.\nsystem/resolve-fqdn/select/\nGET\nResolves the provided FQDNs to FQDN -> IP mappings.\nsystem/usb-log/select/\nGET\nRetrieve information about connected USB drives, including\nestimated log sizes.\nsystem/usb-log/start/\nPOST\nStart backup of logs from current VDOM to USB drive.\nsystem/usb-log/stop/\nPOST\nStop backup of logs to USB drive.\nsystem/ipconf/select/\nGET\nDetermine if there is an IP conflict for a specific IP using ARP.\nsystem/fortiguard/update/\nPOST\nImmediately update status for FortiGuard services.\nsystem/fortiguard/clear-cache/\nPOST\nImmediately clear all FortiGuard statistics.\nsystem/fortiguard/test-\navailability/\nPOST\nTest availability of FortiGuard services.\nsystem/fortiguard/server-info/\nGET\nGet FortiGuard server list and information.\nsystem/fortimanager/status/\nGET\nGet FortiManager status.\nsystem/fortimanager/config/\nPOST\nConfigure FortiManager address.\nsystem/available-\ncertificates/select/\nGET\nGet available certificates.\nsystem/certificate/download/\nGET\nDownload certificate.\nsystem/debug/select/\nPOST\nLog debug messages to the console (if enabled).\nsystem/debug/download/\nGET\nDownload debug report for technical support.\nsystem/com-log/dump/\nPOST\nDump system com-log to file.\nsystem/com-log/update/\nGET\nFetch system com-log file dump progress.\nsystem/com-log/download/\nGET\nDownload com-log file (after file dump is complete).\nsystem/botnet/stat/\nGET\nRetrieve statistics for FortiGuard botnet database.\nMonitor API\n35",
          "char_count": 2022,
          "ocr_used": false
        },
        {
          "page": 36,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nURI\nHTTP Method\nSummary\nsystem/botnet/select/\nGET\nList all known IP-based botnet entries in FortiGuard botnet\ndatabase.\nsystem/botnet-domains/select/\nGET\nList all known domain-based botnet entries in FortiGuard\nbotnet database.\nsystem/botnet-domains/stat/\nGET\nList statistics on domain-based botnet entries in FortiGuard\nbotnet database.\nsystem/ha-statistics/select/\nGET\nList of statistics for members of HA cluster.\nsystem/ha-history/select/\nGET\nGet HA cluster historical logs.\nsystem/ha-checksums/select/\nGET\nList of checksums for members of HA cluster.\nsystem/ha-peer/select/\nGET\nGet configuration of peer(s) in HA cluster. Uptime is expressed\nin seconds.\nsystem/ha-peer/update/\nPOST\nUpdate configuration of peer in HA cluster.\nsystem/ha-peer/disconnect/\nPOST\nUpdate configuration of peer in HA cluster.\nsystem/link-monitor/select/\nGET\nRetrieve per-interface statistics for active link monitors.\nsystem/compliance/run/\nPOST\nImmediately run compliance checks for the selected VDOM.\nsystem/config/restore/\nPOST\nRestore system configuration from uploaded file or from USB.\nsystem/config/backup/\nGET\nBackup system config\nsystem/config/usb-filelist/\nGET\nList configuration files available on connected USB drive.\nsystem/sandbox/status/\nGET\nRetrieve sandbox status.\nsystem/sandbox/stats/\nGET\nRetrieve sandbox statistics.\nsystem/object/usage/\nGET\nRetrieve all objects that are currently using as well as objects\nthat can use the given object.\nsystem/timezone/select/\nGET\nGet world timezone and daylight saving time.\nsystem/vmlicense/upload/\nPOST\nUpdate VM license using uploaded file. Reboots immediately if\nsuccessful.\nsystem/sensor-info/select/\nGET\nRetrieve system sensor status.\nsystem/audit/select/\nGET\nRetrieve Security Fabric audit results.\nsystem/fortiguard-\nblacklist/select/\nGET\nRetrieve blacklist information for a specified IP.\nvpn-certificate/ca/import/\nPOST\nImport CA certificate.\nvpn-certificate/crl/import/\nPOST\nImport certificate revocation lists (CRL) from file content.\nMonitor API\n36",
          "char_count": 2040,
          "ocr_used": false
        },
        {
          "page": 37,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nURI\nHTTP Method\nSummary\nvpn-certificate/local/import/\nPOST\nImport local certificate.\nvpn-certificate/remote/import/\nPOST\nImport remote certificate.\nvpn-certificate/csr/generate/\nPOST\nGenerate a certificate signing request (CSR) and a private key.\nThe CSR can be retrieved / downloaded from CLI, GUI and\nREST API.\nsystem/check-port-\navailability/select/\nGET\nCheck whether a list of TCP port ranges is available for a\ncertain service.\nextender-\ncontroller/extender/select/\nGET\nRetrieve statistics for specific configured FortiExtender units.\nextender-\ncontroller/extender/reset/\nPOST\nReset a specific FortiExtender unit.\nuser/firewall/select/\nGET\nList authenticated firewall users.\nuser/firewall/deauth/\nPOST\nDeauthenticate single, multiple, or all firewall users.\nuser/banned/select/\nGET\nReturn a list of all banned users by IP.\nuser/banned/clear_users/\nPOST\nImmediately clear a list of specific banned users by IP.\nuser/banned/add_users/\nPOST\nImmediately add one or more users to the banned list.\nuser/banned/clear_all/\nPOST\nImmediately clear all banned users.\nuser/fortitoken/select/\nGET\nRetrieve a map of FortiTokens and their status.\nuser/fortitoken/activate/\nPOST\nActivate a set of FortiTokens by serial number.\nuser/device/select/\nGET\nRetrieve a list of detected devices.\nuser/fortitoken/refresh/\nPOST\nRefresh a set of FortiTokens by serial number.\nuser/fortitoken/provision/\nPOST\nProvision a set of FortiTokens by serial number.\nuser/fortitoken/send-activation/\nPOST\nSend a FortiToken activation code to a user via SMS or Email.\nuser/fsso/refresh-server/\nPOST\nRefresh remote agent group list for all fsso agents.\nuser/fsso/select/\nGET\nGet a list of fsso and fsso polling status.\nutm/rating-lookup/select/\nGET\nLookup FortiGuard rating for a specific URL.\nutm/app-lookup/select/\nGET\nQuery remote FortiFlow database to resolve hosts to\napplication control entries.\nutm/application-\ncategories/select/\nGET\nRetrieve a list of application control categories.\nutm/antivirus/stats/\nGET\nRetrieve antivirus scanning statistics.\nMonitor API\n37",
          "char_count": 2080,
          "ocr_used": false
        },
        {
          "page": 38,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nURI\nHTTP Method\nSummary\nvirtual-wan/health-check/select/\nGET\nRetrieve health-check statistics for each SD-WAN link.\nvirtual-wan/members/select/\nGET\nRetrieve interface statistics for each SD-WAN link.\nwebfilter/override/select/\nGET\nList all administrative and user initiated webfilter overrides.\nwebfilter/override/delete/\nPOST\nDelete a configured webfilter override.\nwebfilter/malicious-urls/select/\nGET\nList all URLs in FortiSandbox malicious URL database.\nwebfilter/malicious-urls/stat/\nGET\nRetrieve statistics for the FortiSandbox malicious URL\ndatabase.\nwebfilter/category-quota/select/\nGET\nRetrieve quota usage statistics for webfilter categories.\nwebfilter/category-quota/reset/\nPOST\nReset webfilter quota for user or IP.\nwebfilter/fortiguard-\ncategories/select/\nGET\nReturn FortiGuard web filter categories.\nwebfilter/trusted-urls/select/\nGET\nList all URLs in FortiGuard trusted URL database.\nvpn/ipsec/select/\nGET\nReturn an array of active IPsec VPNs.\nvpn/ipsec/tunnel_up/\nPOST\nBring up a specific IPsec VPN tunnel.\nvpn/ipsec/tunnel_down/\nPOST\nBring down a specific IPsec VPN tunnel.\nvpn/ipsec/tunnel_reset_stats/\nPOST\nReset statistics for a specific IPsec VPN tunnel.\nvpn/ssl/select/\nGET\nRetrieve a list of all SSL-VPN sessions and sub-sessions.\nvpn/ssl/clear_tunnel/\nPOST\nRemove all active tunnel sessions in current virtual domain.\nvpn/ssl/delete/\nPOST\nTerminate the provided SSL-VPN session.\nvpn/ssl/stats/\nGET\nReturn statistics about the SSL-VPN.\nwanopt/history/select/\nGET\nRetrieve WAN opt. statistics history.\nwanopt/history/reset/\nPOST\nReset WAN opt. statistics.\nwanopt/webcache/select/\nGET\nRetrieve webcache statistics history.\nwanopt/webcache/reset/\nPOST\nReset webcache statistics.\nwanopt/peer_stats/select/\nGET\nRetrieve a list of WAN opt peer statistics.\nwanopt/peer_stats/reset/\nPOST\nReset WAN opt peer statistics.\nwebproxy/pacfile/download/\nGET\nDownload webproxy PAC file.\nwebcache/stats/select/\nGET\nRetrieve webcache statistics.\nwebcache/stats/reset/\nPOST\nReset all webcache statistics.\nwifi/client/select/\nGET\nRetrieve a list of connected WiFi clients.\nMonitor API\n38",
          "char_count": 2131,
          "ocr_used": false
        },
        {
          "page": 39,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nURI\nHTTP Method\nSummary\nwifi/managed_ap/select/\nGET\nRetrieve a list of managed FortiAPs.\nwifi/managed_ap/set_status/\nPOST\nUpdate administrative state for a given FortiAP (enable or\ndisable authorization).\nwifi/firmware/select/\nGET\nRetrieve a list of current and recommended firmware for\nFortiAPs in use.\nwifi/managed_ap/restart/\nPOST\nRestart a given FortiAP.\nwifi/managed_ap/upgrade/\nPOST\nUpgrade firmware image on the given FortiAP using uploaded\nfile.\nwifi/ap_status/select/\nGET\nRetrieve statistics for all managed FortiAPs.\nwifi/interfering_ap/select/\nGET\nRetrieve a list of interfering APs for one FortiAP radio.\nwifi/euclid/select/\nGET\nRetrieve presence analytics statistics.\nwifi/euclid/reset/\nPOST\nReset presence analytics statistics.\nwifi/rogue_ap/select/\nGET\nRetrieve a list of detected rogue APs.\nwifi/rogue_ap/clear_all/\nPOST\nClear all detected rogue APs.\nwifi/rogue_ap/set_status/\nPOST\nMark detected APs as rogue APs.\nwifi/spectrum/select/\nGET\nRetrieve spectrum analysis information for a specific FortiAP.\ncoverage/download/select/\nGET\nDownload code coverage.\nendpoint-control\nprofile: xml\nSummary\nList XML representation for each endpoint-control profile.\nURI\nendpoint-control/profile/xml/\nHTTP Method\nGET\nAction\nxml\nAccess Group\nendpoint-control-grp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nstring\nName of endpoint-control profile.\nNo\nMonitor API\n39",
          "char_count": 1434,
          "ocr_used": false
        },
        {
          "page": 40,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nregistration-password: check\nSummary\nCheck if provided registration password is valid for current VDOM.\nURI\nendpoint-control/registration-password/check/\nHTTP Method\nPOST\nAction\ncheck\nAccess Group\nendpoint-control-grp\nResponse Type\nboolean\nExtra parameters\nName\nType\nSummary\nRequired\npassword\nstring\nRegistration password to test.\nYes\nrecord-list: select\nSummary\nList endpoint records.\nURI\nendpoint-control/record-list/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nendpoint-control-grp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nintf_name\nstring\nFilter: Name of interface where the endpoint was detected.\nNo\nregistration: summary\nSummary\nSummary of FortiClient registrations.\nURI\nendpoint-control/registration/summary/\nHTTP Method\nGET\nAction\nsummary\nAccess Group\nendpoint-control-grp\nMonitor API\n40",
          "char_count": 866,
          "ocr_used": false
        },
        {
          "page": 41,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nregistration: quarantine\nSummary\nQuarantine endpoint by FortiClient UID or MAC.\nURI\nendpoint-control/registration/quarantine/\nHTTP Method\nPOST\nAction\nquarantine\nAccess Group\nendpoint-control-grp\nExtra parameters\nName\nType\nSummary\nRequired\nuid\narray\nArray of FortiClient UIDs to quarantine.\nNo\nuid\nstring\nSingle FortiClient UID to quarantine.\nNo\nmac\narray\nArray of MACs to quarantine.\nNo\nmac\nstring\nSingle MAC to quarantine.\nNo\nregistration: unquarantine\nSummary\nUnquarantine endpoint by FortiClient UID or MAC.\nURI\nendpoint-control/registration/unquarantine/\nHTTP Method\nPOST\nAction\nunquarantine\nAccess Group\nendpoint-control-grp\nExtra parameters\nName\nType\nSummary\nRequired\nuid\narray\nArray of FortiClient UIDs to unquarantine.\nNo\nuid\nstring\nSingle FortiClient UID to unquarantine.\nNo\nmac\narray\nArray of MACs to unquarantine.\nNo\nmac\nstring\nSingle MAC to unquarantine.\nNo\nMonitor API\n41",
          "char_count": 926,
          "ocr_used": false
        },
        {
          "page": 42,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nregistration: block\nSummary\nBlock endpoint by FortiClient UID or MAC.\nURI\nendpoint-control/registration/block/\nHTTP Method\nPOST\nAction\nblock\nAccess Group\nendpoint-control-grp\nExtra parameters\nName\nType\nSummary\nRequired\nuid\narray\nArray of FortiClient UIDs to block.\nNo\nuid\nstring\nSingle FortiClient UID to block.\nNo\nmac\narray\nArray of MACs to block.\nNo\nmac\nstring\nSingle MAC to block.\nNo\nregistration: unblock\nSummary\nUnblock endpoint by FortiClient UID or MAC.\nURI\nendpoint-control/registration/unblock/\nHTTP Method\nPOST\nAction\nunblock\nAccess Group\nendpoint-control-grp\nExtra parameters\nName\nType\nSummary\nRequired\nuid\narray\nArray of FortiClient UIDs to unblock.\nNo\nuid\nstring\nSingle FortiClient UID to unblock.\nNo\nmac\narray\nArray of MACs to unblock.\nNo\nmac\nstring\nSingle MAC to unblock.\nNo\nMonitor API\n42",
          "char_count": 846,
          "ocr_used": false
        },
        {
          "page": 43,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nregistration: deregister\nSummary\nDeregister endpoint by FortiClient UID or MAC.\nURI\nendpoint-control/registration/deregister/\nHTTP Method\nPOST\nAction\nderegister\nAccess Group\nendpoint-control-grp\nExtra parameters\nName\nType\nSummary\nRequired\nuid\narray\nArray of FortiClient UIDs to deregister.\nNo\nuid\nstring\nSingle FortiClient UID to deregister.\nNo\nmac\narray\nArray of MACs to deregister.\nNo\nmac\nstring\nSingle MAC to deregister.\nNo\ninstaller: select\nSummary\nList available FortiClient installers.\nURI\nendpoint-control/installer/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nendpoint-control-grp\nExtra parameters\nName\nType\nSummary\nRequired\nmin_version\nstring\nFilter: Minimum installer version. (String of the format n[.n[.n]]).\nNo\ninstaller: download\nSummary\nDownload a FortiClient installer via FortiGuard.\nURI\nendpoint-control/installer/download/\nHTTP Method\nGET\nMonitor API\n43",
          "char_count": 920,
          "ocr_used": false
        },
        {
          "page": 44,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nAction\ndownload\nAccess Group\nendpoint-control-grp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nstring\nName of installer (image_id).\nYes\navatar: download\nSummary\nDownload an endpoint avatar image.\nURI\nendpoint-control/avatar/download/\nHTTP Method\nGET\nAction\ndownload\nAccess Group\nendpoint-control-grp\nETag Caching\nEnabled\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nuid\nstring\nSingle FortiClient UID.\nNo\nuser\nstring\nUser name of the endpoint.\nNo\nalias\nstring\nAlias of the device. Used to lookup device avatar when endpoint\navatar is not available.\nNo\ndefault\nstring\nDefault avatar name ['authuser'|'unauthuser'|'authuser_\n72'|'unauthuser_72']. Default avatar when endpoint / device avatar is\nnot available. If default is not set, Not found 404 is returned.\nNo\nfirewall\nhealth: select\nSummary\nList configured load balance server health monitors.\nMonitor API\n44",
          "char_count": 946,
          "ocr_used": false
        },
        {
          "page": 45,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nURI\nfirewall/health/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\narray\nlocal-in: select\nSummary\nList implicit and explicit local-in firewall policies.\nURI\nfirewall/local-in/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.policy\nResponse Type\narray\nacl: select\nSummary\nList counters for all IPv4 ACL.\nURI\nfirewall/acl/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.policy\nacl: clear_counters\nSummary\nReset counters for one or more IPv4 ACLs by policy ID.\nURI\nfirewall/acl/clear_counters/\nHTTP Method\nPOST\nAction\nclear_counters\nAccess Group\nfwgrp.policy\nMonitor API\n45",
          "char_count": 654,
          "ocr_used": false
        },
        {
          "page": 46,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\npolicy\narray\nArray of policy IDs to reset.\nNo\npolicy\nint\nSingle policy ID to reset.\nNo\nacl6: select\nSummary\nList counters for all IPv6 ACL.\nURI\nfirewall/acl6/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.policy\nacl6: clear_counters\nSummary\nReset counters for one or more IPv6 ACLs by policy ID.\nURI\nfirewall/acl6/clear_counters/\nHTTP Method\nPOST\nAction\nclear_counters\nAccess Group\nfwgrp.policy\nExtra parameters\nName\nType\nSummary\nRequired\npolicy\narray\nArray of policy IDs to reset.\nNo\npolicy\nint\nSingle policy ID to reset.\nNo\ninternet-service-match: select\nSummary\nList internet services that exist at a given IP or Subnet.\nURI\nfirewall/internet-service-match/select/\nHTTP Method\nGET\nAction\nselect\nMonitor API\n46",
          "char_count": 808,
          "ocr_used": false
        },
        {
          "page": 47,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nAccess Group\nfwgrp.address\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nip\nstring\nIP (in dot-decimal notation).\nYes\nmask\nstring\nIP Mask (in dot-decimal notation).\nYes\npolicy: select\nSummary\nList traffic statistics for all IPv4 policies.\nURI\nfirewall/policy/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.policy\npolicy: reset\nSummary\nReset traffic statistics for all IPv4 policies.\nURI\nfirewall/policy/reset/\nHTTP Method\nPOST\nAction\nreset\nAccess Group\nfwgrp.policy\npolicy: clear_counters\nSummary\nReset traffic statistics for one or more IPv4 policies by policy ID.\nURI\nfirewall/policy/clear_counters/\nHTTP Method\nPOST\nAction\nclear_counters\nAccess Group\nfwgrp.policy\nMonitor API\n47",
          "char_count": 750,
          "ocr_used": false
        },
        {
          "page": 48,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\npolicy\narray\nArray of policy IDs to reset.\nNo\npolicy\nint\nSingle policy ID to reset.\nNo\npolicy6: select\nSummary\nList traffic statistics for all IPv6 policies.\nURI\nfirewall/policy6/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.policy\npolicy6: reset\nSummary\nReset traffic statistics for all IPv6 policies.\nURI\nfirewall/policy6/reset/\nHTTP Method\nPOST\nAction\nreset\nAccess Group\nfwgrp.policy\npolicy6: clear_counters\nSummary\nReset traffic statistics for one or more IPv6 policies by policy ID.\nURI\nfirewall/policy6/clear_counters/\nHTTP Method\nPOST\nAction\nclear_counters\nAccess Group\nfwgrp.policy\nExtra parameters\nName\nType\nSummary\nRequired\npolicy\narray\nArray of policy IDs to reset.\nNo\npolicy\nint\nSingle policy ID to reset.\nNo\nMonitor API\n48",
          "char_count": 832,
          "ocr_used": false
        },
        {
          "page": 49,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nproxy-policy: select\nSummary\nList traffic statistics for all explicit proxy policies.\nURI\nfirewall/proxy-policy/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.policy\nproxy-policy: clear_counters\nSummary\nReset traffic statistics for one or more explicit proxy policies by policy ID.\nURI\nfirewall/proxy-policy/clear_counters/\nHTTP Method\nPOST\nAction\nclear_counters\nAccess Group\nfwgrp.policy\nExtra parameters\nName\nType\nSummary\nRequired\npolicy\narray\nArray of policy IDs to reset.\nNo\npolicy\nint\nSingle policy ID to reset.\nNo\npolicy-lookup: select\nSummary\nPerforms a policy lookup by creating a dummy packet and asking the kernel which\npolicy would be hit.\nURI\nfirewall/policy-lookup/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.policy\nResponse Type\nobject\nMonitor API\n49",
          "char_count": 829,
          "ocr_used": false
        },
        {
          "page": 50,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nipv6\nboolean\nPerform an IPv6 lookup?\nNo\nsrcintf\nstring\nSource interface.\nYes\nsourceport\nint\nSource port.\nNo\nsourceip\nstring\nSource IP.\nNo\nprotocol\nstring\nProtocol.\nYes\ndest\nstring\nDestination IP/FQDN.\nYes\ndestport\nint\nDestination port.\nYes\nicmptype\nint\nICMP type.\nNo\nicmpcode\nint\nICMP code.\nNo\nsession: select\nSummary\nList all active firewall sessions (optionally filtered).\nURI\nfirewall/session/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nip_version\nstring\nIP version [*ipv4 | ipv6 | ipboth].\nNo\nstart\nint\nStarting entry index.\nNo\ncount\nint\nMaximum number of entries to return.\nYes\nsummary\nboolean\nEnable/disable inclusion of session summary (setup rate, total\nsessions, etc).\nNo\nsourceport\nint\nFilter: Source port.\nNo\npolicyid\nint\nFilter: Policy ID.\nNo\napplication\nint\nFilter: Application ID.\nNo\nMonitor API\n50",
          "char_count": 976,
          "ocr_used": false
        },
        {
          "page": 51,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nName\nType\nSummary\nRequired\napplication\nstring\nFilter: Application PROTO/PORT. (e.g. \"TCP/443\")\nNo\nprotocol\nint\nFilter: Protocol name [all|igmp|tcp|udp|icmp|etc].\nNo\ndestport\nint\nFilter: Destination port.\nNo\nsrcintf\nstring\nFilter: Source interface name.\nNo\ndstintf\nstring\nFilter: Destination interface name.\nNo\nsrcintfrole\narray\nFilter: Source interface roles.\nNo\ndstintfrole\narray\nFilter: Destination interface roles.\nNo\nsource\nstring\nFilter: Source IP address.\nNo\ndestination\nstring\nFilter: Destination IP address.\nNo\nusername\nstring\nFilter: Authenticated username.\nNo\nshaper\nstring\nFilter: Forward traffic shaper name.\nNo\ncountry\nstring\nFilter: Destination country name.\nNo\nnatsourceaddress\nstring\nFilter: NAT source address.\nNo\nnatsourceport\nstring\nFilter: NAT source port.\nNo\nsession: clear_all\nSummary\nImmediately clear all active IPv4 and IPv6 sessions and IPS sessions of current\nVDOM.\nURI\nfirewall/session/clear_all/\nHTTP Method\nPOST\nAction\nclear_all\nAccess Group\nsysgrp\nResponse Type\nint\nsession: close\nSummary\nClose a specific firewall session that matches all provided criteria.\nURI\nfirewall/session/close/\nHTTP Method\nPOST\nMonitor API\n51",
          "char_count": 1191,
          "ocr_used": false
        },
        {
          "page": 52,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nAction\nclose\nAccess Group\nsysgrp\nExtra parameters\nName\nType\nSummary\nRequired\npro\nstring\nProtocol name [tcp|udp|icmp|...].\nYes\nsaddr\nstring\nSource address.\nYes\ndaddr\nstring\nDestination address.\nYes\nsport\nstring\nSource port.\nYes\ndport\nstring\nDestination port.\nYes\nsession-top: select\nSummary\nList of top sessions by specified grouping criteria.\nURI\nfirewall/session-top/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nreport_by\nstring\nCriteria to group results by [source*|destination|application|web-\ncategory|web-domain|srcintf|dstintf|policy|country].\nNo\nsort_by\nstring\nCriteria to sort results by [bytes|msg-counts].\nNo\ncount\nint\nMaximum number of entries to return.\nNo\nfilter\nobject\nA map of filter keys to string values. The key(s) may be srcintf,\nsource, dstintf, srcintfrole, dstintfrole, destination, policyid,\napplication, web_category_id, web_domain, country.\nNo\nshaper: select\nSummary\nList of statistics for configured firewall shapers.\nMonitor API\n52",
          "char_count": 1077,
          "ocr_used": false
        },
        {
          "page": 53,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nURI\nfirewall/shaper/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.others\nResponse Type\narray\nshaper: reset\nSummary\nReset statistics for all configured traffic shapers.\nURI\nfirewall/shaper/reset/\nHTTP Method\nPOST\nAction\nreset\nAccess Group\nfwgrp.others\nload-balance: select\nSummary\nList all firewall load balance servers.\nURI\nfirewall/load-balance/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.others\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nstart\nint\nStarting entry index.\nNo\ncount\nint\nMaximum number of entries to return.\nYes\naddress-fqdns: select\nSummary\nList of FQDN address objects and the IPs they resolved to.\nURI\nfirewall/address-fqdns/select/\nMonitor API\n53",
          "char_count": 751,
          "ocr_used": false
        },
        {
          "page": 54,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.address\nResponse Type\nobject\nippool: select\nSummary\nList IPv4 pool statistics.\nURI\nfirewall/ippool/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.policy\nResponse Type\nobject\naddress-dynamic: select\nSummary\nList of Dynamic SDN address objects and the IPs they resolve to.\nURI\nfirewall/address-dynamic/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.address\nResponse Type\nobject\naddress6-dynamic: select\nSummary\nList of IPv6 Dynamic SDN address objects and the IPs they resolve to.\nURI\nfirewall/address6-dynamic/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.address\nResponse Type\nobject\nMonitor API\n54",
          "char_count": 720,
          "ocr_used": false
        },
        {
          "page": 55,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nfortiview\nstatistics: select\nSummary\nRetrieve drill-down and summary data for FortiView (both realtime and historical).\nURI\nfortiview/statistics/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nrealtime\nboolean\nSet to true to retrieve realtime results (from kernel).\nNo\nfilter\nobject\nA map of filter keys to arrays of values.\nNo\nsession: cancel\nSummary\nCancel a FortiView request session.\nURI\nfortiview/session/cancel/\nHTTP Method\nPOST\nAction\ncancel\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nsessionid\nint\nSession ID to cancel.\nNo\ndevice\nint\nFortiView request session's device. [disk|faz]\nNo\nreport_by\nint\nReport by field.\nNo\nview_level\nint\nFortiView View level.\nNo\nMonitor API\n55",
          "char_count": 839,
          "ocr_used": false
        },
        {
          "page": 56,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nsandbox-file-details: select\nSummary\nRetrieve FortiSandbox analysis details for a specific file checksum.\nURI\nfortiview/sandbox-file-details/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nchecksum\nstring\nChecksum of a specific file that has been analyzed by the connected\nFortiSandbox.\nYes\ngeoip\ngeoip-query: select\nSummary\nRetrieve location details for IPs queried against FortiGuard's geoip service.\nURI\ngeoip/geoip-query/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nip_addresses\nstring\nOne or more IP address strings to query for location details.\nYes\nMonitor API\n56",
          "char_count": 765,
          "ocr_used": false
        },
        {
          "page": 57,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nips\nrate-based: select\nSummary\nReturns a list of rate-based signatures in IPS package.\nURI\nips/rate-based/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nutmgrp.ips\nResponse Type\narray\nlicense\nstatus: select\nSummary\nGet current license & registration status.\nURI\nlicense/status/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\ndatabase: upgrade\nSummary\nUpgrade IPS database on this device using uploaded file.\nURI\nlicense/database/upgrade/\nHTTP Method\nPOST\nAction\nupgrade\nAccess Group\nupdategrp\nResponse Type\nobject\nMonitor API\n57",
          "char_count": 605,
          "ocr_used": false
        },
        {
          "page": 58,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\ndb_name\nstring\nSecurity service database name [ips_appctrl|antivirus|...]\nNo\nfile_content\nstring\nProvided when uploading a file: base64 encoded file data. Must not\ncontain whitespace or other invalid base64 characters. Must be\nincluded in HTTP body.\nNo\nforticare-resellers: select\nSummary\nGet current FortiCare resellers for the requested country.\nURI\nlicense/forticare-resellers/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\ncountry_code\nint\nFortiGuard country code\nNo\nforticare-org-list: select\nSummary\nGet FortiCare organization size and industry lists.\nURI\nlicense/forticare-org-list/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\nMonitor API\n58",
          "char_count": 844,
          "ocr_used": false
        },
        {
          "page": 59,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nlog\ncurrent-disk-usage: select\nSummary\nReturn current used, free and total disk bytes.\nURI\nlog/current-disk-usage/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nloggrp.data-access\ndevice: state\nSummary\nRetrieve information on state of log devices.\nURI\nlog/device/state/\nHTTP Method\nGET\nAction\nstate\nAccess Group\nloggrp.data-access\nResponse Type\nobject\nforticloud: select\nSummary\nReturn FortiCloud log status.\nURI\nlog/forticloud/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nloggrp.config\nfortianalyzer: select\nSummary\nReturn FortiAnalyzer/FortiManager log status.\nURI\nlog/fortianalyzer/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nloggrp.config\nMonitor API\n59",
          "char_count": 716,
          "ocr_used": false
        },
        {
          "page": 60,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nserver\nstring\nFortiAnalyzer/FortiManager address.\nNo\nfortianalyzer-queue: select\nSummary\nRetrieve information on FortiAnalyzer's queue state. Note:- FortiAnalyzer logs are\nqueued only if upload-option is realtime.\nURI\nlog/fortianalyzer-queue/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nloggrp.config\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nscope\nstring\nScope from which to retrieve FortiAnalyzer's queue state\n[vdom*|global].\nNo\nhourly-disk-usage: select\nSummary\nReturn historic hourly disk usage in bytes.\nURI\nlog/hourly-disk-usage/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nloggrp.data-access\nhistoric-daily-remote-logs: select\nSummary\nReturns the amount of logs in bytes sent daily to a remote logging service\n(FortiCloud or FortiAnalyzer).\nURI\nlog/historic-daily-remote-logs/select/\nHTTP Method\nGET\nMonitor API\n60",
          "char_count": 943,
          "ocr_used": false
        },
        {
          "page": 61,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nAction\nselect\nAccess Group\nloggrp.data-access\nstats: select\nSummary\nReturn number of logs sent by category per day for a specific log device.\nURI\nlog/stats/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nloggrp.data-access\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\ndev\nstring\nLog device [*memory | disk | fortianalyzer | forticloud].\nNo\nstats: reset\nSummary\nReset logging statistics for all log devices.\nURI\nlog/stats/reset/\nHTTP Method\nPOST\nAction\nreset\nAccess Group\nloggrp.data-access\nforticloud-report: download\nSummary\nDownload PDF report from FortiCloud.\nURI\nlog/forticloud-report/download/\nHTTP Method\nGET\nAction\ndownload\nAccess Group\nloggrp.data-access\nResponse Type\nobject\nMonitor API\n61",
          "char_count": 762,
          "ocr_used": false
        },
        {
          "page": 62,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nint\nFortiCloud Report ID.\nYes\ninline\nint\nSet to 1 to download the report inline.\nNo\nips-archive: download\nSummary\nDownload IPS/application control packet capture files. Uses configured log\ndisplay device.\nURI\nlog/ips-archive/download/\nHTTP Method\nGET\nAction\ndownload\nAccess Group\nloggrp.data-access\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nint\nIPS archive ID.\nYes\npcap_no\nint\nPacket capture roll number (required when log device is 'disk')\nNo\npcap_category\nint\nPacket capture category (required when log device is 'disk')\nNo\npolicy-archive: download\nSummary\nDownload policy-based packet capture archive.\nURI\nlog/policy-archive/download/\nHTTP Method\nGET\nAction\ndownload\nAccess Group\nloggrp.data-access\nResponse Type\nobject\nMonitor API\n62",
          "char_count": 856,
          "ocr_used": false
        },
        {
          "page": 63,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nint\nSession ID (from traffic log).\nYes\nsrcip\nstring\nSource IP.\nYes\ndstip\nstring\nDestination IP.\nYes\nav-archive: download\nSummary\nDownload file quarantined by AntiVirus.\nURI\nlog/av-archive/download/\nHTTP Method\nGET\nAction\ndownload\nAccess Group\nloggrp.data-access\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nstring\nChecksum for quarantined file.\nYes\nrouter\nipv4: select\nSummary\nList all active IPv4 routing table entries.\nURI\nrouter/ipv4/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\narray\nMonitor API\n63",
          "char_count": 645,
          "ocr_used": false
        },
        {
          "page": 64,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nstart\nint\nStarting entry index.\nNo\ncount\nint\nMaximum number of entries to return (Default for all routes).\nNo\nip_mask\nstring\nFilter: IP/netmask.\nNo\ngateway\nstring\nFilter: gateway.\nNo\ntype\nstring\nFilter: route type.\nNo\ninterface\nstring\nFilter: interface name.\nNo\nipv6: select\nSummary\nList all active IPv6 routing table entries.\nURI\nrouter/ipv6/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nstart\nint\nStarting entry index.\nNo\ncount\nint\nMaximum number of entries to return (Default for all routes).\nNo\nip_mask\nstring\nFilter: IP/netmask.\nNo\ngateway\nstring\nFilter: gateway.\nNo\ntype\nstring\nFilter: route type.\nNo\ninterface\nstring\nFilter: interface name.\nNo\nstatistics: select\nSummary\nRetrieve routing table statistics, including number of matched routes.\nURI\nrouter/statistics/select/\nMonitor API\n64",
          "char_count": 955,
          "ocr_used": false
        },
        {
          "page": 65,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nip_version\nint\nIP version (4|6). If not present, IPv4 and IPv6 will be returned.\nNo\nip_mask\nstring\nFilter: IP/netmask.\nNo\ngateway\nstring\nFilter: gateway.\nNo\ntype\nstring\nFilter: route type.\nNo\ninterface\nstring\nFilter: interface name.\nNo\nlookup: select\nSummary\nPerforms a route lookup by querying the routing table.\nURI\nrouter/lookup/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nroutegrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nipv6\nboolean\nPerform an IPv6 lookup?\nNo\ndestination\nstring\nDestination IP/FQDN\nYes\npolicy: select\nSummary\nRetrieve a list of active IPv4 policy routes.\nURI\nrouter/policy/select/\nMonitor API\n65",
          "char_count": 806,
          "ocr_used": false
        },
        {
          "page": 66,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nHTTP Method\nGET\nAction\nselect\nAccess Group\nroutegrp\nResponse Type\narray\npolicy6: select\nSummary\nRetrieve a list of active IPv6 policy routes.\nURI\nrouter/policy6/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nroutegrp\nResponse Type\narray\nsystem\nadmin: toggle-vdom-mode\nSummary\nToggles VDOM mode on/off. Enables or disables VDOM mode if it is disabled or\nenabled respectively.\nURI\nsystem/admin/toggle-vdom-mode/\nHTTP Method\nPOST\nAction\ntoggle-vdom-mode\nAccess Group\nsysgrp\nResponse Type\nobject\napi-user: generate-key\nSummary\nGenerate a new api-key for the specified api-key-auth admin. The old api-key will\nbe replaced. The response contains the only chance to read the new api-key\nplaintext in the api_key field.\nURI\nsystem/api-user/generate-key/\nHTTP Method\nPOST\nMonitor API\n66",
          "char_count": 823,
          "ocr_used": false
        },
        {
          "page": 67,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nAction\ngenerate-key\nAccess Group\nadmingrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\napi-user\nstring\nGenerate a new token for this api-user.\nYes\nconfig-revision: select\nSummary\nReturns a list of system configuration revisions.\nURI\nsystem/config-revision/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\nconfig-revision: update-comments\nSummary\nUpdates comments for a system configuration file.\nURI\nsystem/config-revision/update-comments/\nHTTP Method\nPOST\nAction\nupdate-comments\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nconfig_id\nint\nConfiguration id.\nNo\ncomments\nstring\nConfiguration comments.\nNo\nMonitor API\n67",
          "char_count": 749,
          "ocr_used": false
        },
        {
          "page": 68,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nconfig-revision: delete\nSummary\nDeletes one or more system configuration revisions.\nURI\nsystem/config-revision/delete/\nHTTP Method\nPOST\nAction\ndelete\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nconfig_ids\narray\nList of configuration ids.\nYes\nconfig-revision: file\nSummary\nDownload a specific configuration revision.\nURI\nsystem/config-revision/file/\nHTTP Method\nGET\nAction\nfile\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nconfig_id\nint\nConfiguration id.\nNo\nconfig-revision: info\nSummary\nRetrieve meta information for a specific configuration revision.\nURI\nsystem/config-revision/info/\nHTTP Method\nGET\nAction\ninfo\nMonitor API\n68",
          "char_count": 750,
          "ocr_used": false
        },
        {
          "page": 69,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nconfig_id\nint\nConfiguration id.\nNo\nconfig-revision: save\nSummary\nCreate a new config revision checkpoint.\nURI\nsystem/config-revision/save/\nHTTP Method\nPOST\nAction\nsave\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\ncomments\nstring\nOptional revision comments\nNo\ncurrent-admins: select\nSummary\nReturn a list of currently logged in administrators.\nURI\nsystem/current-admins/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\narray\ndisconnect-admins: select\nSummary\nDisconnects logged in administrators.\nMonitor API\n69",
          "char_count": 700,
          "ocr_used": false
        },
        {
          "page": 70,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nURI\nsystem/disconnect-admins/select/\nHTTP Method\nPOST\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nid\nint\nAdmin ID\nNo\nmethod\nstring\nLogin method used to connect admin to FortiGate.\nNo\nadmins\narray\nList of objects with admin id and method.\nNo\ntime: set\nSummary\nSets current system time stamp.\nURI\nsystem/time/set/\nHTTP Method\nPOST\nAction\nset\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nyear\nint\nSpecifies the year for setting/updating time manually.\nYes\nmonth\nint\nSpecifies the month (0 - 11) for setting/updating time manually.\nYes\nday\nint\nSpecifies the day for setting/updating time manually.\nYes\nhour\nint\nSpecifies the hour (0 - 23) for setting/updating time manually.\nYes\nminute\nint\nSpecifies the minute (0 - 59) for setting/updating time manually.\nYes\nsecond\nint\nSpecifies the second (0 - 59) for setting/updating time manually.\nYes\nMonitor API\n70",
          "char_count": 988,
          "ocr_used": false
        },
        {
          "page": 71,
          "text": "FortiOS REST API Reference\nFortinet Inc.\ntime: select\nSummary\nGets current system time stamp.\nURI\nsystem/time/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\nos: reboot\nSummary\nImmediately reboot this device.\nURI\nsystem/os/reboot/\nHTTP Method\nPOST\nAction\nreboot\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nevent_log_\nmessage\nstring\nMessage to be logged in event log.\nNo\nos: shutdown\nSummary\nImmediately shutdown this device.\nURI\nsystem/os/shutdown/\nHTTP Method\nPOST\nAction\nshutdown\nAccess Group\nsysgrp\nResponse Type\nobject\nMonitor API\n71",
          "char_count": 610,
          "ocr_used": false
        },
        {
          "page": 72,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nevent_log_\nmessage\nstring\nMessage to be logged in event log.\nNo\nglobal-resources: select\nSummary\nRetrieve current usage of global resources as well as both the default and user\nconfigured maximum values.\nURI\nsystem/global-resources/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nvdom-resource: select\nSummary\nRetrieve VDOM resource information, including CPU and memory usage.\nURI\nsystem/vdom-resource/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\ndhcp: select\nSummary\nReturns a list of all DHCP IPv4 and IPv6 DHCP leases.\nURI\nsystem/dhcp/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nnetgrp\nResponse Type\narray\nMonitor API\n72",
          "char_count": 740,
          "ocr_used": false
        },
        {
          "page": 73,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nscope\nstring\nScope from which to retrieve DHCP leases [vdom*|global]. Global\nscope is only accessible for global administrators.\nNo\nipv6\nboolean\nInclude IPv6 addresses in the response.\nNo\ndhcp: revoke\nSummary\nRevoke IPv4 DHCP leases.\nURI\nsystem/dhcp/revoke/\nHTTP Method\nPOST\nAction\nrevoke\nAccess Group\nnetgrp\nExtra parameters\nName\nType\nSummary\nRequired\nip\narray\nOptional list of addresses to revoke. Defaults to all addresses if not\nprovided.\nNo\ndhcp6: revoke\nSummary\nRevoke IPv6 DHCP leases.\nURI\nsystem/dhcp6/revoke/\nHTTP Method\nPOST\nAction\nrevoke\nAccess Group\nnetgrp\nExtra parameters\nName\nType\nSummary\nRequired\nip\narray\nOptional list of addresses to revoke. Defaults to all addresses if not\nprovided.\nNo\nMonitor API\n73",
          "char_count": 806,
          "ocr_used": false
        },
        {
          "page": 74,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nfirmware: select\nSummary\nRetrieve a list of firmware images available to use for upgrade on this device.\nURI\nsystem/firmware/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nmntgrp\nfirmware: upgrade\nSummary\nUpgrade firmware image on this device using uploaded file.\nURI\nsystem/firmware/upgrade/\nHTTP Method\nPOST\nAction\nupgrade\nAccess Group\nmntgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nsource\nstring\nFirmware file data source [upload|usb|fortiguard].\nYes\nfilename\nstring\nName of file on fortiguard or USB disk to upgrade to.\nNo\nformat_\npartition\nboolean\nSet to true to format boot partition before upgrade.\nNo\nfile_content\nstring\nProvided when uploading a file: base64 encoded file data. Must not\ncontain whitespace or other invalid base64 characters. Must be\nincluded in HTTP body.\nNo\nfirmware: upgrade-paths\nSummary\nRetrieve a list of supported firmware upgrade paths.\nURI\nsystem/firmware/upgrade-paths/\nHTTP Method\nGET\nAction\nupgrade-paths\nAccess Group\nmntgrp\nMonitor API\n74",
          "char_count": 1047,
          "ocr_used": false
        },
        {
          "page": 75,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nfsck: start\nSummary\nSet file system check flag so that it will be executed on next device reboot.\nURI\nsystem/fsck/start/\nHTTP Method\nPOST\nAction\nstart\nAccess Group\nsysgrp\nstorage: select\nSummary\nRetrieve information for the non-boot disk.\nURI\nsystem/storage/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nchange-password: select\nSummary\nSave admin and guest-admin passwords.\nURI\nsystem/change-password/select/\nHTTP Method\nPOST\nAction\nselect\nAccess Group\nany\npassword-policy-conform: select\nSummary\nCheck whether password conforms to the password policy.\nURI\nsystem/password-policy-conform/select/\nHTTP Method\nPOST\nAction\nselect\nAccess Group\nany\nMonitor API\n75",
          "char_count": 712,
          "ocr_used": false
        },
        {
          "page": 76,
          "text": "FortiOS REST API Reference\nFortinet Inc.\ncsf: select\nSummary\nRetrieve a full tree of downstream FortiGates registered to the Security Fabric.\nURI\nsystem/csf/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nETag Caching\nEnabled\nResponse Type\nobject\nmodem: select\nSummary\nRetrieve statistics for internal/external configured modem.\nURI\nsystem/modem/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nmodem: reset\nSummary\nReset statistics for internal/external configured modem.\nURI\nsystem/modem/reset/\nHTTP Method\nPOST\nAction\nreset\nAccess Group\nsysgrp\nmodem: connect\nSummary\nTrigger a connect for the configured modem.\nURI\nsystem/modem/connect/\nHTTP Method\nPOST\nAction\nconnect\nAccess Group\nsysgrp\nMonitor API\n76",
          "char_count": 726,
          "ocr_used": false
        },
        {
          "page": 77,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nmodem: disconnect\nSummary\nTrigger a disconnect for the configured modem.\nURI\nsystem/modem/disconnect/\nHTTP Method\nPOST\nAction\ndisconnect\nAccess Group\nsysgrp\nmodem: update\nSummary\nUpdate supported modem list from FortiGuard.\nURI\nsystem/modem/update/\nHTTP Method\nPOST\nAction\nupdate\nAccess Group\nsysgrp\n3g-modem: select\nSummary\nList all 3G modems available via FortiGuard.\nURI\nsystem/3g-modem/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nresource: usage\nSummary\nRetreive current and historical usage data for a provided resource.\nURI\nsystem/resource/usage/\nHTTP Method\nGET\nAction\nusage\nAccess Group\nsysgrp\nResponse Type\nobject\nMonitor API\n77",
          "char_count": 693,
          "ocr_used": false
        },
        {
          "page": 78,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nresource\nstring\nResource to get usage data for [cpu|memory|disk|sessions|lograte].\nDefaults to all resources if not provided.\nNo\ninterval\nstring\nTime interval of resource usage [1-min|10-min|30-min|1-hour|12-\nhour|24-hour]. Defaults to all intervals if not provided.\nNo\nsniffer: select\nSummary\nReturn a list of all configured packet captures.\nURI\nsystem/sniffer/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nfwgrp.packet-capture\nResponse Type\narray\nsniffer: restart\nSummary\nRestart specified packet capture.\nURI\nsystem/sniffer/restart/\nHTTP Method\nPOST\nAction\nrestart\nAccess Group\nfwgrp.packet-capture\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nint\nID of packet capture entry.\nYes\nsniffer: start\nSummary\nStart specified packet capture.\nURI\nsystem/sniffer/start/\nMonitor API\n78",
          "char_count": 893,
          "ocr_used": false
        },
        {
          "page": 79,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nHTTP Method\nPOST\nAction\nstart\nAccess Group\nfwgrp.packet-capture\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nint\nID of packet capture entry.\nYes\nsniffer: stop\nSummary\nStop specified packet capture.\nURI\nsystem/sniffer/stop/\nHTTP Method\nPOST\nAction\nstop\nAccess Group\nfwgrp.packet-capture\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nint\nID of packet capture entry.\nYes\nsniffer: download\nSummary\nDownload a stored packet capture.\nURI\nsystem/sniffer/download/\nHTTP Method\nGET\nAction\ndownload\nAccess Group\nfwgrp.packet-capture\nResponse Type\nobject\nMonitor API\n79",
          "char_count": 647,
          "ocr_used": false
        },
        {
          "page": 80,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nint\nID of packet capture entry.\nYes\nfsw: select\nSummary\nRetrieve statistics for configured FortiSwitches\nURI\nsystem/fsw/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nfsw_id\nstring\nFilter: FortiSwitch ID.\nNo\npoe\nboolean\nFilter: Retrieve PoE statistics for ports of configured FortiSwitches.\nPort power usage is in Watt units.\nNo\nport_stats\nboolean\nFilter: Retrieve tx/rx statistics for ports of configured FortiSwitches.\nNo\nfsw: update\nSummary\nUpdate administrative state for a given FortiSwitch (enable or disable\nauthorization).\nURI\nsystem/fsw/update/\nHTTP Method\nPOST\nAction\nupdate\nAccess Group\nsysgrp\nExtra parameters\nName\nType\nSummary\nRequired\nfswname\nstring\nFortiSwitch name.\nNo\nadmin\nstring\nNew FortiSwitch administrative state [enable|disable|discovered].\nNo\nMonitor API\n80",
          "char_count": 947,
          "ocr_used": false
        },
        {
          "page": 81,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nfsw: restart\nSummary\nRestart a given FortiSwitch.\nURI\nsystem/fsw/restart/\nHTTP Method\nPOST\nAction\nrestart\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nstring\nName of managed FortiSwitch.\nYes\nfsw: upgrade\nSummary\nUpgrade firmware image on the given FortiSwitch using uploaded file.\nURI\nsystem/fsw/upgrade/\nHTTP Method\nPOST\nAction\nupgrade\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nstring\nName of managed FortiSwitch.\nYes\nsource\nstring\nFirmware file data source [upload|fortiguard].\nYes\nfile_content\nstring\nProvided when uploading a file: base64 encoded file data. Must not\ncontain whitespace or other invalid base64 characters. Must be\nincluded in HTTP body.\nNo\nfsw: poe-reset\nSummary\nReset PoE on a given FortiSwitch's port.\nMonitor API\n81",
          "char_count": 873,
          "ocr_used": false
        },
        {
          "page": 82,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nURI\nsystem/fsw/poe-reset/\nHTTP Method\nPOST\nAction\npoe-reset\nAccess Group\nsysgrp\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nstring\nName of managed FortiSwitch.\nYes\nport\nstring\nName of port to reset PoE on.\nYes\nfsw-firmware: select\nSummary\nRetrieve a list of recommended firmware for managed FortiSwitches.\nURI\nsystem/fsw-firmware/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nstring\nFilter: FortiSwitch ID.\nNo\ntimeout\nstring\nFortiGuard connection timeout (defaults to 3 seconds).\nNo\nswitch-controller\nmanaged-switch: dhcp-snooping\nSummary\nRetrieve DHCP servers monitored by FortiSwitches.\nURI\nswitch-controller/managed-switch/dhcp-snooping/\nHTTP Method\nGET\nAction\ndhcp-snooping\nMonitor API\n82",
          "char_count": 821,
          "ocr_used": false
        },
        {
          "page": 83,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nAccess Group\nwifi\nResponse Type\narray\nmanaged-switch: faceplate-xml\nSummary\nRetrieve XML for rendering FortiSwitch faceplate widget.\nURI\nswitch-controller/managed-switch/faceplate-xml/\nHTTP Method\nGET\nAction\nfaceplate-xml\nAccess Group\nwifi\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nstring\nName of managed FortiSwitch.\nNo\nvalidate-switch-prefix: select\nSummary\nValidate a FortiSwitch serial number prefix.\nURI\nswitch-controller/validate-switch-prefix/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nwifi\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nprefix\nstring\nPrefix of FortiSwitch serial number.\nNo\ninterface: select\nSummary\nRetrieve statistics for all system interfaces.\nMonitor API\n83",
          "char_count": 781,
          "ocr_used": false
        },
        {
          "page": 84,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nURI\nsystem/interface/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nnetgrp\nETag Caching\nEnabled\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\ninterface_\nname\nstring\nFilter: interface name.\nNo\ninclude_vlan\nboolean\nEnable to include VLANs in result list.\nNo\navailable-interfaces: select\nSummary\nRetrieve a list of all interfaces along with some meta information regarding their\navailability.\nURI\nsystem/available-interfaces/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nany\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nview_type\nstring\nOptionally include additional information for interfaces. This\nparameter can be repeated multiple times. 'poe': Includes PoE\ninformation for supported ports. 'ha': Includes extra meta information\nuseful when dealing with interfaces related to HA configuration.\nInterfaces that are used by an HA cluster as management interfaces\nare also included in this view. 'zone': Includes extra meta information\nfor determining zone membership eligibility.\nNo\nMonitor API\n84",
          "char_count": 1084,
          "ocr_used": false
        },
        {
          "page": 85,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nacquired-dns: select\nSummary\nRetrieve a list of interfaces and their acquired DNS servers.\nURI\nsystem/acquired-dns/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nany\nResponse Type\narray\nresolve-fqdn: select\nSummary\nResolves the provided FQDNs to FQDN -> IP mappings.\nURI\nsystem/resolve-fqdn/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nany\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nipv6\nboolean\nResolve for the AAAA record?\nNo\nfqdn\nstring\nFQDN\nYes\nfqdn\narray\nList of FQDNs to be resolved\nNo\nusb-log: select\nSummary\nRetrieve information about connected USB drives, including estimated log sizes.\nURI\nsystem/usb-log/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nMonitor API\n85",
          "char_count": 761,
          "ocr_used": false
        },
        {
          "page": 86,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nusb-log: start\nSummary\nStart backup of logs from current VDOM to USB drive.\nURI\nsystem/usb-log/start/\nHTTP Method\nPOST\nAction\nstart\nAccess Group\nsysgrp\nusb-log: stop\nSummary\nStop backup of logs to USB drive.\nURI\nsystem/usb-log/stop/\nHTTP Method\nPOST\nAction\nstop\nAccess Group\nsysgrp\nipconf: select\nSummary\nDetermine if there is an IP conflict for a specific IP using ARP.\nURI\nsystem/ipconf/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nnetgrp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\ndev\nobject\nList of interfaces to check for conflict.\nNo\nipaddr\nstring\nIPv4 address to check for conflict.\nNo\nfortiguard: update\nSummary\nImmediately update status for FortiGuard services.\nMonitor API\n86",
          "char_count": 754,
          "ocr_used": false
        },
        {
          "page": 87,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nURI\nsystem/fortiguard/update/\nHTTP Method\nPOST\nAction\nupdate\nAccess Group\nsysgrp\nfortiguard: clear-cache\nSummary\nImmediately clear all FortiGuard statistics.\nURI\nsystem/fortiguard/clear-cache/\nHTTP Method\nPOST\nAction\nclear-cache\nAccess Group\nsysgrp\nfortiguard: test-availability\nSummary\nTest availability of FortiGuard services.\nURI\nsystem/fortiguard/test-availability/\nHTTP Method\nPOST\nAction\ntest-availability\nAccess Group\nsysgrp\nfortiguard: server-info\nSummary\nGet FortiGuard server list and information.\nURI\nsystem/fortiguard/server-info/\nHTTP Method\nGET\nAction\nserver-info\nAccess Group\nsysgrp\nfortimanager: status\nSummary\nGet FortiManager status.\nMonitor API\n87",
          "char_count": 708,
          "ocr_used": false
        },
        {
          "page": 88,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nURI\nsystem/fortimanager/status/\nHTTP Method\nGET\nAction\nstatus\nAccess Group\nsysgrp\nfortimanager: config\nSummary\nConfigure FortiManager address.\nURI\nsystem/fortimanager/config/\nHTTP Method\nPOST\nAction\nconfig\nAccess Group\nsysgrp\navailable-certificates: select\nSummary\nGet available certificates.\nURI\nsystem/available-certificates/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nany\nExtra parameters\nName\nType\nSummary\nRequired\nscope\nstring\nScope of certificate [vdom*|global].\nNo\ncertificate: download\nSummary\nDownload certificate.\nURI\nsystem/certificate/download/\nHTTP Method\nGET\nAction\ndownload\nAccess Group\nvpngrp\nResponse Type\nobject\nMonitor API\n88",
          "char_count": 693,
          "ocr_used": false
        },
        {
          "page": 89,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nstring\nName of certificate.\nYes\ntype\nstring\nType of certificate [local|csr|remote|ca|crl].\nYes\nscope\nstring\nScope of certificate [vdom*|global].\nNo\ndebug: select\nSummary\nLog debug messages to the console (if enabled).\nURI\nsystem/debug/select/\nHTTP Method\nPOST\nAction\nselect\nAccess Group\nany\nExtra parameters\nName\nType\nSummary\nRequired\ntype\nstring\nType of message.\nYes\nmsg\nstring\nMessage content.\nYes\nfile\nstring\nFile name generating message.\nYes\nline\nstring\nLine number in file.\nYes\ndebug: download\nSummary\nDownload debug report for technical support.\nURI\nsystem/debug/download/\nHTTP Method\nGET\nAction\ndownload\nAccess Group\nmntgrp\nResponse Type\nobject\nMonitor API\n89",
          "char_count": 757,
          "ocr_used": false
        },
        {
          "page": 90,
          "text": "FortiOS REST API Reference\nFortinet Inc.\ncom-log: dump\nSummary\nDump system com-log to file.\nURI\nsystem/com-log/dump/\nHTTP Method\nPOST\nAction\ndump\nAccess Group\nsysgrp\ncom-log: update\nSummary\nFetch system com-log file dump progress.\nURI\nsystem/com-log/update/\nHTTP Method\nGET\nAction\nupdate\nAccess Group\nsysgrp\ncom-log: download\nSummary\nDownload com-log file (after file dump is complete).\nURI\nsystem/com-log/download/\nHTTP Method\nGET\nAction\ndownload\nAccess Group\nsysgrp\nResponse Type\nobject\nbotnet: stat\nSummary\nRetrieve statistics for FortiGuard botnet database.\nURI\nsystem/botnet/stat/\nHTTP Method\nGET\nAction\nstat\nAccess Group\nsysgrp\nMonitor API\n90",
          "char_count": 649,
          "ocr_used": false
        },
        {
          "page": 91,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nETag Caching\nEnabled\nResponse Type\nobject\nbotnet: select\nSummary\nList all known IP-based botnet entries in FortiGuard botnet database.\nURI\nsystem/botnet/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nETag Caching\nEnabled\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nstart\nint\nStarting entry index.\nNo\ncount\nint\nMaximum number of entries to return.\nNo\nbotnet-domains: select\nSummary\nList all known domain-based botnet entries in FortiGuard botnet database.\nURI\nsystem/botnet-domains/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nETag Caching\nEnabled\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nstart\nint\nStarting entry index.\nNo\ncount\nint\nMaximum number of entries to return.\nNo\nMonitor API\n91",
          "char_count": 796,
          "ocr_used": false
        },
        {
          "page": 92,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nbotnet-domains: stat\nSummary\nList statistics on domain-based botnet entries in FortiGuard botnet database.\nURI\nsystem/botnet-domains/stat/\nHTTP Method\nGET\nAction\nstat\nAccess Group\nsysgrp\nETag Caching\nEnabled\nResponse Type\nobject\nha-statistics: select\nSummary\nList of statistics for members of HA cluster.\nURI\nsystem/ha-statistics/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\narray\nha-history: select\nSummary\nGet HA cluster historical logs.\nURI\nsystem/ha-history/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nETag Caching\nEnabled\nResponse Type\nobject\nha-checksums: select\nSummary\nList of checksums for members of HA cluster.\nMonitor API\n92",
          "char_count": 719,
          "ocr_used": false
        },
        {
          "page": 93,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nURI\nsystem/ha-checksums/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nETag Caching\nEnabled\nResponse Type\narray\nha-peer: select\nSummary\nGet configuration of peer(s) in HA cluster. Uptime is expressed in seconds.\nURI\nsystem/ha-peer/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nserial_no\nstring\nSerial number of the HA member. If not specified, fetch information\nfor all HA members\nNo\nvcluster_id\nint\nVirtual cluster number. If not specified, fetch information for all active\nvclusters\nNo\nha-peer: update\nSummary\nUpdate configuration of peer in HA cluster.\nURI\nsystem/ha-peer/update/\nHTTP Method\nPOST\nAction\nupdate\nAccess Group\nsysgrp\nResponse Type\nobject\nMonitor API\n93",
          "char_count": 798,
          "ocr_used": false
        },
        {
          "page": 94,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nserial_no\nstring\nSerial number of the HA member.\nYes\nvcluster_id\nint\nVirtual cluster number.\nNo\npriority\nint\nPriority to assign to HA member.\nNo\nhostname\nstring\nName to assign the HA member.\nNo\nha-peer: disconnect\nSummary\nUpdate configuration of peer in HA cluster.\nURI\nsystem/ha-peer/disconnect/\nHTTP Method\nPOST\nAction\ndisconnect\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nserial_no\nstring\nSerial number of the HA member.\nYes\ninterface\nstring\nName of the interface which should be assigned for management.\nYes\nip\nstring\nIP to assign to the selected interface.\nYes\nmask\nstring\nFull network mask to assign to the selected interface.\nYes\nlink-monitor: select\nSummary\nRetrieve per-interface statistics for active link monitors.\nURI\nsystem/link-monitor/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nMonitor API\n94",
          "char_count": 950,
          "ocr_used": false
        },
        {
          "page": 95,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nstring\nName of link monitor.\nNo\ncompliance: run\nSummary\nImmediately run compliance checks for the selected VDOM.\nURI\nsystem/compliance/run/\nHTTP Method\nPOST\nAction\nrun\nAccess Group\nsysgrp\nconfig: restore\nSummary\nRestore system configuration from uploaded file or from USB.\nURI\nsystem/config/restore/\nHTTP Method\nPOST\nAction\nrestore\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nsource\nstring\nConfiguration file data source [upload | usb | revision].\nYes\nusb_filename\nstring\nWhen using 'usb' source: the filename to restore from the connected\nUSB device.\nNo\nconfig_id\nint\nWhen using 'revision' source: valid ID of configuration stored on disk\nto revert to.\nNo\npassword\nstring\nPassword to decrypt configuration data.\nNo\nscope\nstring\nSpecify global or VDOM only restore [global | vdom].\nYes\nvdom\nstring\nIf 'vdom' scope specified, the name of the VDOM to restore\nconfiguration.\nNo\nMonitor API\n95",
          "char_count": 1021,
          "ocr_used": false
        },
        {
          "page": 96,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nName\nType\nSummary\nRequired\nfile_content\nstring\nProvided when uploading a file: base64 encoded file data. Must not\ncontain whitespace or other invalid base64 characters. Must be\nincluded in HTTP body.\nNo\nconfig: backup\nSummary\nBackup system config\nURI\nsystem/config/backup/\nHTTP Method\nGET\nAction\nbackup\nAccess Group\nmntgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\ndestination\nstring\nConfiguration file destination [file* | usb]\nNo\nusb_filename\nstring\nWhen using 'usb' destination: the filename to save to on the\nconnected USB device\nNo\npassword\nstring\nPassword to encrypt configuration data.\nNo\nscope\nstring\nSpecify global or VDOM only backup [global | vdom].\nYes\nvdom\nstring\nIf 'vdom' scope specified, the name of the VDOM to backup\nconfiguration.\nNo\nconfig: usb-filelist\nSummary\nList configuration files available on connected USB drive.\nURI\nsystem/config/usb-filelist/\nHTTP Method\nGET\nAction\nusb-filelist\nAccess Group\nsysgrp\nResponse Type\narray\nMonitor API\n96",
          "char_count": 1028,
          "ocr_used": false
        },
        {
          "page": 97,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nsandbox: status\nSummary\nRetrieve sandbox status.\nURI\nsystem/sandbox/status/\nHTTP Method\nGET\nAction\nstatus\nAccess Group\nsysgrp\nResponse Type\nobject\nsandbox: stats\nSummary\nRetrieve sandbox statistics.\nURI\nsystem/sandbox/stats/\nHTTP Method\nGET\nAction\nstats\nAccess Group\nsysgrp\nResponse Type\nobject\nobject: usage\nSummary\nRetrieve all objects that are currently using as well as objects that can use the\ngiven object.\nURI\nsystem/object/usage/\nHTTP Method\nGET\nAction\nusage\nAccess Group\nany\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\npath\nstring\nThe CMDB table's path\nNo\nname\nstring\nThe CMDB table's name\nNo\nMonitor API\n97",
          "char_count": 679,
          "ocr_used": false
        },
        {
          "page": 98,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nName\nType\nSummary\nRequired\nqtypes\narray\nList of CMDB table qTypes\nNo\nmkey\nstring\nThe mkey for the object\nYes\ntimezone: select\nSummary\nGet world timezone and daylight saving time.\nURI\nsystem/timezone/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nany\nResponse Type\narray\nvmlicense: upload\nSummary\nUpdate VM license using uploaded file. Reboots immediately if successful.\nURI\nsystem/vmlicense/upload/\nHTTP Method\nPOST\nAction\nupload\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nfile_content\nstring\nProvided when uploading a file: base64 encoded file data. Must not\ncontain whitespace or other invalid base64 characters. Must be\nincluded in HTTP body.\nNo\nsensor-info: select\nSummary\nRetrieve system sensor status.\nURI\nsystem/sensor-info/select/\nHTTP Method\nGET\nMonitor API\n98",
          "char_count": 857,
          "ocr_used": false
        },
        {
          "page": 99,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\naudit: select\nSummary\nRetrieve Security Fabric audit results.\nURI\nsystem/audit/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\nfortiguard-blacklist: select\nSummary\nRetrieve blacklist information for a specified IP.\nURI\nsystem/fortiguard-blacklist/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nip\nstring\nIPv4 address to check against.\nYes\ntimeout\nint\nTimeout period in seconds (defaults to 5).\nNo\nvpn-certificate\nca: import\nSummary\nImport CA certificate.\nMonitor API\n99",
          "char_count": 673,
          "ocr_used": false
        },
        {
          "page": 100,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nURI\nvpn-certificate/ca/import/\nHTTP Method\nPOST\nAction\nimport\nAccess Group\nvpngrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nimport_method\nstring\nMethod of importing CA certificate.[file|scep]\nYes\nscep_url\nstring\nSCEP server URL. Required for import via SCEP\nNo\nscep_ca_id\nstring\nSCEP server CA identifier for import via SCEP.\nNo\nscope\nstring\nScope of CA certificate [vdom*|global]. Global scope is only\naccessible for global administrators\nNo\nfile_content\nstring\nProvided when uploading a file: base64 encoded file data. Must not\ncontain whitespace or other invalid base64 characters. Must be\nincluded in HTTP body.\nNo\ncrl: import\nSummary\nImport certificate revocation lists (CRL) from file content.\nURI\nvpn-certificate/crl/import/\nHTTP Method\nPOST\nAction\nimport\nAccess Group\nvpngrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nscope\nstring\nScope of CRL [vdom*|global]. Global scope is only accessible for\nglobal administrators\nNo\nfile_content\nstring\nProvided when uploading a file: base64 encoded file data. Must not\ncontain whitespace or other invalid base64 characters. Must be\nincluded in HTTP body.\nNo\nMonitor API\n100",
          "char_count": 1207,
          "ocr_used": false
        },
        {
          "page": 101,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nlocal: import\nSummary\nImport local certificate.\nURI\nvpn-certificate/local/import/\nHTTP Method\nPOST\nAction\nimport\nAccess Group\nvpngrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\ntype\nstring\nType of certificate.[local|pkcs12|regular]\nYes\ncertname\nstring\nCertificate name for pkcs12 and regular certificate types.\nNo\npassword\nstring\nOptional password for pkcs12 and regular certificate types.\nNo\nkey_file_\ncontent\nstring\nKey content encoded in BASE64 for regular certificate type.\nNo\nscope\nstring\nScope of local certificate [vdom*|global]. Global scope is only\naccessible for global administrators\nNo\nfile_content\nstring\nProvided when uploading a file: base64 encoded file data. Must not\ncontain whitespace or other invalid base64 characters. Must be\nincluded in HTTP body.\nNo\nremote: import\nSummary\nImport remote certificate.\nURI\nvpn-certificate/remote/import/\nHTTP Method\nPOST\nAction\nimport\nAccess Group\nvpngrp\nResponse Type\nobject\nMonitor API\n101",
          "char_count": 1009,
          "ocr_used": false
        },
        {
          "page": 102,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nfile_content\nstring\nProvided when uploading a file: base64 encoded file data. Must not\ncontain whitespace or other invalid base64 characters. Must be\nincluded in HTTP body.\nNo\ncsr: generate\nSummary\nGenerate a certificate signing request (CSR) and a private key. The CSR can be\nretrieved / downloaded from CLI, GUI and REST API.\nURI\nvpn-certificate/csr/generate/\nHTTP Method\nPOST\nAction\ngenerate\nAccess Group\nvpngrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\ncertname\nstring\nCerticate name. Used to retrieve / download the CSR. Not included\nin CSR and key content.\nYes\nsubject\nstring\nSubject (Host IP/Domain Name/E-Mail). Common Name (CN) of the\ncertificate subject.\nYes\nkeytype\nstring\nGenerate a RSA or an elliptic curve certificate request [rsa|ec]. The\nElliptic Curve option is unavailable if the FortiGate is a Low\nEncryption Device (LENC)\nYes\nkeysize\nint\nKey size.[1024|1536|2048|4096]. 512 only if the FortiGate is a Low\nEncryption Device (LENC). Required when keytype is RSA.\nNo\ncurvename\nstring\nElliptic curve name. [secp256r1|secp384r1|secp521r1]. Unavailable if\nthe FortiGate is a Low Encryption Device (LENC). Required when\nkeytype is ec.\nNo\norgunits\narray\nList of organization units. Organization Units (OU) of the certificate\nsubject.\nNo\norg\nstring\nOrganization (O) of the certificate subject.\nNo\ncity\nstring\nLocality (L) of the certificate subject.\nNo\nstate\nstring\nState (ST) of the certificate subject.\nNo\nMonitor API\n102",
          "char_count": 1543,
          "ocr_used": false
        },
        {
          "page": 103,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nName\nType\nSummary\nRequired\ncountrycode\nstring\nCountry (C) of the certificate subject.\nNo\nemail\nstring\nEmail of the certificate subject.\nNo\nsub_alt_name\nstring\nSubject alternative name (SAN) of the certificate.\nNo\npassword\nstring\nPassword / pass phrase for the private key. If not provided, FortiGate\ngenerates a random one.\nNo\nscep_url\nstring\nSCEP server URL. If provided, use the url to enroll the csr through\nSCEP.\nNo\nscep_password\nstring\nSCEP challenge password. Some SCEP servers may require\nchallege password. Provide it when SCEP server requires.\nNo\nscope\nstring\nScope of CSR [vdom*|global]. Global scope is only accessible for\nglobal administrators\nNo\ncheck-port-availability: select\nSummary\nCheck whether a list of TCP port ranges is available for a certain service.\nURI\nsystem/check-port-availability/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nport_ranges\narray\nList of TCP port range objects to check against.\nYes\nservice\nstring\nThe service in which the ports could be available. 'service' options are\n[reserved | sysglobal | webproxy | ftpproxy | sslvpn | slaprobe | fsso |\nftm_push]. If 'service' is not specified, the port ranges availablity is\nchecked against all services.\nNo\nextender-controller\nextender: select\nSummary\nRetrieve statistics for specific configured FortiExtender units.\nMonitor API\n103",
          "char_count": 1437,
          "ocr_used": false
        },
        {
          "page": 104,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nURI\nextender-controller/extender/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nnetgrp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nid\narray\nList of FortiExtender IDs to query.\nYes\nextender: reset\nSummary\nReset a specific FortiExtender unit.\nURI\nextender-controller/extender/reset/\nHTTP Method\nPOST\nAction\nreset\nAccess Group\nnetgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nid\nstring\nFortiExtender ID to reset.\nYes\nuser\nfirewall: select\nSummary\nList authenticated firewall users.\nURI\nuser/firewall/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nauthgrp\nMonitor API\n104",
          "char_count": 660,
          "ocr_used": false
        },
        {
          "page": 105,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nstart\nint\nStarting entry index.\nNo\ncount\nint\nMaximum number of entries to return.\nNo\nipv4\nboolean\nInclude IPv4 user (default=true).\nNo\nipv6\nboolean\nInclude IPv6 users.\nNo\nfirewall: deauth\nSummary\nDeauthenticate single, multiple, or all firewall users.\nURI\nuser/firewall/deauth/\nHTTP Method\nPOST\nAction\ndeauth\nAccess Group\nauthgrp\nExtra parameters\nName\nType\nSummary\nRequired\nuser_type\nstring\nUser type [proxy|firewall]. Required for both proxy and firewall users.\nNo\nid\nint\nUser ID. Required for both proxy and firewall users.\nNo\nip\nstring\nUser IP address. Required for both proxy and firewall users.\nNo\nip_version\nstring\nIP version [ip4|ip6]. Only required if user_type is firewall.\nNo\nmethod\nstring\nAuthentication method [fsso|rsso|ntlm|firewall|wsso|fsso_citrix|sso_\nguest]. Only required if user_type is firewall.\nNo\nall\nboolean\nSet to true to deauthenticate all users. Other parameters will be\nignored.\nNo\nusers\narray\nArray of user objects to deauthenticate. Use this to deauthenticate\nmultiple users at once. Each object should include the above\nproperties.\nNo\nMonitor API\n105",
          "char_count": 1187,
          "ocr_used": false
        },
        {
          "page": 106,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nbanned: select\nSummary\nReturn a list of all banned users by IP.\nURI\nuser/banned/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nauthgrp\nbanned: clear_users\nSummary\nImmediately clear a list of specific banned users by IP.\nURI\nuser/banned/clear_users/\nHTTP Method\nPOST\nAction\nclear_users\nAccess Group\nauthgrp\nExtra parameters\nName\nType\nSummary\nRequired\nip_addresses\narray\nList of banned user IPs to clear. IPv4 and IPv6 addresses are\nallowed.\nYes\nbanned: add_users\nSummary\nImmediately add one or more users to the banned list.\nURI\nuser/banned/add_users/\nHTTP Method\nPOST\nAction\nadd_users\nAccess Group\nauthgrp\nExtra parameters\nName\nType\nSummary\nRequired\nip_addresses\narray\nList of IP Addresses to ban. IPv4 and IPv6 addresses are allowed.\nYes\nexpiry\nint\nTime until expiry in seconds. 0 for indefinite ban.\nNo\nMonitor API\n106",
          "char_count": 866,
          "ocr_used": false
        },
        {
          "page": 107,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nbanned: clear_all\nSummary\nImmediately clear all banned users.\nURI\nuser/banned/clear_all/\nHTTP Method\nPOST\nAction\nclear_all\nAccess Group\nauthgrp\nfortitoken: select\nSummary\nRetrieve a map of FortiTokens and their status.\nURI\nuser/fortitoken/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nauthgrp\nResponse Type\nobject\nfortitoken: activate\nSummary\nActivate a set of FortiTokens by serial number.\nURI\nuser/fortitoken/activate/\nHTTP Method\nPOST\nAction\nactivate\nAccess Group\nauthgrp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\ntokens\narray\nList of FortiToken serial numbers to activate. If omitted, all tokens\nwill be used.\nNo\nMonitor API\n107",
          "char_count": 701,
          "ocr_used": false
        },
        {
          "page": 108,
          "text": "FortiOS REST API Reference\nFortinet Inc.\ndevice: select\nSummary\nRetrieve a list of detected devices.\nURI\nuser/device/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nETag Caching\nEnabled\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nmaster_only\nboolean\nList of master device only.\nNo\nfortilink_\nvisibility\nboolean\nAdd port and switch info for devices behind a managed FortiSwitch.\nNo\ncompliance_\nvisibility\nboolean\nAdd compliance status to indicate if a device is 'exempt' or 'non-\ncompliant' by interface's FortiClient host check.\nNo\nintf_name\nstring\nFilter: Name of interface where the device was detected. Only\navailable when compliance_visibility is true.\nNo\nmaster_mac\nstring\nFilter: Master MAC of a device. Multiple entries could be returned.\nNo\nfortitoken: refresh\nSummary\nRefresh a set of FortiTokens by serial number.\nURI\nuser/fortitoken/refresh/\nHTTP Method\nPOST\nAction\nrefresh\nAccess Group\nauthgrp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\ntokens\narray\nList of FortiToken serial numbers to refresh. If omitted, all tokens will\nbe used.\nNo\nMonitor API\n108",
          "char_count": 1115,
          "ocr_used": false
        },
        {
          "page": 109,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nfortitoken: provision\nSummary\nProvision a set of FortiTokens by serial number.\nURI\nuser/fortitoken/provision/\nHTTP Method\nPOST\nAction\nprovision\nAccess Group\nauthgrp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\ntokens\narray\nList of FortiToken serial numbers to provision. If omitted, all tokens\nwill be used.\nNo\nfortitoken: send-activation\nSummary\nSend a FortiToken activation code to a user via SMS or Email.\nURI\nuser/fortitoken/send-activation/\nHTTP Method\nPOST\nAction\nsend-activation\nAccess Group\nauthgrp\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\ntoken\nstring\nFortiToken serial number. The token must be assigned to a\nuser/admin.\nYes\nmethod\nstring\nMethod to send activation code [email|sms].\nYes\nemail\nstring\nOverride email address.\nNo\nsms_phone\nstring\nOverride SMS phone number. SMS provider must be set in the\nassigned user/admin.\nNo\nMonitor API\n109",
          "char_count": 937,
          "ocr_used": false
        },
        {
          "page": 110,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nfsso: refresh-server\nSummary\nRefresh remote agent group list for all fsso agents.\nURI\nuser/fsso/refresh-server/\nHTTP Method\nPOST\nAction\nrefresh-server\nAccess Group\nauthgrp\nfsso: select\nSummary\nGet a list of fsso and fsso polling status.\nURI\nuser/fsso/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nauthgrp\nutm\nrating-lookup: select\nSummary\nLookup FortiGuard rating for a specific URL.\nURI\nutm/rating-lookup/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nutmgrp.webfilter\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nurl\nstring\nURL to query.\nYes\nurl\narray\nList of URLs to query.\nNo\nMonitor API\n110",
          "char_count": 666,
          "ocr_used": false
        },
        {
          "page": 111,
          "text": "FortiOS REST API Reference\nFortinet Inc.\napp-lookup: select\nSummary\nQuery remote FortiFlow database to resolve hosts to application control entries.\nURI\nutm/app-lookup/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nany\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nhosts\narray\nList of hosts to resolve.\nNo\naddress\nstring\nDestination IP for one host entry.\nNo\ndst_port\nint\nDestination port for one host entry.\nNo\nprotocol\nint\nProtocol for one host entry.\nNo\napplication-categories: select\nSummary\nRetrieve a list of application control categories.\nURI\nutm/application-categories/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nany\nResponse Type\narray\nantivirus: stats\nSummary\nRetrieve antivirus scanning statistics.\nURI\nutm/antivirus/stats/\nHTTP Method\nGET\nAction\nstats\nAccess Group\nutmgrp.antivirus\nResponse Type\nobject\nMonitor API\n111",
          "char_count": 861,
          "ocr_used": false
        },
        {
          "page": 112,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nvirtual-wan\nhealth-check: select\nSummary\nRetrieve health-check statistics for each SD-WAN link.\nURI\nvirtual-wan/health-check/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nmembers: select\nSummary\nRetrieve interface statistics for each SD-WAN link.\nURI\nvirtual-wan/members/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nsysgrp\nwebfilter\noverride: select\nSummary\nList all administrative and user initiated webfilter overrides.\nURI\nwebfilter/override/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nutmgrp.webfilter\noverride: delete\nSummary\nDelete a configured webfilter override.\nURI\nwebfilter/override/delete/\nHTTP Method\nPOST\nMonitor API\n112",
          "char_count": 701,
          "ocr_used": false
        },
        {
          "page": 113,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nAction\ndelete\nAccess Group\nutmgrp.webfilter\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nstring\nID of webfilter override to delete.\nNo\nmalicious-urls: select\nSummary\nList all URLs in FortiSandbox malicious URL database.\nURI\nwebfilter/malicious-urls/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nutmgrp.webfilter\nETag Caching\nEnabled\nResponse Type\nobject\nmalicious-urls: stat\nSummary\nRetrieve statistics for the FortiSandbox malicious URL database.\nURI\nwebfilter/malicious-urls/stat/\nHTTP Method\nGET\nAction\nstat\nAccess Group\nutmgrp.webfilter\nETag Caching\nEnabled\nResponse Type\nobject\ncategory-quota: select\nSummary\nRetrieve quota usage statistics for webfilter categories.\nURI\nwebfilter/category-quota/select/\nHTTP Method\nGET\nMonitor API\n113",
          "char_count": 791,
          "ocr_used": false
        },
        {
          "page": 114,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nAction\nselect\nAccess Group\nutmgrp.webfilter\nExtra parameters\nName\nType\nSummary\nRequired\nprofile\nstring\nWebfilter profile.\nNo\nuser\nstring\nUser or IP (required if profile specified).\nNo\ncategory-quota: reset\nSummary\nReset webfilter quota for user or IP.\nURI\nwebfilter/category-quota/reset/\nHTTP Method\nPOST\nAction\nreset\nAccess Group\nutmgrp.webfilter\nExtra parameters\nName\nType\nSummary\nRequired\nprofile\nstring\nWebfilter profile to reset.\nNo\nuser\nstring\nUser or IP to reset with.\nNo\nfortiguard-categories: select\nSummary\nReturn FortiGuard web filter categories.\nURI\nwebfilter/fortiguard-categories/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nany\nResponse Type\narray\nMonitor API\n114",
          "char_count": 726,
          "ocr_used": false
        },
        {
          "page": 115,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\ninclude_\nunrated\nboolean\nInclude Unrated category in result list.\nNo\nconvert_\nunrated_id\nboolean\nConvert Unrated category id to the one for CLI use.\nNo\ntrusted-urls: select\nSummary\nList all URLs in FortiGuard trusted URL database.\nURI\nwebfilter/trusted-urls/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nutmgrp.webfilter\nETag Caching\nEnabled\nResponse Type\nobject\nvpn\nipsec: select\nSummary\nReturn an array of active IPsec VPNs.\nURI\nvpn/ipsec/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nvpngrp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\ntunnel\nstring\nFilter for a specific IPsec tunnel name.\nNo\nstart\nint\nStarting entry index.\nNo\ncount\nint\nMaximum number of entries to return.\nNo\nMonitor API\n115",
          "char_count": 812,
          "ocr_used": false
        },
        {
          "page": 116,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nipsec: tunnel_up\nSummary\nBring up a specific IPsec VPN tunnel.\nURI\nvpn/ipsec/tunnel_up/\nHTTP Method\nPOST\nAction\ntunnel_up\nAccess Group\nvpngrp\nExtra parameters\nName\nType\nSummary\nRequired\np1name\nstring\nIPsec phase1 name.\nYes\np2name\nstring\nIPsec phase2 name.\nYes\np2serial\nstring\nIPsec phase2 serial.\nNo\nipsec: tunnel_down\nSummary\nBring down a specific IPsec VPN tunnel.\nURI\nvpn/ipsec/tunnel_down/\nHTTP Method\nPOST\nAction\ntunnel_down\nAccess Group\nvpngrp\nExtra parameters\nName\nType\nSummary\nRequired\np1name\nstring\nIPsec phase1 name.\nYes\np2name\nstring\nIPsec phase2 name.\nYes\np2serial\nstring\nIPsec phase2 serial.\nNo\nipsec: tunnel_reset_stats\nSummary\nReset statistics for a specific IPsec VPN tunnel.\nURI\nvpn/ipsec/tunnel_reset_stats/\nMonitor API\n116",
          "char_count": 783,
          "ocr_used": false
        },
        {
          "page": 117,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nHTTP Method\nPOST\nAction\ntunnel_reset_stats\nAccess Group\nvpngrp\nExtra parameters\nName\nType\nSummary\nRequired\np1name\nstring\nIPsec phase1 name.\nYes\nssl: select\nSummary\nRetrieve a list of all SSL-VPN sessions and sub-sessions.\nURI\nvpn/ssl/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nvpngrp\nssl: clear_tunnel\nSummary\nRemove all active tunnel sessions in current virtual domain.\nURI\nvpn/ssl/clear_tunnel/\nHTTP Method\nPOST\nAction\nclear_tunnel\nAccess Group\nvpngrp\nssl: delete\nSummary\nTerminate the provided SSL-VPN session.\nURI\nvpn/ssl/delete/\nHTTP Method\nPOST\nAction\ndelete\nAccess Group\nvpngrp\nMonitor API\n117",
          "char_count": 650,
          "ocr_used": false
        },
        {
          "page": 118,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\ntype\nstring\nThe session type [websession|subsession].\nYes\nindex\nint\nThe session index.\nYes\nssl: stats\nSummary\nReturn statistics about the SSL-VPN.\nURI\nvpn/ssl/stats/\nHTTP Method\nGET\nAction\nstats\nAccess Group\nvpngrp\nwanopt\nhistory: select\nSummary\nRetrieve WAN opt. statistics history.\nURI\nwanopt/history/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nwanoptgrp\nExtra parameters\nName\nType\nSummary\nRequired\nperiod\nstring\nStatistics period [10-min*|hour|day|week|30-day].\nNo\nhistory: reset\nSummary\nReset WAN opt. statistics.\nURI\nwanopt/history/reset/\nHTTP Method\nPOST\nMonitor API\n118",
          "char_count": 669,
          "ocr_used": false
        },
        {
          "page": 119,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nAction\nreset\nAccess Group\nwanoptgrp\nwebcache: select\nSummary\nRetrieve webcache statistics history.\nURI\nwanopt/webcache/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nwanoptgrp\nExtra parameters\nName\nType\nSummary\nRequired\nperiod\nstring\nStatistics period [10-min*|hour|day|week|30-day].\nNo\nwebcache: reset\nSummary\nReset webcache statistics.\nURI\nwanopt/webcache/reset/\nHTTP Method\nPOST\nAction\nreset\nAccess Group\nwanoptgrp\npeer_stats: select\nSummary\nRetrieve a list of WAN opt peer statistics.\nURI\nwanopt/peer_stats/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nwanoptgrp\nMonitor API\n119",
          "char_count": 633,
          "ocr_used": false
        },
        {
          "page": 120,
          "text": "FortiOS REST API Reference\nFortinet Inc.\npeer_stats: reset\nSummary\nReset WAN opt peer statistics.\nURI\nwanopt/peer_stats/reset/\nHTTP Method\nPOST\nAction\nreset\nAccess Group\nwanoptgrp\nwebproxy\npacfile: download\nSummary\nDownload webproxy PAC file.\nURI\nwebproxy/pacfile/download/\nHTTP Method\nGET\nAction\ndownload\nAccess Group\nnetgrp\nResponse Type\nobject\nwebcache\nstats: select\nSummary\nRetrieve webcache statistics.\nURI\nwebcache/stats/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nwanoptgrp\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nperiod\nstring\nStatistics period [10min|hour|day|month].\nNo\nMonitor API\n120",
          "char_count": 627,
          "ocr_used": false
        },
        {
          "page": 121,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nstats: reset\nSummary\nReset all webcache statistics.\nURI\nwebcache/stats/reset/\nHTTP Method\nPOST\nAction\nreset\nAccess Group\nwanoptgrp\nwifi\nclient: select\nSummary\nRetrieve a list of connected WiFi clients.\nURI\nwifi/client/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nwifi\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nstart\nint\nStarting entry index.\nNo\ncount\nint\nMaximum number of entries to return.\nNo\ntype\nstring\nRequest type [all*|fail-login].\nNo\nmanaged_ap: select\nSummary\nRetrieve a list of managed FortiAPs.\nURI\nwifi/managed_ap/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nwifi\nResponse Type\narray\nMonitor API\n121",
          "char_count": 687,
          "ocr_used": false
        },
        {
          "page": 122,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nExtra parameters\nName\nType\nSummary\nRequired\nwtp_id\nstring\nFilter: single managed FortiAP by ID.\nNo\nincl_local\nboolean\nEnable to include the local FortiWiFi device in the results.\nNo\nmanaged_ap: set_status\nSummary\nUpdate administrative state for a given FortiAP (enable or disable authorization).\nURI\nwifi/managed_ap/set_status/\nHTTP Method\nPOST\nAction\nset_status\nAccess Group\nwifi\nExtra parameters\nName\nType\nSummary\nRequired\nwtpname\nstring\nFortiAP name.\nNo\nadmin\nstring\nNew FortiAP administrative state [enable|disable|discovered].\nNo\nfirmware: select\nSummary\nRetrieve a list of current and recommended firmware for FortiAPs in use.\nURI\nwifi/firmware/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nwifi\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\ntimeout\nstring\nFortiGuard connection timeout (defaults to 2 seconds).\nNo\nMonitor API\n122",
          "char_count": 902,
          "ocr_used": false
        },
        {
          "page": 123,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nmanaged_ap: restart\nSummary\nRestart a given FortiAP.\nURI\nwifi/managed_ap/restart/\nHTTP Method\nPOST\nAction\nrestart\nAccess Group\nwifi\nExtra parameters\nName\nType\nSummary\nRequired\nwtpname\nstring\nFortiAP name.\nNo\nmanaged_ap: upgrade\nSummary\nUpgrade firmware image on the given FortiAP using uploaded file.\nURI\nwifi/managed_ap/upgrade/\nHTTP Method\nPOST\nAction\nupgrade\nAccess Group\nwifi\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nmkey\nstring\nSerial number of FortiAP to upgrade.\nYes\nsource\nstring\nFirmware file data source [upload|fortiguard].\nYes\nfilename\nstring\nFirmware image file for when 'source' is 'upload'.\nNo\nimage_id\nstring\nFortiguard image file ID for when 'source' is 'fortiguard'.\nNo\nfile_content\nstring\nProvided when uploading a file: base64 encoded file data. Must not\ncontain whitespace or other invalid base64 characters. Must be\nincluded in HTTP body.\nNo\nMonitor API\n123",
          "char_count": 945,
          "ocr_used": false
        },
        {
          "page": 124,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nap_status: select\nSummary\nRetrieve statistics for all managed FortiAPs.\nURI\nwifi/ap_status/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nwifi\ninterfering_ap: select\nSummary\nRetrieve a list of interfering APs for one FortiAP radio.\nURI\nwifi/interfering_ap/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nwifi\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nwtp\nstring\nFortiAP ID to query.\nNo\nradio\nint\nRadio ID.\nNo\nstart\nint\nStarting entry index.\nNo\ncount\nint\nMaximum number of entries to return.\nNo\neuclid: select\nSummary\nRetrieve presence analytics statistics.\nURI\nwifi/euclid/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nwifi\nMonitor API\n124",
          "char_count": 715,
          "ocr_used": false
        },
        {
          "page": 125,
          "text": "FortiOS REST API Reference\nFortinet Inc.\neuclid: reset\nSummary\nReset presence analytics statistics.\nURI\nwifi/euclid/reset/\nHTTP Method\nPOST\nAction\nreset\nAccess Group\nwifi\nrogue_ap: select\nSummary\nRetrieve a list of detected rogue APs.\nURI\nwifi/rogue_ap/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nwifi\nResponse Type\narray\nExtra parameters\nName\nType\nSummary\nRequired\nstart\nint\nStarting entry index.\nNo\ncount\nint\nMaximum number of entries to return.\nNo\nrogue_ap: clear_all\nSummary\nClear all detected rogue APs.\nURI\nwifi/rogue_ap/clear_all/\nHTTP Method\nPOST\nAction\nclear_all\nAccess Group\nwifi\nrogue_ap: set_status\nSummary\nMark detected APs as rogue APs.\nMonitor API\n125",
          "char_count": 674,
          "ocr_used": false
        },
        {
          "page": 126,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nURI\nwifi/rogue_ap/set_status/\nHTTP Method\nPOST\nAction\nset_status\nAccess Group\nwifi\nExtra parameters\nName\nType\nSummary\nRequired\nbssid\narray\nList of rogue AP MAC addresses.\nNo\nssid\narray\nCorresponding list of rogue AP SSIDs.\nNo\nstatus\nstring\nStatus to assign matching APs\n[unclassified|rogue|accepted|suppressed].\nNo\nspectrum: select\nSummary\nRetrieve spectrum analysis information for a specific FortiAP.\nURI\nwifi/spectrum/select/\nHTTP Method\nGET\nAction\nselect\nAccess Group\nwifi\nResponse Type\nobject\nExtra parameters\nName\nType\nSummary\nRequired\nwtp_id\nstring\nFortiAP ID to query.\nYes\ncoverage\ndownload: select\nSummary\nDownload code coverage.\nURI\ncoverage/download/select/\nHTTP Method\nGET\nMonitor API\n126",
          "char_count": 742,
          "ocr_used": false
        },
        {
          "page": 127,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nAction\nselect\nAccess Group\nany\nResponse Type\nobject\nExamples\nMethod\nURL\nURL Parameters\nBody Data\nAccess\nGroup\nDescription\nGET\n/api/v2/monitor/\nfirewall/policy\n?vdom=root\nfwgrp.policy\nList traffic statistics for all\nIPv4 policies, vdom root\nGET\n/api/v2/monitor/\nfirewall/policy\n?global=1\nfwgrp.policy\nList traffic statistics for all\nIPv4 policies, all\naccessible vdoms\nPOST\n/api/v2/monitor/\nfirewall/policy/reset\n?vdom=root\nfwgrp.policy\nReset traffic statistics for\nall IPv4 policies, vdom root\nPOST\n/api/v2/monitor/\nfirewall/policy/reset\n?global=1\nfwgrp.policy\nReset traffic statistics for\nall IPv4 policies, all\naccessible vdoms\nPOST\n/api/v2/monitor/\nfirewall/policy6/\nclear_counters\n?vdom=root\n{'policy': 1,}\nfwgrp.policy\nReset traffic statistics for\nsingle IPv4 policy, vdom\nroot\nPOST\n/api/v2/monitor/\nfirewall/policy6/\nclear_counters\n?vdom=root\n{'policy': [1, 2]}\nfwgrp.policy\nReset traffic statistics for\nmultiple IPv4 policies,\nvdom root\nGET\n/api/v2/monitor/\nfirewall/session\n?vdom=root&\nip_version=ipv4&\nstart=0&count=\n1&summary=True\nsysgrp\nList the first active ipv4\nfirewall sessions, vdom\nroot\nPOST\n/api/v2/monitor/\nfirewall/session/\nclear_all\n?vdom=root\nsysgrp\nImmediately clear all\nactive IPv4 and IPv6\nsessions, vdom root\nPOST\n/api/v2/monitor/\nfirewall/session/\nclose\n?vdom=root\n{'pro': \"udp\", 'saddr':\n\"192.168.100.110\",\n'daddr': \"96.45.33.\n73\", 'sport': 55933,\n'dport': 8888}\nsysgrp\nImmediately close specific\nsession matched with the\nfilter, vdom root\nMonitor API\n127",
          "char_count": 1525,
          "ocr_used": false
        },
        {
          "page": 128,
          "text": "FortiOS REST API Reference\nFortinet Inc.\nMethod\nURL\nURL Parameters\nBody Data\nAccess\nGroup\nDescription\nPOST\n/api/v2/monitor/\nsystem/os/\nreboot\nsysgrp\nImmediately reboot this\ndevice\nPOST\n/api/v2/monitor/\nsystem/os/\nshutdown\nsysgrp\nImmediately shutdown this\ndevice\nMonitor API\n128",
          "char_count": 278,
          "ocr_used": false
        },
        {
          "page": 129,
          "text": "Copyright© 2019 Fortinet, Inc. All rights reserved. Fortinet®, FortiGate®, FortiCare® and FortiGuard®, and certain other marks are registered trademarks of Fortinet, Inc., in\nthe U.S. and other jurisdictions, and other Fortinet names herein may also be registered and/or common law trademarks of Fortinet. All other product or company names may\nbe trademarks of their respective owners. Performance and other metrics contained herein were attained in internal lab tests under ideal conditions, and actual performance\nand other results may vary. Network variables, different network environments and other conditions may affect performance results. Nothing herein represents any binding\ncommitment by Fortinet, and Fortinet disclaims all warranties, whether express or implied, except to the extent Fortinet enters a binding written contract, signed by Fortinet’s\nGeneral Counsel, with a purchaser that expressly warrants that the identified product will perform according to certain expressly-identified performance metrics and, in such\nevent, only the specific performance metrics expressly identified in such binding written contract shall be binding on Fortinet. For absolute clarity, any such warranty will be\nlimited to performance in the same ideal conditions as in Fortinet’s internal lab tests. In no event does Fortinet make any commitment related to future deliverables, features\nor development, and circumstances may change such that any forward-looking statements herein are not accurate. Fortinet disclaims in full any covenants, representations,\nand guarantees pursuant hereto, whether express or implied. Fortinet reserves the right to change, modify, transfer, or otherwise revise this publication without notice, and\nthe most current version of the publication shall be applicable.",
          "char_count": 1799,
          "ocr_used": false
        }
      ],
      "metadata": {
        "format": "PDF 1.4",
        "title": "FortiOS REST API Reference",
        "author": "Fortinet Technologies Inc.",
        "subject": "FortiOS",
        "keywords": "FortiOS, 5.6.11, REST API Reference",
        "creator": "",
        "producer": "madbuild",
        "creationDate": "D:20190813125624-07'00'",
        "modDate": "D:20190813125624-07'00'",
        "trapped": "",
        "encryption": null
      },
      "char_count": 132580,
      "word_count": 17940,
      "ocr_pages_count": 1,
      "error": null,
      "file_id": "13yAlVVwc4WkLVf0P5ozaMAU1yfLcJzFc",
      "filename": "FortiOS-5.6.11-Rest-API_Reference.pdf",
      "filepath": "downloads/FortiOS-5.6.11-Rest-API_Reference.pdf",
      "download_link": "https://drive.google.com/uc?export=download&id=13yAlVVwc4WkLVf0P5ozaMAU1yfLcJzFc"
    },
    {
      "success": true,
      "text": "Heap and Stack memory\n05/23/94\nIntroduction +\nVirtual memory in a computer is a large long array of bits and these bits are divided into blocks called bytes (every 8\n.bits = 1 byte) and each byte is assigned an address for access\nIn programming languages, when you work with non-physical data (such as variables, objects, functions, etc.), the\nvalue and address of these data are stored in virtual memory . At a lower level, when you define and use a local\nvariable or a function, their values and addresses are placed in the Stack section of the virtual memory. But by\ncreating the object or allocating the memory manually (Dynamic), their value and address are placed in the Heap\npart of the virtual memory. And finally, all of these are arranged in the RAM cells of the computer hardware in an\n.orderly manner\nos\nFree space\nA Static program data\n7 ¢ \\\no AEETTC TTT TTT CUEESEN ESTEE CATS ERTS UTTTEET TPE TE\n+) < AC BRINIG SORT it 0\n1 5 of ij\nt Program stack Program heap t\nHigher memory addr Lower memory addr\nOxFFFFFFFF 0x00000000\n:Table of contents\nStack memory «\nStack memory rules o\nStackoverflow problem o\nHeap memory rules o\nGarbage Collector / GC o\nMemory leak problem o\nStack memory +\nStack memory is like a bunch of plates that are placed on top of each other... whenever you want to put a plate on the\npile of plates (push), you put it on top. In the same way, whenever you want to remove a plate from this category\n+ (Pop), you always remove the top plate... This category is called Stack or LIFO or LastIn-FirstOut\n{6}\nGS) Push\n{4} push) [5]\n{3} Push Y [4] [4]\n(2) Push 31 3]\nPushy fa FI 2]\ncn] L4] 4]\nVaal)\n(Pop 72t{s)\n[3] ( Pop 2)\n(4) ka\nThe Stack memory is located in the user-space part of the memory and is automatically managed by the CPU . This\nare accumulated inside this memory as LIFO (Last In First Out ) and are known as Frame . When a function is called\nand executed, the function, its parameters and all its internal non-static variables are placed in the Stack memory. By\nalling the new function, this function is placed on top of the previous function, and when the work of one of these\nfunctions is finished, that function is removed from the Stack memory along with all the relevant variables... and the\n.Work continues in the same way\n.Note: Another name for this type of memory allocation is Static memory allocation\nNote 2: The memory that is talked about in Memory Firewall software, this part of Stack and Heap is virtual memory\n.and not physical memory/RAM\nNote 3: In some programming languages such as Cpp, you can store the object in the Stack memory as well,\n.provided that it is inside the function and you do not use new\nExample\nfunction myFunction\n{ paraml , param2 ) // To Stack memory {\n_testVar = paraml + param2 ; // To stack memory return testVar ; // To Stack men\nImage example\nStack area\n= .\n=\n|\nmain ()\npeo\nint numer = 6; 5\nreturn myPOgc (30) Mx j\n}\n< Ss 4 myFunction ()\nmyFunction (param1) NY,\n.\nbool isEnable = FAL’\na | 2S\ni]\noe | ee\nSl\n\nStack memory rules +\n\n.In the virtual memory of the device, the Stack section is located .1\n\nIt is the place to store local variables (non-static), parameters of functions, return addresses of functions. (also .2\n\n.The stack size of the program is determined and allocated during compilation .3\n\n. Itis responsive to the problem of Stackoverflow .4\n\nIt is automatically managed by the CPU .5\n\nWhen the function is called, it is pushed into the Stack memory and when the function is finished, it is Popped .6\n.from the Stack memory\n\n-In terms of speed in occupying space, it is faster than Heap memory .7\n\n. language , classes and structures are stored in Stack memory if pointers are not used In C/Cpp .8\n\n.A stackoverflow error can occur during unwise allocation/ occupation or over capacity .9\n\nEach program has a main thread and each thread has a private Stack memory. So, as long as the thread is not .10\nclosed, the stack is still there. (closing here means exit and not terminate)\n\n.The Stack memory area of the program is attached to the thread of that program .11\n\nThe data in the Stack memory is sequentially placed on top of each other. (with the LIFO rule) .12\n\n.As stack memory consumption increases, less memory is left for heap .13\n\n“This type of memory is readable and writable .14\n\nAnd .15\nFrame\ner )\nStackoverflow problem «\nThe word stackoverflow literally means stack overflow. Among the reasons for the stackoverflow error, we can mention\nthe depth of nested and chain functions, bulky local variables, excessive stack size, destruction or corruption of part of\n.the memory, wrong use of Native API\n.Note: Stackoverflow problems are errors and not exceptions\n:By running the following codes, you will get a better understanding of Stack memory and the Stackoverflow problem\n:PHP example\n<? php\nfunction myInfiniteRecursion () {\nmyInfiniteRecursion (); }\nImyInfiniteRecursion ();\nPHP Fatal error: Allowed memory size of 134217728 bytes(134MB) exhausted (tried to\nallocate 130968 bytes) in develop.php on line 7.\nIn this example, every time the function is called myInfiniteRecursion(), a Stack Frame is created for that\nfunction in the Stack memory, and this process continues as long as the PHP engine (memory_limit value in php.ini)\nallows... Finally, the allowed capacity of the engine's Stack memory PHP fills up and the script tries to access beyond\nthe capacity of the Stack memory, but the system does not allow it and stops the script with the\n.Stackoverflow/OxCOO000FD error\n:Java example\nclass MyClass { private MyClass myCls = new MyClass ();\npublic static void main ( String [] args ) { new MyClass (); } } // javac MyClas\nException in thread \"main\" java.lang.StackOverflowError\n:C# example\nclass Program { static void Recursive () { Recursive (); }\nstatic void Main () { Recursive (); } }\nSystem. StackOverflow Exception: ‘Exception of type \" was thrown.\"\nExample C\nint main () { int large [ 10000000 ] = { 0 }; return 0 ; }\nLinux: Segmentation fault.\n\nWindows: Unhandled exception in develop.exe: OxCO00000FD: Stackoverflow.\n;You can see that the words stack and overflow are common in all errors\nWindows tip: You can change the stack size of executable programs with the editbin program available in VS\nand nasm32\n.Note: The buffer overflow error is not related to the stack overflow error and these two are different from each other\nHeap memory «\nHeap memory is located in the user-space part of the virtual memory and is manually managed by the programmer\n. In fact, data is stored in this memory by the programmer and must be emptied/destroyed by the\nprogrammer himself . The data stored in the Heap memory will not be lost when the call is completed and will remain\nuntil the programmer deletes this data from the Heap memory and even cause a memory leak or OutOfMemory.\n.Unless they are detected and destroyed by Garbage Collector\nNote: Here we mean data, objects that neware created by, variables that calloc/alloc/malloe are defined by,\n. and values that are moved by pointers\nNote 2: Another name for this type of memory allocation is Dynamic memory allocation\nint myfunction ()\n{ Heap memory\nint i;\nMyObject object = new MyObject();\nint #myP = (int#)malloc(10) ; on 7\nreturn 4 1)\n. @)\nHeap memory size is determined relative to the size of the total data (mentioned above) and the operating system\n.gives heap space to the program as much as it can and the user is not limited and has free space in RAM\nNote: In scripting languages, Heap memory management is done by the interpreter and in VMs by the runtime\n-language\nHeap memory rules «\n\n. The Heap section is stored in the device's virtual memory .1\n\nIt is the place to store allocated objects and data .2\n\n.Heap memory is responsible for memory leak and OutOfMemory problems .3\n\n-In terms of speed in occupying space, it is slower than Stack memory because it is managed by pointers .4\n\n-In Cpp, the data stored in the Heap memory is called by the pointer and can be reoccupied .5\n\n.OutOfMemory can occur when allocation/reservation of memory is unwise .6\n\n.The data in this space are randomly placed together .7\n\nHeap memory area depends on the program runtime. That is, it is occupied by starting the program and .8\n\nemptied by closing the program\n\n.Heap memory size of the program is reserved/allocated during runtime/starting the program .9\n\n.all threads use only one Heap memory (in most languages) .10\n\n.As the Heap memory consumption increases, less memory is left for the stack .11\n\n.When the program is closed, the allocated space in Heap is emptied by the program .12\n\n«And .13\n.Note: The problem of OutOfMemory belongs to the category of Errors and not Exceptions\n:PHP example\n\nSobjTest = new MyObject (); // MyObject to Heap memory\n\nJava and C# example\nExample C\nIn the first and second examples, by creating the MyObject object, we occupied some Heap memory, in fact, we put\nthe MyObject object into the Heap memory and not its variable. In the third example, we put 4 bytes with the\nvariable ptr into malloc Heap memory , which we have to free and destroy later by coding_. But this is only\nCpp/C and low-level languages, and not in Java, PHP and high-level languages! possible in\nSo what is the task of memory emptying in high-level languages? Because the memory has a maximum capacity and\n...only a small part of it is available to our program\nThe answer to this problem is Garbage Collector ! The garbage collection system is designed for high-level\nlanguages to cover this deficiency to some extent\n‘Windows tip: You can change the heap size of executable programs with the editbin program available in VS\nsand nasm32\nGarbage Collector /GC «\nGarbage collector system (abbreviated GC) is a feature that [usually] runs automatically and starts checking objects\nthat are no longer used by the program (they are dead) and no references point to them (unreferenced ) . Then he\ndestroys them\nThis is the same 2-phase process that every garbage collector performs to empty the memory\nGarbage collector usually comes into action when\n\n.The read/write operation is done in the system memory .1\n\nThe program/script is finished and wants to be closed .2\n\n.Manually mute the sound (such as IDEs Netbeans, Eclipse, Java language, PHPS.3, etc.) .3\nNote: Whether or not objects are collected by the garbage collector has nothing to do with the issue of scope\n‘As mentioned, VMs and interpreters store objects created by the running application in the heap memory. Like objects\nthat are made by new. When the Heap is slow, the VM or the interpreter starts the garbage collection system; Then\nthe garbage collection process is executed in a circular manner and deletes the objects and data that can be collected\n.from the Heap memory... which, of course, slows down the system a little until its operation is finished\nMemory leak «\nIn short, it means: the objects are no longer used by the program, they are referenced somewhere, and the Garbage\n.Collector is not able to release (destroy) them\nIn other words: keeping built objects busy (in use) for no reason , which is equal to being ignored by the garbage\ncollector . (According to GC law)\nMemory Leak\nr Unused Objects —tRase Objects >\n‘Note: memory leaks are critical bugs and app-vulnerabilities, which originate from programmer's wrong coding\n:Other definitions\na failure in a program to release discarded memory, causing impaired performance or failure.\nFailure to release unreachable memory, which can no longer be allocated again by any process during the\nexecution of allocating process.\nDefinition: Failure to release memory after allocation.\nNote 2: Garbage collector system is not an insurance to fix such vandalism and wrong coding, but it is only an\n.auxiliary system\nExample of memory leak in PHP\nclass MyClass { // ... }\nSobj = new MyClass ;\nSobj -> self = Sobj ; // Memory leak by Sob}\n:Example of memory leak in C\nint main () { char * myVar = new char [ 5 J]; // Got 5 bytes in Heap memory. return q\n:Example of memory leak in Android Java\n@Override protected void onCreate ( Bundle state ) { super . onCreate ( state ); Te:\nbackgroundImage = getDrawable ( R . drawable . large bitmap );\nmyTextView . setBackgroundDrawable ( backgroundImage );\nsetContentView ( myTextView );\n// Memory leak }\nIn this Java example, memory leak occurs 2 times, 1- in the current state because the myTextView object variable is\nkept busy by the activity, 2- in the state that the orientation of the device changes, so the onCreate method is callea\nagai\n.Of course, the most sensitive memory leak is in systems that are facing limited resources or service type programs\nFull description of stack, heap, data of each process in virtual memory \n \n \nPlease note, these are images from the website that is made for ease share. The original author of the \nabove post is Yusha Al-Ayoub. \n Link for the website: https://yousha.blog.ir/post/45",
      "page_count": 14,
      "pages": [
        {
          "page": 1,
          "text": "Heap and Stack memory\n05/23/94\nIntroduction +\nVirtual memory in a computer is a large long array of bits and these bits are divided into blocks called bytes (every 8\n.bits = 1 byte) and each byte is assigned an address for access\nIn programming languages, when you work with non-physical data (such as variables, objects, functions, etc.), the\nvalue and address of these data are stored in virtual memory . At a lower level, when you define and use a local\nvariable or a function, their values and addresses are placed in the Stack section of the virtual memory. But by\ncreating the object or allocating the memory manually (Dynamic), their value and address are placed in the Heap\npart of the virtual memory. And finally, all of these are arranged in the RAM cells of the computer hardware in an\n.orderly manner\nos\nFree space\nA Static program data\n7 ¢ \\\no AEETTC TTT TTT CUEESEN ESTEE CATS ERTS UTTTEET TPE TE\n+) < AC BRINIG SORT it 0\n1 5 of ij\nt Program stack Program heap t\nHigher memory addr Lower memory addr\nOxFFFFFFFF 0x00000000\n:Table of contents\nStack memory «\nStack memory rules o\nStackoverflow problem o\nHeap memory rules o\nGarbage Collector / GC o\nMemory leak problem o",
          "char_count": 1182,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 2,
          "text": "Stack memory +\nStack memory is like a bunch of plates that are placed on top of each other... whenever you want to put a plate on the\npile of plates (push), you put it on top. In the same way, whenever you want to remove a plate from this category\n+ (Pop), you always remove the top plate... This category is called Stack or LIFO or LastIn-FirstOut\n{6}\nGS) Push\n{4} push) [5]\n{3} Push Y [4] [4]\n(2) Push 31 3]\nPushy fa FI 2]\ncn] L4] 4]\nVaal)\n(Pop 72t{s)\n[3] ( Pop 2)\n(4) ka\nThe Stack memory is located in the user-space part of the memory and is automatically managed by the CPU . This\nare accumulated inside this memory as LIFO (Last In First Out ) and are known as Frame . When a function is called\nand executed, the function, its parameters and all its internal non-static variables are placed in the Stack memory. By\nalling the new function, this function is placed on top of the previous function, and when the work of one of these\nfunctions is finished, that function is removed from the Stack memory along with all the relevant variables... and the\n.Work continues in the same way\n.Note: Another name for this type of memory allocation is Static memory allocation\nNote 2: The memory that is talked about in Memory Firewall software, this part of Stack and Heap is virtual memory\n.and not physical memory/RAM\nNote 3: In some programming languages such as Cpp, you can store the object in the Stack memory as well,\n.provided that it is inside the function and you do not use new",
          "char_count": 1484,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 3,
          "text": "Example\nfunction myFunction\n{ paraml , param2 ) // To Stack memory {\n_testVar = paraml + param2 ; // To stack memory return testVar ; // To Stack men\nImage example\nStack area\n= .\n=\n|\nmain ()\npeo\nint numer = 6; 5\nreturn myPOgc (30) Mx j\n}\n< Ss 4 myFunction ()\nmyFunction (param1) NY,\n.\nbool isEnable = FAL’\na | 2S\ni]\noe | ee",
          "char_count": 324,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 4,
          "text": "Sl\n\nStack memory rules +\n\n.In the virtual memory of the device, the Stack section is located .1\n\nIt is the place to store local variables (non-static), parameters of functions, return addresses of functions. (also .2\n\n.The stack size of the program is determined and allocated during compilation .3\n\n. Itis responsive to the problem of Stackoverflow .4\n\nIt is automatically managed by the CPU .5\n\nWhen the function is called, it is pushed into the Stack memory and when the function is finished, it is Popped .6\n.from the Stack memory\n\n-In terms of speed in occupying space, it is faster than Heap memory .7\n\n. language , classes and structures are stored in Stack memory if pointers are not used In C/Cpp .8\n\n.A stackoverflow error can occur during unwise allocation/ occupation or over capacity .9\n\nEach program has a main thread and each thread has a private Stack memory. So, as long as the thread is not .10\nclosed, the stack is still there. (closing here means exit and not terminate)\n\n.The Stack memory area of the program is attached to the thread of that program .11\n\nThe data in the Stack memory is sequentially placed on top of each other. (with the LIFO rule) .12\n\n.As stack memory consumption increases, less memory is left for heap .13\n\n“This type of memory is readable and writable .14\n\nAnd .15",
          "char_count": 1310,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 5,
          "text": "Frame\ner )\nStackoverflow problem «\nThe word stackoverflow literally means stack overflow. Among the reasons for the stackoverflow error, we can mention\nthe depth of nested and chain functions, bulky local variables, excessive stack size, destruction or corruption of part of\n.the memory, wrong use of Native API\n.Note: Stackoverflow problems are errors and not exceptions",
          "char_count": 372,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 6,
          "text": ":By running the following codes, you will get a better understanding of Stack memory and the Stackoverflow problem\n:PHP example\n<? php\nfunction myInfiniteRecursion () {\nmyInfiniteRecursion (); }\nImyInfiniteRecursion ();\nPHP Fatal error: Allowed memory size of 134217728 bytes(134MB) exhausted (tried to\nallocate 130968 bytes) in develop.php on line 7.\nIn this example, every time the function is called myInfiniteRecursion(), a Stack Frame is created for that\nfunction in the Stack memory, and this process continues as long as the PHP engine (memory_limit value in php.ini)\nallows... Finally, the allowed capacity of the engine's Stack memory PHP fills up and the script tries to access beyond\nthe capacity of the Stack memory, but the system does not allow it and stops the script with the\n.Stackoverflow/OxCOO000FD error",
          "char_count": 824,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 7,
          "text": ":Java example\nclass MyClass { private MyClass myCls = new MyClass ();\npublic static void main ( String [] args ) { new MyClass (); } } // javac MyClas\nException in thread \"main\" java.lang.StackOverflowError\n:C# example\nclass Program { static void Recursive () { Recursive (); }\nstatic void Main () { Recursive (); } }\nSystem. StackOverflow Exception: ‘Exception of type \" was thrown.\"",
          "char_count": 385,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 8,
          "text": "Example C\nint main () { int large [ 10000000 ] = { 0 }; return 0 ; }\nLinux: Segmentation fault.\n\nWindows: Unhandled exception in develop.exe: OxCO00000FD: Stackoverflow.\n;You can see that the words stack and overflow are common in all errors\nWindows tip: You can change the stack size of executable programs with the editbin program available in VS\nand nasm32\n.Note: The buffer overflow error is not related to the stack overflow error and these two are different from each other",
          "char_count": 480,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 9,
          "text": "Heap memory «\nHeap memory is located in the user-space part of the virtual memory and is manually managed by the programmer\n. In fact, data is stored in this memory by the programmer and must be emptied/destroyed by the\nprogrammer himself . The data stored in the Heap memory will not be lost when the call is completed and will remain\nuntil the programmer deletes this data from the Heap memory and even cause a memory leak or OutOfMemory.\n.Unless they are detected and destroyed by Garbage Collector\nNote: Here we mean data, objects that neware created by, variables that calloc/alloc/malloe are defined by,\n. and values that are moved by pointers\nNote 2: Another name for this type of memory allocation is Dynamic memory allocation\nint myfunction ()\n{ Heap memory\nint i;\nMyObject object = new MyObject();\nint #myP = (int#)malloc(10) ; on 7\nreturn 4 1)\n. @)\nHeap memory size is determined relative to the size of the total data (mentioned above) and the operating system\n.gives heap space to the program as much as it can and the user is not limited and has free space in RAM\nNote: In scripting languages, Heap memory management is done by the interpreter and in VMs by the runtime\n-language",
          "char_count": 1194,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 10,
          "text": "Heap memory rules «\n\n. The Heap section is stored in the device's virtual memory .1\n\nIt is the place to store allocated objects and data .2\n\n.Heap memory is responsible for memory leak and OutOfMemory problems .3\n\n-In terms of speed in occupying space, it is slower than Stack memory because it is managed by pointers .4\n\n-In Cpp, the data stored in the Heap memory is called by the pointer and can be reoccupied .5\n\n.OutOfMemory can occur when allocation/reservation of memory is unwise .6\n\n.The data in this space are randomly placed together .7\n\nHeap memory area depends on the program runtime. That is, it is occupied by starting the program and .8\n\nemptied by closing the program\n\n.Heap memory size of the program is reserved/allocated during runtime/starting the program .9\n\n.all threads use only one Heap memory (in most languages) .10\n\n.As the Heap memory consumption increases, less memory is left for the stack .11\n\n.When the program is closed, the allocated space in Heap is emptied by the program .12\n\n«And .13\n.Note: The problem of OutOfMemory belongs to the category of Errors and not Exceptions\n:PHP example\n\nSobjTest = new MyObject (); // MyObject to Heap memory\n\nJava and C# example\nExample C\nIn the first and second examples, by creating the MyObject object, we occupied some Heap memory, in fact, we put\nthe MyObject object into the Heap memory and not its variable. In the third example, we put 4 bytes with the\nvariable ptr into malloc Heap memory , which we have to free and destroy later by coding_. But this is only\nCpp/C and low-level languages, and not in Java, PHP and high-level languages! possible in\nSo what is the task of memory emptying in high-level languages? Because the memory has a maximum capacity and\n...only a small part of it is available to our program\nThe answer to this problem is Garbage Collector ! The garbage collection system is designed for high-level\nlanguages to cover this deficiency to some extent\n‘Windows tip: You can change the heap size of executable programs with the editbin program available in VS\nsand nasm32",
          "char_count": 2071,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 11,
          "text": "Garbage Collector /GC «\nGarbage collector system (abbreviated GC) is a feature that [usually] runs automatically and starts checking objects\nthat are no longer used by the program (they are dead) and no references point to them (unreferenced ) . Then he\ndestroys them\nThis is the same 2-phase process that every garbage collector performs to empty the memory\nGarbage collector usually comes into action when\n\n.The read/write operation is done in the system memory .1\n\nThe program/script is finished and wants to be closed .2\n\n.Manually mute the sound (such as IDEs Netbeans, Eclipse, Java language, PHPS.3, etc.) .3\nNote: Whether or not objects are collected by the garbage collector has nothing to do with the issue of scope\n‘As mentioned, VMs and interpreters store objects created by the running application in the heap memory. Like objects\nthat are made by new. When the Heap is slow, the VM or the interpreter starts the garbage collection system; Then\nthe garbage collection process is executed in a circular manner and deletes the objects and data that can be collected\n.from the Heap memory... which, of course, slows down the system a little until its operation is finished",
          "char_count": 1183,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 12,
          "text": "Memory leak «\nIn short, it means: the objects are no longer used by the program, they are referenced somewhere, and the Garbage\n.Collector is not able to release (destroy) them\nIn other words: keeping built objects busy (in use) for no reason , which is equal to being ignored by the garbage\ncollector . (According to GC law)\nMemory Leak\nr Unused Objects —tRase Objects >\n‘Note: memory leaks are critical bugs and app-vulnerabilities, which originate from programmer's wrong coding\n:Other definitions\na failure in a program to release discarded memory, causing impaired performance or failure.\nFailure to release unreachable memory, which can no longer be allocated again by any process during the\nexecution of allocating process.\nDefinition: Failure to release memory after allocation.\nNote 2: Garbage collector system is not an insurance to fix such vandalism and wrong coding, but it is only an\n.auxiliary system",
          "char_count": 916,
          "ocr_used": true,
          "original_char_count": 0
        },
        {
          "page": 13,
          "text": "Example of memory leak in PHP\nclass MyClass { // ... }\nSobj = new MyClass ;\nSobj -> self = Sobj ; // Memory leak by Sob}\n:Example of memory leak in C\nint main () { char * myVar = new char [ 5 J]; // Got 5 bytes in Heap memory. return q\n:Example of memory leak in Android Java\n@Override protected void onCreate ( Bundle state ) { super . onCreate ( state ); Te:\nbackgroundImage = getDrawable ( R . drawable . large bitmap );\nmyTextView . setBackgroundDrawable ( backgroundImage );\nsetContentView ( myTextView );\n// Memory leak }\nIn this Java example, memory leak occurs 2 times, 1- in the current state because the myTextView object variable is\nkept busy by the activity, 2- in the state that the orientation of the device changes, so the onCreate method is callea\nagai\n.Of course, the most sensitive memory leak is in systems that are facing limited resources or service type programs",
          "char_count": 885,
          "ocr_used": true,
          "original_char_count": 22
        },
        {
          "page": 14,
          "text": "Full description of stack, heap, data of each process in virtual memory \n \n \nPlease note, these are images from the website that is made for ease share. The original author of the \nabove post is Yusha Al-Ayoub. \n Link for the website: https://yousha.blog.ir/post/45",
          "char_count": 267,
          "ocr_used": false
        }
      ],
      "metadata": {
        "format": "PDF 1.7",
        "title": "",
        "author": "",
        "subject": "",
        "keywords": "",
        "creator": "",
        "producer": "",
        "creationDate": "",
        "modDate": "",
        "trapped": "",
        "encryption": null
      },
      "char_count": 12876,
      "word_count": 2260,
      "ocr_pages_count": 13,
      "error": null,
      "file_id": "1Vq261q4ERdhgAG9QQafR-_V3SsZyEWCm",
      "filename": "Heap and Stack memory written by Yusha Al-Ayoub.pdf",
      "filepath": "downloads/Heap and Stack memory written by Yusha Al-Ayoub.pdf",
      "download_link": "https://drive.google.com/uc?export=download&id=1Vq261q4ERdhgAG9QQafR-_V3SsZyEWCm"
    },
    {
      "success": true,
      "text": "written by Ben Meer\n\n5 Mental Models from\nCharlie Munger to Improve\nYour Decision-Making:\nwritten by Ben Meer\n1. Inversion \nThe best way to solve a problem? \nThink backwards.\n3 steps to apply inversion:\n1. Identify your goals in key life areas \n• Ex: Save time for self-improvement\n2. List anti-habits that could derail progress\n• Ex: Scrolling social media in bed\n3. Create an \"I Don't Do List\" \n• Ex: \"I don't scroll social media\" is more\npowerful than \"No scrolling\"\n2. Second-Order Thinking \nMost people think in the short-term, but\nwinners ask, “And then what:”\nSee beyond the obvious for better decisions.\nFirst-Order\nSecond-Order\n• Taking a high-paying\n job with long hours\n• Chasing a \"hot\" stock tip\n• Skipping breakfast \nto save time\n• Less time for skill\n development, potential burnout\n• Increased portfolio risk,\n missed long-term opportunities\n• Less energy, lower\n productivity throughout the day\n3. Circle of Competence \nNormalize I don’t know anything about that\nyet” as a successful answer.\n• Know what you know—and what you don't\n• Focus on your areas of expertise\n• Avoid overconfidence in unfamiliar areas\n• Play to strengths, delegate weaknesses\n• Expand your circle through learning\n4. Opportunity Cost \nEvery \"yes\" is a \"no\" to something else. \nTime and resources are finite. \nBefore committing, ask: \n\"What am I giving up by making this choice?\" \nIs this the best use of your time, money, and\nenergy?\nMake decisions understanding the trade-offs.\n5. Compounding \nSmall actions, consistently applied, yield\nmassive results. \n1% daily improvement = 37x better in a year. \nThis concept applies to more than just money:\n• Knowledge: Read 20 pages daily\n• Skills: Practice deliberately for 20 minutes\n• Relationships: 1+ kind interaction per day\nTL;DR Mental Models that will make you\nsmarter:\n1. Inversion\n2. Second-Order Thinking\n3. Circle of Competence\n4. Opportunity Cost\n5. Compounding\nIn the words of Munger:\n“Spend each day trying to be a little wiser\nthan you were when you woke up.\"\nThanks for reading!\nIf this post hit for you, please share it so\nmore people can know about this.\nAll systems go,\nBen\nP.S.\n\nIf you enjoyed this post, you'll love\nmy free newsletter—System Sunday.\nJoin 270,000+ subscribers in\nthe bio link.\n\nJG svsteMsuNoay\nMinimalism\n[1 in, 1 Out Rule]\n\nSUPERHUMAN SCORE: 8.60",
      "page_count": 9,
      "pages": [
        {
          "page": 1,
          "text": "written by Ben Meer\n\n5 Mental Models from\nCharlie Munger to Improve\nYour Decision-Making:\nwritten by Ben Meer",
          "char_count": 110,
          "ocr_used": true,
          "original_char_count": 20
        },
        {
          "page": 2,
          "text": "1. Inversion \nThe best way to solve a problem? \nThink backwards.\n3 steps to apply inversion:\n1. Identify your goals in key life areas \n• Ex: Save time for self-improvement\n2. List anti-habits that could derail progress\n• Ex: Scrolling social media in bed\n3. Create an \"I Don't Do List\" \n• Ex: \"I don't scroll social media\" is more\npowerful than \"No scrolling\"",
          "char_count": 360,
          "ocr_used": false
        },
        {
          "page": 3,
          "text": "2. Second-Order Thinking \nMost people think in the short-term, but\nwinners ask, “And then what:”\nSee beyond the obvious for better decisions.\nFirst-Order\nSecond-Order\n• Taking a high-paying\n job with long hours\n• Chasing a \"hot\" stock tip\n• Skipping breakfast \nto save time\n• Less time for skill\n development, potential burnout\n• Increased portfolio risk,\n missed long-term opportunities\n• Less energy, lower\n productivity throughout the day",
          "char_count": 442,
          "ocr_used": false
        },
        {
          "page": 4,
          "text": "3. Circle of Competence \nNormalize I don’t know anything about that\nyet” as a successful answer.\n• Know what you know—and what you don't\n• Focus on your areas of expertise\n• Avoid overconfidence in unfamiliar areas\n• Play to strengths, delegate weaknesses\n• Expand your circle through learning",
          "char_count": 294,
          "ocr_used": false
        },
        {
          "page": 5,
          "text": "4. Opportunity Cost \nEvery \"yes\" is a \"no\" to something else. \nTime and resources are finite. \nBefore committing, ask: \n\"What am I giving up by making this choice?\" \nIs this the best use of your time, money, and\nenergy?\nMake decisions understanding the trade-offs.",
          "char_count": 265,
          "ocr_used": false
        },
        {
          "page": 6,
          "text": "5. Compounding \nSmall actions, consistently applied, yield\nmassive results. \n1% daily improvement = 37x better in a year. \nThis concept applies to more than just money:\n• Knowledge: Read 20 pages daily\n• Skills: Practice deliberately for 20 minutes\n• Relationships: 1+ kind interaction per day",
          "char_count": 295,
          "ocr_used": false
        },
        {
          "page": 7,
          "text": "TL;DR Mental Models that will make you\nsmarter:\n1. Inversion\n2. Second-Order Thinking\n3. Circle of Competence\n4. Opportunity Cost\n5. Compounding\nIn the words of Munger:\n“Spend each day trying to be a little wiser\nthan you were when you woke up.\"",
          "char_count": 246,
          "ocr_used": false
        },
        {
          "page": 8,
          "text": "Thanks for reading!\nIf this post hit for you, please share it so\nmore people can know about this.\nAll systems go,\nBen",
          "char_count": 118,
          "ocr_used": false
        },
        {
          "page": 9,
          "text": "P.S.\n\nIf you enjoyed this post, you'll love\nmy free newsletter—System Sunday.\nJoin 270,000+ subscribers in\nthe bio link.\n\nJG svsteMsuNoay\nMinimalism\n[1 in, 1 Out Rule]\n\nSUPERHUMAN SCORE: 8.60",
          "char_count": 192,
          "ocr_used": true,
          "original_char_count": 0
        }
      ],
      "metadata": {
        "format": "PDF 1.4",
        "title": "",
        "author": "",
        "subject": "",
        "keywords": "",
        "creator": "",
        "producer": "",
        "creationDate": "",
        "modDate": "",
        "trapped": "",
        "encryption": null
      },
      "char_count": 2321,
      "word_count": 384,
      "ocr_pages_count": 2,
      "error": null,
      "file_id": "15_RsSWF559ItgYSnlVFSamRS1p5wLGES",
      "filename": "Mental Models To Improve Your Thinking_.pdf",
      "filepath": "downloads/Mental Models To Improve Your Thinking_.pdf",
      "download_link": "https://drive.google.com/uc?export=download&id=15_RsSWF559ItgYSnlVFSamRS1p5wLGES"
    },
    {
      "success": true,
      "text": "Information security, cybersecurity \nand privacy protection — Information \nsecurity management systems — \nRequirements\nSécurité de l'information, cybersécurité et protection de la vie \nprivée — Systèmes de management de la sécurité de l'information — \nExigences\nINTERNATIONAL \nSTANDARD\nISO/IEC \n27001\nThird edition \n2022-10\nReference number \nISO/IEC 27001:2022(E)\n© ISO/IEC 2022\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---\nii\nISO/IEC 27001:2022(E)\nCOPYRIGHT PROTECTED DOCUMENT\n©  ISO/IEC 2022\nAll rights reserved. Unless otherwise specified, or required in the context of its implementation, no part of this publication may \nbe reproduced or utilized otherwise in any form or by any means, electronic or mechanical, including photocopying, or posting on \nthe internet or an intranet, without prior written permission. Permission can be requested from either ISO at the address below \nor ISO’s member body in the country of the requester.\nISO copyright office\nCP 401 • Ch. de Blandonnet 8\nCH-1214 Vernier, Geneva\nPhone: +41 22 749 01 11\nEmail: copyright@iso.org\nWebsite: www.iso.org\nPublished in Switzerland\n\t\n﻿\b\n© ISO/IEC 2022 – All rights reserved\n\b\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---\nISO/IEC 27001:2022(E)\nForeword........................................................................................................................................................................................................................................iv\nIntroduction..................................................................................................................................................................................................................................v\n1\t\n​Scope.................................................................................................................................................................................................................................1\n2\t\n​Normative references......................................................................................................................................................................................1\n3 \n​Terms and definitions.....................................................................................................................................................................................1\n4\t\n​Context of the organization......................................................................................................................................................................1\n4.1\t\n​Understanding the organization and its context......................................................................................................1\n4.2\t\n​Understanding the needs and expectations of interested parties...........................................................1\n4.3\t\n​Determining the scope of the information security management system........................................2\n4.4\t\n​Information security management system................................................................................................................... 2\n5\t\n​Leadership..................................................................................................................................................................................................................2\n5.1\t\n​Leadership and commitment..................................................................................................................................................... 2\n5.2\t\n​Policy................................................................................................................................................................................................................ 3\n5.3\t\n​Organizational roles, responsibilities and authorities........................................................................................3\n6\t\n​Planning.........................................................................................................................................................................................................................3\n6.1\t\n​Actions to address risks and opportunities.................................................................................................................. 3\n6.1.1\t\n​General.........................................................................................................................................................................................3\n6.1.2\t\n​Information security risk assessment.............................................................................................................4\n6.1.3\t\n​Information security risk treatment................................................................................................................4\n6.2\t\n​Information security objectives and planning to achieve them.................................................................5\n7\t\n​Support...........................................................................................................................................................................................................................6\n7.1\t\n​Resources..................................................................................................................................................................................................... 6\n7.2\t\n​Competence................................................................................................................................................................................................ 6\n7.3\t\n​Awareness................................................................................................................................................................................................... 6\n7.4\t\n​Communication......................................................................................................................................................................................6\n7.5\t\n​Documented information.............................................................................................................................................................. 6\n7.5.1\t\n​General.........................................................................................................................................................................................6\n7.5.2\t\n​Creating and updating................................................................................................................................................... 7\n7.5.3\t\n​Control of documented information..................................................................................................................7\n8\t\n​Operation.....................................................................................................................................................................................................................7\n8.1\t\n​Operational planning and control.......................................................................................................................................... 7\n8.2\t\n​Information security risk assessment............................................................................................................................... 8\n8.3\t\n​Information security risk treatment................................................................................................................................... 8\n9\t\n​Performance evaluation...............................................................................................................................................................................8\n9.1\t\n​Monitoring, measurement, analysis and evaluation.............................................................................................8\n9.2\t\n​Internal audit............................................................................................................................................................................................8\n9.2.1\t\nGeneral.........................................................................................................................................................................................8\n9.2.2\t\nInternal audit programme......................................................................................................................................... 9\n9.3\t\n​Management review...........................................................................................................................................................................9\n9.3.1\t\nGeneral.........................................................................................................................................................................................9\n9.3.2\t\nManagement review inputs....................................................................................................................................... 9\n9.3.3\t\nManagement review results..................................................................................................................................... 9\n10\t\n​Improvement.........................................................................................................................................................................................................10\n10.1\t\n​Continual improvement...............................................................................................................................................................10\n10.2\t\n​Nonconformity and corrective action.............................................................................................................................10\nAnnex A (normative) Information security controls reference...........................................................................................11\nBibliography.............................................................................................................................................................................................................................19\niii\n© ISO/IEC 2022 – All rights reserved\t\n﻿\nContents\b\nPage\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---\nISO/IEC 27001:2022(E)\nForeword\nISO (the International Organization for Standardization) and IEC (the International Electrotechnical \nCommission) form the specialized system for worldwide standardization. National bodies that are \nmembers of ISO or IEC participate in the development of International Standards through technical \ncommittees established by the respective organization to deal with particular fields of technical \nactivity. ISO and IEC technical committees collaborate in fields of mutual interest. Other international \norganizations, governmental and non-governmental, in liaison with ISO and IEC, also take part in the \nwork.\nThe procedures used to develop this document and those intended for its further maintenance \nare described in the ISO/IEC Directives, Part 1. In particular, the different approval criteria \nneeded for the different types of document should be noted. This document was drafted in \naccordance with the editorial rules of the ISO/IEC Directives, Part 2 (see www.iso.org/directives or \nwww.iec.ch/members_experts/refdocs).\nAttention is drawn to the possibility that some of the elements of this document may be the subject \nof patent rights. ISO and IEC shall not be held responsible for identifying any or all such patent \nrights. Details of any patent rights identified during the development of the document will be in the \nIntroduction and/or on the ISO list of patent declarations received (see www.iso.org/patents) or the IEC \nlist of patent declarations received (see https://patents.iec.ch).\nAny trade name used in this document is information given for the convenience of users and does not \nconstitute an endorsement.\nFor an explanation of the voluntary nature of standards, the meaning of ISO specific terms and \nexpressions related to conformity assessment, as well as information about ISO's adherence to \nthe World Trade Organization (WTO) principles in the Technical Barriers to Trade (TBT) see \nwww.iso.org/iso/foreword.html. In the IEC, see www.iec.ch/understanding-standards.\nThis document was prepared by Joint Technical Committee ISO/IEC  JTC  1, Information Technology, \nSubcommittee SC 27, Information security, cybersecurity and privacy protection.\nThis third edition cancels and replaces the second edition (ISO/IEC 27001:2013), which has been \ntechnically revised. It also incorporates the Technical Corrigenda ISO/IEC 27001:2013/Cor 1:2014 and \nISO/IEC 27001:2013/Cor 2:2015.\nThe main changes are as follows:\n—\t\nthe text has been aligned with the harmonized structure for management system standards \nand ISO/IEC 27002:2022.\nAny feedback or questions on this document should be directed to the user’s national standards \nbody. A complete listing of these bodies can be found at www.iso.org/members.html and \nwww.iec.ch/national-committees.\niv\n\t\n﻿\b\n© ISO/IEC 2022 – All rights reserved\n\b\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---\nISO/IEC 27001:2022(E)\nIntroduction\n0.1   General\nThis document has been prepared to provide requirements for establishing, implementing, maintaining \nand continually improving an information security management system. The adoption of an \ninformation security management system is a strategic decision for an organization. The establishment \nand implementation of an organization’s information security management system is influenced by the \norganization’s needs and objectives, security requirements, the organizational processes used and the \nsize and structure of the organization. All of these influencing factors are expected to change over time.\nThe information security management system preserves the confidentiality, integrity and availability \nof information by applying a risk management process and gives confidence to interested parties that \nrisks are adequately managed.\nIt is important that the information security management system is part of and integrated with the \norganization’s processes and overall management structure and that information security is considered \nin the design of processes, information systems, and controls. It is expected that an information security \nmanagement system implementation will be scaled in accordance with the needs of the organization.\nThis document can be used by internal and external parties to assess the organization's ability to meet \nthe organization’s own information security requirements.\nThe order in which requirements are presented in this document does not reflect their importance \nor imply the order in which they are to be implemented. The list items are enumerated for reference \npurpose only.\nISO/IEC  27000 describes the overview and the vocabulary of information security management \nsystems, referencing the information security management system family of standards (including \nISO/IEC 27003[2], ISO/IEC 27004[3] and ISO/IEC 27005[4]), with related terms and definitions.\n \n0.2   Compatibility with other management system standards\nThis document applies the high-level structure, identical sub-clause titles, identical text, common terms, \nand core definitions defined in Annex SL of ISO/IEC Directives, Part 1, Consolidated ISO Supplement, \nand therefore maintains compatibility with other management system standards that have adopted the \nAnnex SL.\nThis common approach defined in the Annex SL will be useful for those organizations that choose to \noperate a single management system that meets the requirements of two or more management system \nstandards.\nv\n© ISO/IEC 2022 – All rights reserved\t\n﻿\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---\nINTERNATIONAL STANDARD\nISO/IEC 27001:2022(E)\nInformation security, cybersecurity and privacy \nprotection — Information security management systems \n— Requirements\n1\t ​Scope\nThis document specifies the requirements for establishing, implementing, maintaining and continually \nimproving an information security management system within the context of the organization. This \ndocument also includes requirements for the assessment and treatment of information security risks \ntailored to the needs of the organization. The requirements set out in this document are generic and are \nintended to be applicable to all organizations, regardless of type, size or nature. Excluding any of the \nrequirements specified in Clauses 4 to 10 is not acceptable when an organization claims conformity to \nthis document.\n2\t ​Normative references\nThe following documents are referred to in the text in such a way that some or all of their content \nconstitutes requirements of this document. For dated references, only the edition cited applies. For \nundated references, the latest edition of the referenced document (including any amendments) applies.\nISO/IEC  27000, Information technology — Security techniques — Information security management \nsystems — Overview and vocabulary\n3 ​Terms and definitions\nFor the purposes of this document, the terms and definitions given in ISO/IEC 27000 apply.\nISO and IEC maintain terminology databases for use in standardization at the following addresses:\n—\t ISO Online browsing platform: available at https://​www​.iso​.org/​obp\n—\t IEC Electropedia: available at https://​www​.electropedia​.org/​\n4\t ​Context of the organization\n4.1\t ​Understanding the organization and its context\nThe organization shall determine external and internal issues that are relevant to its purpose and that \naffect its ability to achieve the intended outcome(s) of its information security management system.\nNOTE\t\nDetermining these issues refers to establishing the external and internal context of the organization \nconsidered in Clause 5.4.1 of ISO 31000:2018[5].\n4.2\t ​Understanding the needs and expectations of interested parties\nThe organization shall determine:\na)\t interested parties that are relevant to the information security management system; \nb)\t the relevant requirements of these interested parties; \nc)\t\nwhich of these requirements will be addressed through the information security management \nsystem. \n1\n© ISO/IEC 2022 – All rights reserved\t\n﻿\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---\nISO/IEC 27001:2022(E)\nNOTE\t\nThe requirements of interested parties can include legal and regulatory requirements and contractual \nobligations.\n4.3\t ​Determining the scope of the information security management system\nThe organization shall determine the boundaries and applicability of the information security \nmanagement system to establish its scope.\nWhen determining this scope, the organization shall consider:\na)\t the external and internal issues referred to in 4.1;\nb)\t the requirements referred to in 4.2; \nc)\t\ninterfaces and dependencies between activities performed by the organization, and those that are \nperformed by other organizations.\nThe scope shall be available as documented information.\n4.4\t ​Information security management system\nThe organization shall establish, implement, maintain and continually improve an information security \nmanagement system, including the processes needed and their interactions, in accordance with the \nrequirements of this document.\n5\t ​Leadership\n5.1\t ​Leadership and commitment\nTop management shall demonstrate leadership and commitment with respect to the information \nsecurity management system by:\na)\t ensuring the information security policy and the information security objectives are established \nand are compatible with the strategic direction of the organization;\nb)\t ensuring the integration of the information security management system requirements into the \norganization’s processes;\nc)\t\nensuring that the resources needed for the information security management system are available;\nd)\t communicating the importance of effective information security management and of conforming \nto the information security management system requirements;\ne)\t ensuring that the information security management system achieves its intended outcome(s);\nf)\t\ndirecting and supporting persons to contribute to the effectiveness of the information security \nmanagement system;\ng)\t promoting continual improvement; and\nh)\t supporting other relevant management roles to demonstrate their leadership as it applies to their \nareas of responsibility.\nNOTE\t\nReference to “business” in this document can be interpreted broadly to mean those activities that are \ncore to the purposes of the organization’s existence.\n\t\n﻿\b\n© ISO/IEC 2022 – All rights reserved\n\b\n﻿\n2\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---\nISO/IEC 27001:2022(E)\n5.2\t ​Policy\nTop management shall establish an information security policy that:\na)\t is appropriate to the purpose of the organization;\nb)\t includes information security objectives (see 6.2) or provides the framework for setting information \nsecurity objectives;\nc)\t\nincludes a commitment to satisfy applicable requirements related to information security; \nd)\t includes a commitment to continual improvement of the information security management system.\nThe information security policy shall:\ne)\t be available as documented information;\nf)\t\nbe communicated within the organization; \ng)\t be available to interested parties, as appropriate.\n5.3\t ​Organizational roles, responsibilities and authorities\nTop management shall ensure that the responsibilities and authorities for roles relevant to information \nsecurity are assigned and communicated within the organization.\nTop management shall assign the responsibility and authority for:\na)\t ensuring that the information security management system conforms to the requirements of this \ndocument; \nb)\t reporting on the performance of the information security management system to top management.\nNOTE\t\nTop management can also assign responsibilities and authorities for reporting performance of the \ninformation security management system within the organization.\n6\t ​Planning\n6.1\t ​Actions to address risks and opportunities\n6.1.1\t\n​General\nWhen planning for the information security management system, the organization shall consider \nthe issues referred to in 4.1 and the requirements referred to in 4.2 and determine the risks and \nopportunities that need to be addressed to:\na)\t ensure the information security management system can achieve its intended outcome(s);\nb)\t prevent, or reduce, undesired effects; \nc)\t\nachieve continual improvement.\nThe organization shall plan:\nd)\t actions to address these risks and opportunities; and\ne)\t how to\n1)\t integrate and implement the actions into its information security management system \nprocesses; and\n2)\t evaluate the effectiveness of these actions.\n© ISO/IEC 2022 – All rights reserved\t\n﻿\n﻿\n3\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---\nISO/IEC 27001:2022(E)\n6.1.2\t\n​Information security risk assessment\nThe organization shall define and apply an information security risk assessment process that:\na)\t establishes and maintains information security risk criteria that include:\n1)\t the risk acceptance criteria; and\n2)\t criteria for performing information security risk assessments;\nb)\t ensures that repeated information security risk assessments produce consistent, valid and \ncomparable results;\nc)\t\nidentifies the information security risks:\n1)\t apply the information security risk assessment process to identify risks associated with \nthe loss of confidentiality, integrity and availability for information within the scope of the \ninformation security management system; and\n2)\t identify the risk owners;\nd)\t analyses the information security risks:\n1)\t assess the potential consequences that would result if the risks identified in 6.1.2 c) 1) were to \nmaterialize;\n2)\t assess the realistic likelihood of the occurrence of the risks identified in 6.1.2 c) 1); and\n3)\t determine the levels of risk;\ne)\t evaluates the information security risks:\n1)\t compare the results of risk analysis with the risk criteria established in 6.1.2 a); and\n2)\t prioritize the analysed risks for risk treatment.\nThe organization shall retain documented information about the information security risk assessment \nprocess.\n6.1.3\t\n​Information security risk treatment\nThe organization shall define and apply an information security risk treatment process to:\na)\t select appropriate information security risk treatment options, taking account of the risk \nassessment results;\nb)\t determine all controls that are necessary to implement the information security risk treatment \noption(s) chosen;\nNOTE 1\t\nOrganizations can design controls as required, or identify them from any source.\nc)\t\ncompare the controls determined in 6.1.3 b) above with those in Annex  A and verify that no \nnecessary controls have been omitted;\nNOTE 2\t\nAnnex  A contains a list of possible information security controls. Users of this document are \ndirected to Annex A to ensure that no necessary information security controls are overlooked.\nNOTE 3\t\nThe information security controls listed in Annex A are not exhaustive and additional information \nsecurity controls can be included if needed.\nd)\t produce a Statement of Applicability that contains:\n—\t the necessary controls (see 6.1.3 b) and c));\n\t\n﻿\b\n© ISO/IEC 2022 – All rights reserved\n\b\n﻿\n4\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---\nISO/IEC 27001:2022(E)\n—\t justification for their inclusion;\n—\t whether the necessary controls are implemented or not; and\n—\t the justification for excluding any of the Annex A controls.\ne)\t formulate an information security risk treatment plan; and\nf)\t\nobtain risk owners’ approval of the information security risk treatment plan and acceptance of the \nresidual information security risks.\nThe organization shall retain documented information about the information security risk treatment \nprocess.\nNOTE 4\t\nThe information security risk assessment and treatment process in this document aligns with the \nprinciples and generic guidelines provided in ISO 31000[5].\n6.2\t ​Information security objectives and planning to achieve them\nThe organization shall establish information security objectives at relevant functions and levels.\nThe information security objectives shall:\na)\t be consistent with the information security policy;\nb)\t be measurable (if practicable);\nc)\t\ntake into account applicable information security requirements, and results from risk assessment \nand risk treatment;\nd)\t be monitored;\ne)\t be communicated; \nf)\t\nbe updated as appropriate;\ng)\t be available as documented information.\nThe organization shall retain documented information on the information security objectives.\nWhen planning how to achieve its information security objectives, the organization shall determine:\nh)\t what will be done;\ni)\t\nwhat resources will be required;\nj)\t\nwho will be responsible;\nk)\t when it will be completed; and\nl)\t\nhow the results will be evaluated.\n6.3\t\nPlanning of changes\nWhen the organization determines the need for changes to the information security management \nsystem, the changes shall be carried out in a planned manner.\n© ISO/IEC 2022 – All rights reserved\t\n﻿\n﻿\n5\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---\nISO/IEC 27001:2022(E)\n7\t ​Support\n7.1\t ​Resources\nThe organization shall determine and provide the resources needed for the establishment, \nimplementation, maintenance and continual improvement of the information security management \nsystem.\n7.2\t ​Competence\nThe organization shall:\na)\t determine the necessary competence of person(s) doing work under its control that affects its \ninformation security performance;\nb)\t ensure that these persons are competent on the basis of appropriate education, training, or \nexperience;\nc)\t\nwhere applicable, take actions to acquire the necessary competence, and evaluate the effectiveness \nof the actions taken; and\nd)\t retain appropriate documented information as evidence of competence.\nNOTE\t\nApplicable actions can include, for example: the provision of training to, the mentoring of, or the re-\nassignment of current employees; or the hiring or contracting of competent persons.\n7.3\t ​Awareness\nPersons doing work under the organization’s control shall be aware of:\na)\t the information security policy;\nb)\t their contribution to the effectiveness of the information security management system, including \nthe benefits of improved information security performance; and\nc)\t\nthe implications of not conforming with the information security management system \nrequirements.\n7.4\t ​Communication\nThe organization shall determine the need for internal and external communications relevant to the \ninformation security management system including:\na)\t on what to communicate;\nb)\t when to communicate;\nc)\t\nwith whom to communicate;\nd)\t how to communicate.\n7.5\t ​Documented information\n7.5.1\t\n​General\nThe organization’s information security management system shall include:\na)\t documented information required by this document; and\n\t\n﻿\b\n© ISO/IEC 2022 – All rights reserved\n\b\n﻿\n6\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---\nISO/IEC 27001:2022(E)\nb)\t documented information determined by the organization as being necessary for the effectiveness \nof the information security management system.\nNOTE\t\nThe extent of documented information for an information security management system can differ \nfrom one organization to another due to:\n1)\t\nthe size of organization and its type of activities, processes, products and services;\n2)\t\nthe complexity of processes and their interactions; and\n3)\t\nthe competence of persons.\n7.5.2\t\n​Creating and updating\nWhen creating and updating documented information the organization shall ensure appropriate:\na)\t identification and description (e.g. a title, date, author, or reference number);\nb)\t format (e.g. language, software version, graphics) and media (e.g. paper, electronic); and\nc)\t\nreview and approval for suitability and adequacy.\n7.5.3\t\n​Control of documented information\nDocumented information required by the information security management system and by this \ndocument shall be controlled to ensure:\na)\t it is available and suitable for use, where and when it is needed; and\nb)\t it is adequately protected (e.g. from loss of confidentiality, improper use, or loss of integrity).\nFor the control of documented information, the organization shall address the following activities, as \napplicable:\nc)\t\ndistribution, access, retrieval and use;\nd)\t storage and preservation, including the preservation of legibility;\ne)\t control of changes (e.g. version control); and\nf)\t\nretention and disposition.\nDocumented information of external origin, determined by the organization to be necessary for \nthe planning and operation of the information security management system, shall be identified as \nappropriate, and controlled.\nNOTE\t\nAccess can imply a decision regarding the permission to view the documented information only, or \nthe permission and authority to view and change the documented information, etc.\n8\t ​Operation\n8.1\t ​Operational planning and control\nThe organization shall plan, implement and control the processes needed to meet requirements, and to \nimplement the actions determined in Clause 6, by: \n—\t establishing criteria for the processes; \n—\t implementing control of the processes in accordance with the criteria. \nDocumented information shall be available to the extent necessary to have confidence that the \nprocesses have been carried out as planned. \n© ISO/IEC 2022 – All rights reserved\t\n﻿\n﻿\n7\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---\nISO/IEC 27001:2022(E)\nThe organization shall control planned changes and review the consequences of unintended changes, \ntaking action to mitigate any adverse effects, as necessary. \nThe organization shall ensure that externally provided processes, products or services that are relevant \nto the information security management system are controlled.\n8.2\t ​Information security risk assessment\nThe organization shall perform information security risk assessments at planned intervals or when \nsignificant changes are proposed or occur, taking account of the criteria established in 6.1.2 a).\nThe organization shall retain documented information of the results of the information security risk \nassessments.\n8.3\t ​Information security risk treatment\nThe organization shall implement the information security risk treatment plan.\nThe organization shall retain documented information of the results of the information security risk \ntreatment.\n9\t ​Performance evaluation\n9.1\t ​Monitoring, measurement, analysis and evaluation\nThe organization shall determine:\na)\t what needs to be monitored and measured, including information security processes and controls;\nb)\t the methods for monitoring, measurement, analysis and evaluation, as applicable, to ensure \nvalid results. The methods selected should produce comparable and reproducible results to be \nconsidered valid;\nc)\t\nwhen the monitoring and measuring shall be performed;\nd)\t who shall monitor and measure;\ne)\t when the results from monitoring and measurement shall be analysed and evaluated; \nf)\t\nwho shall analyse and evaluate these results.\nDocumented information shall be available as evidence of the results.\nThe organization shall evaluate the information security performance and the effectiveness of the \ninformation security management system.\n9.2\t ​Internal audit\n9.2.1\t\nGeneral\nThe organization shall conduct internal audits at planned intervals to provide information on whether \nthe information security management system:\na)\t conforms to\n1)\t the organization’s own requirements for its information security management system; \n\t\n﻿\b\n© ISO/IEC 2022 – All rights reserved\n\b\n﻿\n8\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---\nISO/IEC 27001:2022(E)\n2)\t the requirements of this document;\nb)\t is effectively implemented and maintained.\n9.2.2\t\nInternal audit programme\nThe organization shall plan, establish, implement and maintain an audit programme(s), including the \nfrequency, methods, responsibilities, planning requirements and reporting. \nWhen establishing the internal audit programme(s), the organization shall consider the importance of \nthe processes concerned and the results of previous audits. \nThe organization shall: \na)\t define the audit criteria and scope for each audit;\nb)\t select auditors and conduct audits that ensure objectivity and the impartiality of the audit process;\nc)\t\nensure that the results of the audits are reported to relevant management; \nDocumented information shall be available as evidence of the implementation of the audit programme(s) \nand the audit results.\n9.3\t ​Management review\n9.3.1\t\nGeneral\nTop management shall review the organization's information security management system at planned \nintervals to ensure its continuing suitability, adequacy and effectiveness.\n9.3.2\t\nManagement review inputs\nThe management review shall include consideration of:\na)\t the status of actions from previous management reviews;\nb)\t changes in external and internal issues that are relevant to the information security management \nsystem;\nc)\t\nchanges in needs and expectations of interested parties that are relevant to the information \nsecurity management system;\nd)\t feedback on the information security performance, including trends in:\n1)\t nonconformities and corrective actions;\n2)\t monitoring and measurement results;\n3)\t audit results; \n4)\t fulfilment of information security objectives;\ne)\t feedback from interested parties;\nf)\t\nresults of risk assessment and status of risk treatment plan; \ng)\t opportunities for continual improvement.\n9.3.3\t\nManagement review results\nThe results of the management review shall include decisions related to continual improvement \nopportunities and any needs for changes to the information security management system.\n© ISO/IEC 2022 – All rights reserved\t\n﻿\n﻿\n9\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---\nISO/IEC 27001:2022(E)\nDocumented information shall be available as evidence of the results of management reviews.\n10\t​Improvement\n10.1\t​Continual improvement\nThe organization shall continually improve the suitability, adequacy and effectiveness of the information \nsecurity management system.\n10.2\t​Nonconformity and corrective action\nWhen a nonconformity occurs, the organization shall:\na)\t react to the nonconformity, and as applicable:\n1)\t take action to control and correct it; \n2)\t deal with the consequences;\nb)\t evaluate the need for action to eliminate the causes of nonconformity, in order that it does not recur \nor occur elsewhere, by:\n1)\t reviewing the nonconformity;\n2)\t determining the causes of the nonconformity; and\n3)\t determining if similar nonconformities exist, or could potentially occur;\nc)\t\nimplement any action needed;\nd)\t review the effectiveness of any corrective action taken; and\ne)\t make changes to the information security management system, if necessary.\nCorrective actions shall be appropriate to the effects of the nonconformities encountered.\nDocumented information shall be available as evidence of: \nf)\t\nthe nature of the nonconformities and any subsequent actions taken, \ng)\t the results of any corrective action.\n\t\n﻿\b\n© ISO/IEC 2022 – All rights reserved\n\b\n﻿\n10\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---\nISO/IEC 27001:2022(E)\nAnnex A \n(normative) \n \nInformation security controls reference\nThe information security controls listed in Table A.1 are directly derived from and aligned with those \nlisted in ISO/IEC 27002:2022[1], Clauses 5 to 8, and shall be used in context with 6.1.3.\nTable A.1 — Information security controls\n5\nOrganizational controls\n5.1\nPolicies for information secu-\nrity\nControl\nInformation security policy and topic-specific policies shall be de-\nfined, approved by management, published, communicated to and \nacknowledged by relevant personnel and relevant interested parties, \nand reviewed at planned intervals and if significant changes occur.\n5.2\nInformation security roles and \nresponsibilities\nControl\nInformation security roles and responsibilities shall be defined and \nallocated according to the organization needs.\n5.3\nSegregation of duties\nControl\nConflicting duties and conflicting areas of responsibility shall be seg-\nregated.\n5.4\nManagement responsibilities\nControl\nManagement shall require all personnel to apply information security \nin accordance with the established information security policy, top-\nic-specific policies and procedures of the organization.\n5.5\nContact with authorities\nControl\nThe organization shall establish and maintain contact with relevant \nauthorities.\n5.6\nContact with special interest \ngroups\nControl\nThe organization shall establish and maintain contact with special \ninterest groups or other specialist security forums and professional \nassociations.\n5.7\nThreat intelligence\nControl\nInformation relating to information security threats shall be collected \nand analysed to produce threat intelligence.\n5.8\nInformation security in project \nmanagement\nControl\nInformation security shall be integrated into project management.\n5.9\nInventory of information and \nother associated assets\nControl\nAn inventory of information and other associated assets, including \nowners, shall be developed and maintained.\n5.10\nAcceptable use of information \nand other associated assets\nControl\nRules for the acceptable use and procedures for handling information and \nother associated assets shall be identified, documented and implemented.\n5.11\nReturn of assets\nControl\nPersonnel and other interested parties as appropriate shall return all \nthe organization’s assets in their possession upon change or termination \nof their employment, contract or agreement.\n© ISO/IEC 2022 – All rights reserved\t\n﻿\n﻿\n11\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---\nISO/IEC 27001:2022(E)\n5.12\nClassification of information\nControl\nInformation shall be classified according to the information security \nneeds of the organization based on confidentiality, integrity, availability \nand relevant interested party requirements.\n5.13\nLabelling of information\nControl\nAn appropriate set of procedures for information labelling shall be \ndeveloped and implemented in accordance with the information clas-\nsification scheme adopted by the organization.\n5.14\nInformation transfer\nControl\nInformation transfer rules, procedures, or agreements shall be in place \nfor all types of transfer facilities within the organization and between \nthe organization and other parties.\n5.15\nAccess control\nControl\nRules to control physical and logical access to information and other \nassociated assets shall be established and implemented based on busi-\nness and information security requirements.\n5.16\nIdentity management\nControl\nThe full life cycle of identities shall be managed.\n5.17\nAuthentication information\nControl\nAllocation and management of authentication information shall be \ncontrolled by a management process, including advising personnel on \nappropriate handling of authentication information.\n5.18\nAccess rights\nControl\nAccess rights to information and other associated assets shall be \nprovisioned, reviewed, modified and removed in accordance with the \norganization’s topic-specific policy on and rules for access control.\n5.19\nInformation security in supplier \nrelationships\nControl\nProcesses and procedures shall be defined and implemented to manage \nthe information security risks associated with the use of supplier’s \nproducts or services.\n5.20\nAddressing information security \nwithin supplier agreements\nControl\nRelevant information security requirements shall be established and \nagreed with each supplier based on the type of supplier relationship.\n5.21\nManaging information security \nin the information and commu-\nnication technology (ICT) supply \nchain\nControl\nProcesses and procedures shall be defined and implemented to manage \nthe information security risks associated with the ICT products and \nservices supply chain.\n5.22\nMonitoring, review and change \nmanagement of supplier services\nControl\nThe organization shall regularly monitor, review, evaluate and manage \nchange in supplier information security practices and service delivery.\n5.23\nInformation security for use of \ncloud services\nControl\nProcesses for acquisition, use, management and exit from cloud services \nshall be established in accordance with the organization’s information \nsecurity requirements.\n5.24\nInformation security incident \nmanagement planning and prepa-\nration\nControl\nThe organization shall plan and prepare for managing information secu-\nrity incidents by defining, establishing and communicating information \nsecurity incident management processes, roles and responsibilities.\nTable A.1 (continued)\nTable A.1 (continued)\n\t\n﻿\b\n© ISO/IEC 2022 – All rights reserved\n\b\n﻿\n12\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---\nISO/IEC 27001:2022(E)\n5.25\nAssessment and decision on in-\nformation security events\nControl\nThe organization shall assess information security events and decide if \nthey are to be categorized as information security incidents.\n5.26\nResponse to information security \nincidents\nControl\nInformation security incidents shall be responded to in accordance with \nthe documented procedures.\n5.27\nLearning from information se-\ncurity incidents\nControl\nKnowledge gained from information security incidents shall be used to \nstrengthen and improve the information security controls.\n5.28\nCollection of evidence\nControl\nThe organization shall establish and implement procedures for the iden-\ntification, collection, acquisition and preservation of evidence related \nto information security events.\n5.29\nInformation security during \ndisruption\nControl\nThe organization shall plan how to maintain information security at an \nappropriate level during disruption.\n5.30\nICT readiness for business con-\ntinuity\nControl\nICT readiness shall be planned, implemented, maintained and tested \nbased on business continuity objectives and ICT continuity requirements.\n5.31\nLegal, statutory, regulatory and \ncontractual requirements\nControl\nLegal, statutory, regulatory and contractual requirements relevant to \ninformation security and the organization’s approach to meet these \nrequirements shall be identified, documented and kept up to date.\n5.32\nIntellectual property rights\nControl\nThe organization shall implement appropriate procedures to protect \nintellectual property rights.\n5.33\nProtection of records\nControl\nRecords shall be protected from loss, destruction, falsification, unau-\nthorized access and unauthorized release.\n5.34\nPrivacy and protection of person-\nal identifiable information (PII)\nControl\nThe organization shall identify and meet the requirements regarding \nthe preservation of privacy and protection of PII according to applicable \nlaws and regulations and contractual requirements.\n5.35\nIndependent review of informa-\ntion security\nControl\nThe organization’s approach to managing information security and \nits implementation including people, processes and technologies shall \nbe reviewed independently at planned intervals, or when significant \nchanges occur.\n5.36\nCompliance with policies, rules \nand standards for information \nsecurity\nControl\nCompliance with the organization’s information security policy, top-\nic-specific policies, rules and standards shall be regularly reviewed.\n5.37\nDocumented operating proce-\ndures\nControl\nOperating procedures for information processing facilities shall be \ndocumented and made available to personnel who need them.\nTable A.1 (continued)\nTable A.1 (continued)\n© ISO/IEC 2022 – All rights reserved\t\n﻿\n﻿\n13\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---\nISO/IEC 27001:2022(E)\n6\nPeople controls\n6.1\nScreening\nControl\nBackground verification checks on all candidates to become personnel \nshall be carried out prior to joining the organization and on an ongoing \nbasis taking into consideration applicable laws, regulations and ethics \nand be proportional to the business requirements, the classification of \nthe information to be accessed and the perceived risks.\n6.2\nTerms and conditions of em-\nployment\nControl\nThe employment contractual agreements shall state the personnel’s and \nthe organization’s responsibilities for information security.\n6.3\nInformation security awareness, \neducation and training\nControl\nPersonnel of the organization and relevant interested parties shall receive \nappropriate information security awareness, education and training \nand regular updates of the organization's information security policy, \ntopic-specific policies and procedures, as relevant for their job function.\n6.4\nDisciplinary process\nControl\nA disciplinary process shall be formalized and communicated to take \nactions against personnel and other relevant interested parties who \nhave committed an information security policy violation.\n6.5\nResponsibilities after termination \nor change of employment\nControl\nInformation security responsibilities and duties that remain valid after \ntermination or change of employment shall be defined, enforced and \ncommunicated to relevant personnel and other interested parties.\n6.6\nConfidentiality or non-disclosure \nagreements\nControl\nConfidentiality or non-disclosure agreements reflecting the organ-\nization’s needs for the protection of information shall be identified, \ndocumented, regularly reviewed and signed by personnel and other \nrelevant interested parties.\n6.7\nRemote working\nControl\nSecurity measures shall be implemented when personnel are working \nremotely to protect information accessed, processed or stored outside \nthe organization’s premises.\n6.8\nInformation security event re-\nporting\nControl\nThe organization shall provide a mechanism for personnel to report \nobserved or suspected information security events through appropriate \nchannels in a timely manner.\n7\nPhysical controls\n7.1\nPhysical security perimeters\nControl\nSecurity perimeters shall be defined and used to protect areas that \ncontain information and other associated assets.\n7.2\nPhysical entry\nControl\nSecure areas shall be protected by appropriate entry controls and \naccess points.\n7.3\nSecuring offices, rooms and fa-\ncilities\nControl\nPhysical security for offices, rooms and facilities shall be designed and \nimplemented.\n7.4\nPhysical security monitoring\nControl\nPremises shall be continuously monitored for unauthorized physical \naccess.\nTable A.1 (continued)\nTable A.1 (continued)\n\t\n﻿\b\n© ISO/IEC 2022 – All rights reserved\n\b\n﻿\n14\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---\nISO/IEC 27001:2022(E)\n7.5\nProtecting against physical and \nenvironmental threats\nControl\nProtection against physical and environmental threats, such as natural \ndisasters and other intentional or unintentional physical threats to \ninfrastructure shall be designed and implemented.\n7.6\nWorking in secure areas\nControl\nSecurity measures for working in secure areas shall be designed and \nimplemented.\n7.7\nClear desk and clear screen\nControl\nClear desk rules for papers and removable storage media and clear \nscreen rules for information processing facilities shall be defined and \nappropriately enforced.\n7.8\nEquipment siting and protection Control\nEquipment shall be sited securely and protected.\n7.9\nSecurity of assets off-premises Control\nOff-site assets shall be protected.\n7.10\nStorage media\nControl\nStorage media shall be managed through their life cycle of acquisition, \nuse, transportation and disposal in accordance with the organization’s \nclassification scheme and handling requirements.\n7.11\nSupporting utilities\nControl\nInformation processing facilities shall be protected from power failures \nand other disruptions caused by failures in supporting utilities.\n7.12\nCabling security\nControl\nCables carrying power, data or supporting information services shall \nbe protected from interception, interference or damage.\n7.13\nEquipment maintenance\nControl\nEquipment shall be maintained correctly to ensure availability, integrity \nand confidentiality of information.\n7.14\nSecure disposal or re-use of \nequipment\nControl\nItems of equipment containing storage media shall be verified to en-\nsure that any sensitive data and licensed software has been removed \nor securely overwritten prior to disposal or re-use.\n8\nTechnological controls\n8.1\nUser end point devices\nControl\nInformation stored on, processed by or accessible via user end point \ndevices shall be protected.\n8.2\nPrivileged access rights\nControl\nThe allocation and use of privileged access rights shall be restricted \nand managed.\n8.3\nInformation access restriction\nControl\nAccess to information and other associated assets shall be restricted in \naccordance with the established topic-specific policy on access control.\n8.4\nAccess to source code\nControl\nRead and write access to source code, development tools and software \nlibraries shall be appropriately managed.\nTable A.1 (continued)\nTable A.1 (continued)\n© ISO/IEC 2022 – All rights reserved\t\n﻿\n﻿\n15\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---\nISO/IEC 27001:2022(E)\n8.5\nSecure authentication\nControl\nSecure authentication technologies and procedures shall be implemented \nbased on information access restrictions and the topic-specific policy \non access control.\n8.6\nCapacity management\nControl\nThe use of resources shall be monitored and adjusted in line with current \nand expected capacity requirements.\n8.7\nProtection against malware\nControl\nProtection against malware shall be implemented and supported by \nappropriate user awareness.\n8.8\nManagement of technical vul-\nnerabilities\nControl\nInformation about technical vulnerabilities of information systems in \nuse shall be obtained, the organization’s exposure to such vulnerabilities \nshall be evaluated and appropriate measures shall be taken.\n8.9\nConfiguration management\nControl\nConfigurations, including security configurations, of hardware, software, \nservices and networks shall be established, documented, implemented, \nmonitored and reviewed.\n8.10\nInformation deletion\nControl\nInformation stored in information systems, devices or in any other \nstorage media shall be deleted when no longer required.\n8.11\nData masking\nControl\nData masking shall be used in accordance with the organization’s \ntopic-specific policy on access control and other related topic-specific \npolicies, and business requirements, taking applicable legislation into \nconsideration.\n8.12\nData leakage prevention\nControl\nData leakage prevention measures shall be applied to systems, net-\nworks and any other devices that process, store or transmit sensitive \ninformation.\n8.13\nInformation backup\nControl\nBackup copies of information, software and systems shall be maintained \nand regularly tested in accordance with the agreed topic-specific policy \non backup.\n8.14\nRedundancy of information pro-\ncessing facilities\nControl\nInformation processing facilities shall be implemented with redundancy \nsufficient to meet availability requirements.\n8.15\nLogging\nControl\nLogs that record activities, exceptions, faults and other relevant events \nshall be produced, stored, protected and analysed.\n8.16\nMonitoring activities\nControl\nNetworks, systems and applications shall be monitored for anomalous \nbehaviour and appropriate actions taken to evaluate potential infor-\nmation security incidents.\n8.17\nClock synchronization\nControl\nThe clocks of information processing systems used by the organization \nshall be synchronized to approved time sources.\nTable A.1 (continued)\nTable A.1 (continued)\n\t\n﻿\b\n© ISO/IEC 2022 – All rights reserved\n\b\n﻿\n16\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---\nISO/IEC 27001:2022(E)\n8.18\nUse of privileged utility programs Control\nThe use of utility programs that can be capable of overriding system \nand application controls shall be restricted and tightly controlled.\n8.19\nInstallation of software on op-\nerational systems\nControl\nProcedures and measures shall be implemented to securely manage \nsoftware installation on operational systems.\n8.20\nNetworks security\nControl\nNetworks and network devices shall be secured, managed and controlled \nto protect information in systems and applications.\n8.21\nSecurity of network services\nControl\nSecurity mechanisms, service levels and service requirements of network \nservices shall be identified, implemented and monitored.\n8.22\nSegregation of networks\nControl\nGroups of information services, users and information systems shall \nbe segregated in the organization’s networks.\n8.23\nWeb filtering\nControl\nAccess to external websites shall be managed to reduce exposure to \nmalicious content.\n8.24\nUse of cryptography\nControl\nRules for the effective use of cryptography, including cryptographic key \nmanagement, shall be defined and implemented.\n8.25\nSecure development life cycle\nControl\nRules for the secure development of software and systems shall be \nestablished and applied.\n8.26\nApplication security require-\nments\nControl\nInformation security requirements shall be identified, specified and \napproved when developing or acquiring applications.\n8.27\nSecure system architecture and \nengineering principles\nControl\nPrinciples for engineering secure systems shall be established, docu-\nmented, maintained and applied to any information system development \nactivities.\n8.28\nSecure coding\nControl\nSecure coding principles shall be applied to software development.\n8.29\nSecurity testing in development \nand acceptance\nControl\nSecurity testing processes shall be defined and implemented in the \ndevelopment life cycle.\n8.30\nOutsourced development\nControl\nThe organization shall direct, monitor and review the activities related \nto outsourced system development.\n8.31\nSeparation of development, test \nand production environments\nControl\nDevelopment, testing and production environments shall be separated \nand secured.\n8.32\nChange management\nControl\nChanges to information processing facilities and information systems \nshall be subject to change management procedures.\n8.33\nTest information\nControl\nTest information shall be appropriately selected, protected and managed.\nTable A.1 (continued)\nTable A.1 (continued)\n© ISO/IEC 2022 – All rights reserved\t\n﻿\n﻿\n17\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---\nISO/IEC 27001:2022(E)\n8.34\nProtection of information sys-\ntems during audit testing\nControl\nAudit tests and other assurance activities involving assessment of op-\nerational systems shall be planned and agreed between the tester and \nappropriate management.\nTable A.1 (continued)\nTable A.1 (continued)\n\t\n﻿\b\n© ISO/IEC 2022 – All rights reserved\n\b\n﻿\n18\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---\nISO/IEC 27001:2022(E)\nBibliography\n[1]\t\nISO/IEC 27002:2022, Information security, cybersecurity and privacy protection — Information \nsecurity controls\n[2]\t\nISO/IEC 27003, Information technology — Security techniques — Information security management \nsystems — Guidance\n[3]\t\nISO/IEC 27004, Information technology — Security techniques — Information security management \n— Monitoring, measurement, analysis and evaluation\n[4]\t\nISO/IEC 27005, Information security, cybersecurity and privacy protection — Guidance on \nmanaging information security risks\n[5]\t\nISO 31000:2018, Risk management — Guidelines\n© ISO/IEC 2022 – All rights reserved\t\n﻿\n﻿\n19\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---\nISO/IEC 27001:2022(E)\nICS 03.100.70; 35.030\nPrice based on 19 pages\n© ISO/IEC 2022 – All rights reserved\t\n﻿\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---",
      "page_count": 26,
      "pages": [
        {
          "page": 1,
          "text": "Information security, cybersecurity \nand privacy protection — Information \nsecurity management systems — \nRequirements\nSécurité de l'information, cybersécurité et protection de la vie \nprivée — Systèmes de management de la sécurité de l'information — \nExigences\nINTERNATIONAL \nSTANDARD\nISO/IEC \n27001\nThird edition \n2022-10\nReference number \nISO/IEC 27001:2022(E)\n© ISO/IEC 2022\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---",
          "char_count": 429,
          "ocr_used": false
        },
        {
          "page": 2,
          "text": "ii\nISO/IEC 27001:2022(E)\nCOPYRIGHT PROTECTED DOCUMENT\n©  ISO/IEC 2022\nAll rights reserved. Unless otherwise specified, or required in the context of its implementation, no part of this publication may \nbe reproduced or utilized otherwise in any form or by any means, electronic or mechanical, including photocopying, or posting on \nthe internet or an intranet, without prior written permission. Permission can be requested from either ISO at the address below \nor ISO’s member body in the country of the requester.\nISO copyright office\nCP 401 • Ch. de Blandonnet 8\nCH-1214 Vernier, Geneva\nPhone: +41 22 749 01 11\nEmail: copyright@iso.org\nWebsite: www.iso.org\nPublished in Switzerland\n\t\n﻿\b\n© ISO/IEC 2022 – All rights reserved\n\b\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---",
          "char_count": 778,
          "ocr_used": false
        },
        {
          "page": 3,
          "text": "ISO/IEC 27001:2022(E)\nForeword........................................................................................................................................................................................................................................iv\nIntroduction..................................................................................................................................................................................................................................v\n1\t\n​Scope.................................................................................................................................................................................................................................1\n2\t\n​Normative references......................................................................................................................................................................................1\n3 \n​Terms and definitions.....................................................................................................................................................................................1\n4\t\n​Context of the organization......................................................................................................................................................................1\n4.1\t\n​Understanding the organization and its context......................................................................................................1\n4.2\t\n​Understanding the needs and expectations of interested parties...........................................................1\n4.3\t\n​Determining the scope of the information security management system........................................2\n4.4\t\n​Information security management system................................................................................................................... 2\n5\t\n​Leadership..................................................................................................................................................................................................................2\n5.1\t\n​Leadership and commitment..................................................................................................................................................... 2\n5.2\t\n​Policy................................................................................................................................................................................................................ 3\n5.3\t\n​Organizational roles, responsibilities and authorities........................................................................................3\n6\t\n​Planning.........................................................................................................................................................................................................................3\n6.1\t\n​Actions to address risks and opportunities.................................................................................................................. 3\n6.1.1\t\n​General.........................................................................................................................................................................................3\n6.1.2\t\n​Information security risk assessment.............................................................................................................4\n6.1.3\t\n​Information security risk treatment................................................................................................................4\n6.2\t\n​Information security objectives and planning to achieve them.................................................................5\n7\t\n​Support...........................................................................................................................................................................................................................6\n7.1\t\n​Resources..................................................................................................................................................................................................... 6\n7.2\t\n​Competence................................................................................................................................................................................................ 6\n7.3\t\n​Awareness................................................................................................................................................................................................... 6\n7.4\t\n​Communication......................................................................................................................................................................................6\n7.5\t\n​Documented information.............................................................................................................................................................. 6\n7.5.1\t\n​General.........................................................................................................................................................................................6\n7.5.2\t\n​Creating and updating................................................................................................................................................... 7\n7.5.3\t\n​Control of documented information..................................................................................................................7\n8\t\n​Operation.....................................................................................................................................................................................................................7\n8.1\t\n​Operational planning and control.......................................................................................................................................... 7\n8.2\t\n​Information security risk assessment............................................................................................................................... 8\n8.3\t\n​Information security risk treatment................................................................................................................................... 8\n9\t\n​Performance evaluation...............................................................................................................................................................................8\n9.1\t\n​Monitoring, measurement, analysis and evaluation.............................................................................................8\n9.2\t\n​Internal audit............................................................................................................................................................................................8\n9.2.1\t\nGeneral.........................................................................................................................................................................................8\n9.2.2\t\nInternal audit programme......................................................................................................................................... 9\n9.3\t\n​Management review...........................................................................................................................................................................9\n9.3.1\t\nGeneral.........................................................................................................................................................................................9\n9.3.2\t\nManagement review inputs....................................................................................................................................... 9\n9.3.3\t\nManagement review results..................................................................................................................................... 9\n10\t\n​Improvement.........................................................................................................................................................................................................10\n10.1\t\n​Continual improvement...............................................................................................................................................................10\n10.2\t\n​Nonconformity and corrective action.............................................................................................................................10\nAnnex A (normative) Information security controls reference...........................................................................................11\nBibliography.............................................................................................................................................................................................................................19\niii\n© ISO/IEC 2022 – All rights reserved\t\n﻿\nContents\b\nPage\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---",
          "char_count": 9044,
          "ocr_used": false
        },
        {
          "page": 4,
          "text": "ISO/IEC 27001:2022(E)\nForeword\nISO (the International Organization for Standardization) and IEC (the International Electrotechnical \nCommission) form the specialized system for worldwide standardization. National bodies that are \nmembers of ISO or IEC participate in the development of International Standards through technical \ncommittees established by the respective organization to deal with particular fields of technical \nactivity. ISO and IEC technical committees collaborate in fields of mutual interest. Other international \norganizations, governmental and non-governmental, in liaison with ISO and IEC, also take part in the \nwork.\nThe procedures used to develop this document and those intended for its further maintenance \nare described in the ISO/IEC Directives, Part 1. In particular, the different approval criteria \nneeded for the different types of document should be noted. This document was drafted in \naccordance with the editorial rules of the ISO/IEC Directives, Part 2 (see www.iso.org/directives or \nwww.iec.ch/members_experts/refdocs).\nAttention is drawn to the possibility that some of the elements of this document may be the subject \nof patent rights. ISO and IEC shall not be held responsible for identifying any or all such patent \nrights. Details of any patent rights identified during the development of the document will be in the \nIntroduction and/or on the ISO list of patent declarations received (see www.iso.org/patents) or the IEC \nlist of patent declarations received (see https://patents.iec.ch).\nAny trade name used in this document is information given for the convenience of users and does not \nconstitute an endorsement.\nFor an explanation of the voluntary nature of standards, the meaning of ISO specific terms and \nexpressions related to conformity assessment, as well as information about ISO's adherence to \nthe World Trade Organization (WTO) principles in the Technical Barriers to Trade (TBT) see \nwww.iso.org/iso/foreword.html. In the IEC, see www.iec.ch/understanding-standards.\nThis document was prepared by Joint Technical Committee ISO/IEC  JTC  1, Information Technology, \nSubcommittee SC 27, Information security, cybersecurity and privacy protection.\nThis third edition cancels and replaces the second edition (ISO/IEC 27001:2013), which has been \ntechnically revised. It also incorporates the Technical Corrigenda ISO/IEC 27001:2013/Cor 1:2014 and \nISO/IEC 27001:2013/Cor 2:2015.\nThe main changes are as follows:\n—\t\nthe text has been aligned with the harmonized structure for management system standards \nand ISO/IEC 27002:2022.\nAny feedback or questions on this document should be directed to the user’s national standards \nbody. A complete listing of these bodies can be found at www.iso.org/members.html and \nwww.iec.ch/national-committees.\niv\n\t\n﻿\b\n© ISO/IEC 2022 – All rights reserved\n\b\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---",
          "char_count": 2901,
          "ocr_used": false
        },
        {
          "page": 5,
          "text": "ISO/IEC 27001:2022(E)\nIntroduction\n0.1   General\nThis document has been prepared to provide requirements for establishing, implementing, maintaining \nand continually improving an information security management system. The adoption of an \ninformation security management system is a strategic decision for an organization. The establishment \nand implementation of an organization’s information security management system is influenced by the \norganization’s needs and objectives, security requirements, the organizational processes used and the \nsize and structure of the organization. All of these influencing factors are expected to change over time.\nThe information security management system preserves the confidentiality, integrity and availability \nof information by applying a risk management process and gives confidence to interested parties that \nrisks are adequately managed.\nIt is important that the information security management system is part of and integrated with the \norganization’s processes and overall management structure and that information security is considered \nin the design of processes, information systems, and controls. It is expected that an information security \nmanagement system implementation will be scaled in accordance with the needs of the organization.\nThis document can be used by internal and external parties to assess the organization's ability to meet \nthe organization’s own information security requirements.\nThe order in which requirements are presented in this document does not reflect their importance \nor imply the order in which they are to be implemented. The list items are enumerated for reference \npurpose only.\nISO/IEC  27000 describes the overview and the vocabulary of information security management \nsystems, referencing the information security management system family of standards (including \nISO/IEC 27003[2], ISO/IEC 27004[3] and ISO/IEC 27005[4]), with related terms and definitions.\n \n0.2   Compatibility with other management system standards\nThis document applies the high-level structure, identical sub-clause titles, identical text, common terms, \nand core definitions defined in Annex SL of ISO/IEC Directives, Part 1, Consolidated ISO Supplement, \nand therefore maintains compatibility with other management system standards that have adopted the \nAnnex SL.\nThis common approach defined in the Annex SL will be useful for those organizations that choose to \noperate a single management system that meets the requirements of two or more management system \nstandards.\nv\n© ISO/IEC 2022 – All rights reserved\t\n﻿\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---",
          "char_count": 2636,
          "ocr_used": false
        },
        {
          "page": 6,
          "text": "--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---",
          "char_count": 51,
          "ocr_used": true,
          "original_char_count": 50
        },
        {
          "page": 7,
          "text": "INTERNATIONAL STANDARD\nISO/IEC 27001:2022(E)\nInformation security, cybersecurity and privacy \nprotection — Information security management systems \n— Requirements\n1\t ​Scope\nThis document specifies the requirements for establishing, implementing, maintaining and continually \nimproving an information security management system within the context of the organization. This \ndocument also includes requirements for the assessment and treatment of information security risks \ntailored to the needs of the organization. The requirements set out in this document are generic and are \nintended to be applicable to all organizations, regardless of type, size or nature. Excluding any of the \nrequirements specified in Clauses 4 to 10 is not acceptable when an organization claims conformity to \nthis document.\n2\t ​Normative references\nThe following documents are referred to in the text in such a way that some or all of their content \nconstitutes requirements of this document. For dated references, only the edition cited applies. For \nundated references, the latest edition of the referenced document (including any amendments) applies.\nISO/IEC  27000, Information technology — Security techniques — Information security management \nsystems — Overview and vocabulary\n3 ​Terms and definitions\nFor the purposes of this document, the terms and definitions given in ISO/IEC 27000 apply.\nISO and IEC maintain terminology databases for use in standardization at the following addresses:\n—\t ISO Online browsing platform: available at https://​www​.iso​.org/​obp\n—\t IEC Electropedia: available at https://​www​.electropedia​.org/​\n4\t ​Context of the organization\n4.1\t ​Understanding the organization and its context\nThe organization shall determine external and internal issues that are relevant to its purpose and that \naffect its ability to achieve the intended outcome(s) of its information security management system.\nNOTE\t\nDetermining these issues refers to establishing the external and internal context of the organization \nconsidered in Clause 5.4.1 of ISO 31000:2018[5].\n4.2\t ​Understanding the needs and expectations of interested parties\nThe organization shall determine:\na)\t interested parties that are relevant to the information security management system; \nb)\t the relevant requirements of these interested parties; \nc)\t\nwhich of these requirements will be addressed through the information security management \nsystem. \n1\n© ISO/IEC 2022 – All rights reserved\t\n﻿\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---",
          "char_count": 2516,
          "ocr_used": false
        },
        {
          "page": 8,
          "text": "ISO/IEC 27001:2022(E)\nNOTE\t\nThe requirements of interested parties can include legal and regulatory requirements and contractual \nobligations.\n4.3\t ​Determining the scope of the information security management system\nThe organization shall determine the boundaries and applicability of the information security \nmanagement system to establish its scope.\nWhen determining this scope, the organization shall consider:\na)\t the external and internal issues referred to in 4.1;\nb)\t the requirements referred to in 4.2; \nc)\t\ninterfaces and dependencies between activities performed by the organization, and those that are \nperformed by other organizations.\nThe scope shall be available as documented information.\n4.4\t ​Information security management system\nThe organization shall establish, implement, maintain and continually improve an information security \nmanagement system, including the processes needed and their interactions, in accordance with the \nrequirements of this document.\n5\t ​Leadership\n5.1\t ​Leadership and commitment\nTop management shall demonstrate leadership and commitment with respect to the information \nsecurity management system by:\na)\t ensuring the information security policy and the information security objectives are established \nand are compatible with the strategic direction of the organization;\nb)\t ensuring the integration of the information security management system requirements into the \norganization’s processes;\nc)\t\nensuring that the resources needed for the information security management system are available;\nd)\t communicating the importance of effective information security management and of conforming \nto the information security management system requirements;\ne)\t ensuring that the information security management system achieves its intended outcome(s);\nf)\t\ndirecting and supporting persons to contribute to the effectiveness of the information security \nmanagement system;\ng)\t promoting continual improvement; and\nh)\t supporting other relevant management roles to demonstrate their leadership as it applies to their \nareas of responsibility.\nNOTE\t\nReference to “business” in this document can be interpreted broadly to mean those activities that are \ncore to the purposes of the organization’s existence.\n\t\n﻿\b\n© ISO/IEC 2022 – All rights reserved\n\b\n﻿\n2\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---",
          "char_count": 2352,
          "ocr_used": false
        },
        {
          "page": 9,
          "text": "ISO/IEC 27001:2022(E)\n5.2\t ​Policy\nTop management shall establish an information security policy that:\na)\t is appropriate to the purpose of the organization;\nb)\t includes information security objectives (see 6.2) or provides the framework for setting information \nsecurity objectives;\nc)\t\nincludes a commitment to satisfy applicable requirements related to information security; \nd)\t includes a commitment to continual improvement of the information security management system.\nThe information security policy shall:\ne)\t be available as documented information;\nf)\t\nbe communicated within the organization; \ng)\t be available to interested parties, as appropriate.\n5.3\t ​Organizational roles, responsibilities and authorities\nTop management shall ensure that the responsibilities and authorities for roles relevant to information \nsecurity are assigned and communicated within the organization.\nTop management shall assign the responsibility and authority for:\na)\t ensuring that the information security management system conforms to the requirements of this \ndocument; \nb)\t reporting on the performance of the information security management system to top management.\nNOTE\t\nTop management can also assign responsibilities and authorities for reporting performance of the \ninformation security management system within the organization.\n6\t ​Planning\n6.1\t ​Actions to address risks and opportunities\n6.1.1\t\n​General\nWhen planning for the information security management system, the organization shall consider \nthe issues referred to in 4.1 and the requirements referred to in 4.2 and determine the risks and \nopportunities that need to be addressed to:\na)\t ensure the information security management system can achieve its intended outcome(s);\nb)\t prevent, or reduce, undesired effects; \nc)\t\nachieve continual improvement.\nThe organization shall plan:\nd)\t actions to address these risks and opportunities; and\ne)\t how to\n1)\t integrate and implement the actions into its information security management system \nprocesses; and\n2)\t evaluate the effectiveness of these actions.\n© ISO/IEC 2022 – All rights reserved\t\n﻿\n﻿\n3\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---",
          "char_count": 2166,
          "ocr_used": false
        },
        {
          "page": 10,
          "text": "ISO/IEC 27001:2022(E)\n6.1.2\t\n​Information security risk assessment\nThe organization shall define and apply an information security risk assessment process that:\na)\t establishes and maintains information security risk criteria that include:\n1)\t the risk acceptance criteria; and\n2)\t criteria for performing information security risk assessments;\nb)\t ensures that repeated information security risk assessments produce consistent, valid and \ncomparable results;\nc)\t\nidentifies the information security risks:\n1)\t apply the information security risk assessment process to identify risks associated with \nthe loss of confidentiality, integrity and availability for information within the scope of the \ninformation security management system; and\n2)\t identify the risk owners;\nd)\t analyses the information security risks:\n1)\t assess the potential consequences that would result if the risks identified in 6.1.2 c) 1) were to \nmaterialize;\n2)\t assess the realistic likelihood of the occurrence of the risks identified in 6.1.2 c) 1); and\n3)\t determine the levels of risk;\ne)\t evaluates the information security risks:\n1)\t compare the results of risk analysis with the risk criteria established in 6.1.2 a); and\n2)\t prioritize the analysed risks for risk treatment.\nThe organization shall retain documented information about the information security risk assessment \nprocess.\n6.1.3\t\n​Information security risk treatment\nThe organization shall define and apply an information security risk treatment process to:\na)\t select appropriate information security risk treatment options, taking account of the risk \nassessment results;\nb)\t determine all controls that are necessary to implement the information security risk treatment \noption(s) chosen;\nNOTE 1\t\nOrganizations can design controls as required, or identify them from any source.\nc)\t\ncompare the controls determined in 6.1.3 b) above with those in Annex  A and verify that no \nnecessary controls have been omitted;\nNOTE 2\t\nAnnex  A contains a list of possible information security controls. Users of this document are \ndirected to Annex A to ensure that no necessary information security controls are overlooked.\nNOTE 3\t\nThe information security controls listed in Annex A are not exhaustive and additional information \nsecurity controls can be included if needed.\nd)\t produce a Statement of Applicability that contains:\n—\t the necessary controls (see 6.1.3 b) and c));\n\t\n﻿\b\n© ISO/IEC 2022 – All rights reserved\n\b\n﻿\n4\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---",
          "char_count": 2515,
          "ocr_used": false
        },
        {
          "page": 11,
          "text": "ISO/IEC 27001:2022(E)\n—\t justification for their inclusion;\n—\t whether the necessary controls are implemented or not; and\n—\t the justification for excluding any of the Annex A controls.\ne)\t formulate an information security risk treatment plan; and\nf)\t\nobtain risk owners’ approval of the information security risk treatment plan and acceptance of the \nresidual information security risks.\nThe organization shall retain documented information about the information security risk treatment \nprocess.\nNOTE 4\t\nThe information security risk assessment and treatment process in this document aligns with the \nprinciples and generic guidelines provided in ISO 31000[5].\n6.2\t ​Information security objectives and planning to achieve them\nThe organization shall establish information security objectives at relevant functions and levels.\nThe information security objectives shall:\na)\t be consistent with the information security policy;\nb)\t be measurable (if practicable);\nc)\t\ntake into account applicable information security requirements, and results from risk assessment \nand risk treatment;\nd)\t be monitored;\ne)\t be communicated; \nf)\t\nbe updated as appropriate;\ng)\t be available as documented information.\nThe organization shall retain documented information on the information security objectives.\nWhen planning how to achieve its information security objectives, the organization shall determine:\nh)\t what will be done;\ni)\t\nwhat resources will be required;\nj)\t\nwho will be responsible;\nk)\t when it will be completed; and\nl)\t\nhow the results will be evaluated.\n6.3\t\nPlanning of changes\nWhen the organization determines the need for changes to the information security management \nsystem, the changes shall be carried out in a planned manner.\n© ISO/IEC 2022 – All rights reserved\t\n﻿\n﻿\n5\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---",
          "char_count": 1833,
          "ocr_used": false
        },
        {
          "page": 12,
          "text": "ISO/IEC 27001:2022(E)\n7\t ​Support\n7.1\t ​Resources\nThe organization shall determine and provide the resources needed for the establishment, \nimplementation, maintenance and continual improvement of the information security management \nsystem.\n7.2\t ​Competence\nThe organization shall:\na)\t determine the necessary competence of person(s) doing work under its control that affects its \ninformation security performance;\nb)\t ensure that these persons are competent on the basis of appropriate education, training, or \nexperience;\nc)\t\nwhere applicable, take actions to acquire the necessary competence, and evaluate the effectiveness \nof the actions taken; and\nd)\t retain appropriate documented information as evidence of competence.\nNOTE\t\nApplicable actions can include, for example: the provision of training to, the mentoring of, or the re-\nassignment of current employees; or the hiring or contracting of competent persons.\n7.3\t ​Awareness\nPersons doing work under the organization’s control shall be aware of:\na)\t the information security policy;\nb)\t their contribution to the effectiveness of the information security management system, including \nthe benefits of improved information security performance; and\nc)\t\nthe implications of not conforming with the information security management system \nrequirements.\n7.4\t ​Communication\nThe organization shall determine the need for internal and external communications relevant to the \ninformation security management system including:\na)\t on what to communicate;\nb)\t when to communicate;\nc)\t\nwith whom to communicate;\nd)\t how to communicate.\n7.5\t ​Documented information\n7.5.1\t\n​General\nThe organization’s information security management system shall include:\na)\t documented information required by this document; and\n\t\n﻿\b\n© ISO/IEC 2022 – All rights reserved\n\b\n﻿\n6\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---",
          "char_count": 1864,
          "ocr_used": false
        },
        {
          "page": 13,
          "text": "ISO/IEC 27001:2022(E)\nb)\t documented information determined by the organization as being necessary for the effectiveness \nof the information security management system.\nNOTE\t\nThe extent of documented information for an information security management system can differ \nfrom one organization to another due to:\n1)\t\nthe size of organization and its type of activities, processes, products and services;\n2)\t\nthe complexity of processes and their interactions; and\n3)\t\nthe competence of persons.\n7.5.2\t\n​Creating and updating\nWhen creating and updating documented information the organization shall ensure appropriate:\na)\t identification and description (e.g. a title, date, author, or reference number);\nb)\t format (e.g. language, software version, graphics) and media (e.g. paper, electronic); and\nc)\t\nreview and approval for suitability and adequacy.\n7.5.3\t\n​Control of documented information\nDocumented information required by the information security management system and by this \ndocument shall be controlled to ensure:\na)\t it is available and suitable for use, where and when it is needed; and\nb)\t it is adequately protected (e.g. from loss of confidentiality, improper use, or loss of integrity).\nFor the control of documented information, the organization shall address the following activities, as \napplicable:\nc)\t\ndistribution, access, retrieval and use;\nd)\t storage and preservation, including the preservation of legibility;\ne)\t control of changes (e.g. version control); and\nf)\t\nretention and disposition.\nDocumented information of external origin, determined by the organization to be necessary for \nthe planning and operation of the information security management system, shall be identified as \nappropriate, and controlled.\nNOTE\t\nAccess can imply a decision regarding the permission to view the documented information only, or \nthe permission and authority to view and change the documented information, etc.\n8\t ​Operation\n8.1\t ​Operational planning and control\nThe organization shall plan, implement and control the processes needed to meet requirements, and to \nimplement the actions determined in Clause 6, by: \n—\t establishing criteria for the processes; \n—\t implementing control of the processes in accordance with the criteria. \nDocumented information shall be available to the extent necessary to have confidence that the \nprocesses have been carried out as planned. \n© ISO/IEC 2022 – All rights reserved\t\n﻿\n﻿\n7\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---",
          "char_count": 2485,
          "ocr_used": false
        },
        {
          "page": 14,
          "text": "ISO/IEC 27001:2022(E)\nThe organization shall control planned changes and review the consequences of unintended changes, \ntaking action to mitigate any adverse effects, as necessary. \nThe organization shall ensure that externally provided processes, products or services that are relevant \nto the information security management system are controlled.\n8.2\t ​Information security risk assessment\nThe organization shall perform information security risk assessments at planned intervals or when \nsignificant changes are proposed or occur, taking account of the criteria established in 6.1.2 a).\nThe organization shall retain documented information of the results of the information security risk \nassessments.\n8.3\t ​Information security risk treatment\nThe organization shall implement the information security risk treatment plan.\nThe organization shall retain documented information of the results of the information security risk \ntreatment.\n9\t ​Performance evaluation\n9.1\t ​Monitoring, measurement, analysis and evaluation\nThe organization shall determine:\na)\t what needs to be monitored and measured, including information security processes and controls;\nb)\t the methods for monitoring, measurement, analysis and evaluation, as applicable, to ensure \nvalid results. The methods selected should produce comparable and reproducible results to be \nconsidered valid;\nc)\t\nwhen the monitoring and measuring shall be performed;\nd)\t who shall monitor and measure;\ne)\t when the results from monitoring and measurement shall be analysed and evaluated; \nf)\t\nwho shall analyse and evaluate these results.\nDocumented information shall be available as evidence of the results.\nThe organization shall evaluate the information security performance and the effectiveness of the \ninformation security management system.\n9.2\t ​Internal audit\n9.2.1\t\nGeneral\nThe organization shall conduct internal audits at planned intervals to provide information on whether \nthe information security management system:\na)\t conforms to\n1)\t the organization’s own requirements for its information security management system; \n\t\n﻿\b\n© ISO/IEC 2022 – All rights reserved\n\b\n﻿\n8\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---",
          "char_count": 2190,
          "ocr_used": false
        },
        {
          "page": 15,
          "text": "ISO/IEC 27001:2022(E)\n2)\t the requirements of this document;\nb)\t is effectively implemented and maintained.\n9.2.2\t\nInternal audit programme\nThe organization shall plan, establish, implement and maintain an audit programme(s), including the \nfrequency, methods, responsibilities, planning requirements and reporting. \nWhen establishing the internal audit programme(s), the organization shall consider the importance of \nthe processes concerned and the results of previous audits. \nThe organization shall: \na)\t define the audit criteria and scope for each audit;\nb)\t select auditors and conduct audits that ensure objectivity and the impartiality of the audit process;\nc)\t\nensure that the results of the audits are reported to relevant management; \nDocumented information shall be available as evidence of the implementation of the audit programme(s) \nand the audit results.\n9.3\t ​Management review\n9.3.1\t\nGeneral\nTop management shall review the organization's information security management system at planned \nintervals to ensure its continuing suitability, adequacy and effectiveness.\n9.3.2\t\nManagement review inputs\nThe management review shall include consideration of:\na)\t the status of actions from previous management reviews;\nb)\t changes in external and internal issues that are relevant to the information security management \nsystem;\nc)\t\nchanges in needs and expectations of interested parties that are relevant to the information \nsecurity management system;\nd)\t feedback on the information security performance, including trends in:\n1)\t nonconformities and corrective actions;\n2)\t monitoring and measurement results;\n3)\t audit results; \n4)\t fulfilment of information security objectives;\ne)\t feedback from interested parties;\nf)\t\nresults of risk assessment and status of risk treatment plan; \ng)\t opportunities for continual improvement.\n9.3.3\t\nManagement review results\nThe results of the management review shall include decisions related to continual improvement \nopportunities and any needs for changes to the information security management system.\n© ISO/IEC 2022 – All rights reserved\t\n﻿\n﻿\n9\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---",
          "char_count": 2157,
          "ocr_used": false
        },
        {
          "page": 16,
          "text": "ISO/IEC 27001:2022(E)\nDocumented information shall be available as evidence of the results of management reviews.\n10\t​Improvement\n10.1\t​Continual improvement\nThe organization shall continually improve the suitability, adequacy and effectiveness of the information \nsecurity management system.\n10.2\t​Nonconformity and corrective action\nWhen a nonconformity occurs, the organization shall:\na)\t react to the nonconformity, and as applicable:\n1)\t take action to control and correct it; \n2)\t deal with the consequences;\nb)\t evaluate the need for action to eliminate the causes of nonconformity, in order that it does not recur \nor occur elsewhere, by:\n1)\t reviewing the nonconformity;\n2)\t determining the causes of the nonconformity; and\n3)\t determining if similar nonconformities exist, or could potentially occur;\nc)\t\nimplement any action needed;\nd)\t review the effectiveness of any corrective action taken; and\ne)\t make changes to the information security management system, if necessary.\nCorrective actions shall be appropriate to the effects of the nonconformities encountered.\nDocumented information shall be available as evidence of: \nf)\t\nthe nature of the nonconformities and any subsequent actions taken, \ng)\t the results of any corrective action.\n\t\n﻿\b\n© ISO/IEC 2022 – All rights reserved\n\b\n﻿\n10\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---",
          "char_count": 1351,
          "ocr_used": false
        },
        {
          "page": 17,
          "text": "ISO/IEC 27001:2022(E)\nAnnex A \n(normative) \n \nInformation security controls reference\nThe information security controls listed in Table A.1 are directly derived from and aligned with those \nlisted in ISO/IEC 27002:2022[1], Clauses 5 to 8, and shall be used in context with 6.1.3.\nTable A.1 — Information security controls\n5\nOrganizational controls\n5.1\nPolicies for information secu-\nrity\nControl\nInformation security policy and topic-specific policies shall be de-\nfined, approved by management, published, communicated to and \nacknowledged by relevant personnel and relevant interested parties, \nand reviewed at planned intervals and if significant changes occur.\n5.2\nInformation security roles and \nresponsibilities\nControl\nInformation security roles and responsibilities shall be defined and \nallocated according to the organization needs.\n5.3\nSegregation of duties\nControl\nConflicting duties and conflicting areas of responsibility shall be seg-\nregated.\n5.4\nManagement responsibilities\nControl\nManagement shall require all personnel to apply information security \nin accordance with the established information security policy, top-\nic-specific policies and procedures of the organization.\n5.5\nContact with authorities\nControl\nThe organization shall establish and maintain contact with relevant \nauthorities.\n5.6\nContact with special interest \ngroups\nControl\nThe organization shall establish and maintain contact with special \ninterest groups or other specialist security forums and professional \nassociations.\n5.7\nThreat intelligence\nControl\nInformation relating to information security threats shall be collected \nand analysed to produce threat intelligence.\n5.8\nInformation security in project \nmanagement\nControl\nInformation security shall be integrated into project management.\n5.9\nInventory of information and \nother associated assets\nControl\nAn inventory of information and other associated assets, including \nowners, shall be developed and maintained.\n5.10\nAcceptable use of information \nand other associated assets\nControl\nRules for the acceptable use and procedures for handling information and \nother associated assets shall be identified, documented and implemented.\n5.11\nReturn of assets\nControl\nPersonnel and other interested parties as appropriate shall return all \nthe organization’s assets in their possession upon change or termination \nof their employment, contract or agreement.\n© ISO/IEC 2022 – All rights reserved\t\n﻿\n﻿\n11\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---",
          "char_count": 2499,
          "ocr_used": false
        },
        {
          "page": 18,
          "text": "ISO/IEC 27001:2022(E)\n5.12\nClassification of information\nControl\nInformation shall be classified according to the information security \nneeds of the organization based on confidentiality, integrity, availability \nand relevant interested party requirements.\n5.13\nLabelling of information\nControl\nAn appropriate set of procedures for information labelling shall be \ndeveloped and implemented in accordance with the information clas-\nsification scheme adopted by the organization.\n5.14\nInformation transfer\nControl\nInformation transfer rules, procedures, or agreements shall be in place \nfor all types of transfer facilities within the organization and between \nthe organization and other parties.\n5.15\nAccess control\nControl\nRules to control physical and logical access to information and other \nassociated assets shall be established and implemented based on busi-\nness and information security requirements.\n5.16\nIdentity management\nControl\nThe full life cycle of identities shall be managed.\n5.17\nAuthentication information\nControl\nAllocation and management of authentication information shall be \ncontrolled by a management process, including advising personnel on \nappropriate handling of authentication information.\n5.18\nAccess rights\nControl\nAccess rights to information and other associated assets shall be \nprovisioned, reviewed, modified and removed in accordance with the \norganization’s topic-specific policy on and rules for access control.\n5.19\nInformation security in supplier \nrelationships\nControl\nProcesses and procedures shall be defined and implemented to manage \nthe information security risks associated with the use of supplier’s \nproducts or services.\n5.20\nAddressing information security \nwithin supplier agreements\nControl\nRelevant information security requirements shall be established and \nagreed with each supplier based on the type of supplier relationship.\n5.21\nManaging information security \nin the information and commu-\nnication technology (ICT) supply \nchain\nControl\nProcesses and procedures shall be defined and implemented to manage \nthe information security risks associated with the ICT products and \nservices supply chain.\n5.22\nMonitoring, review and change \nmanagement of supplier services\nControl\nThe organization shall regularly monitor, review, evaluate and manage \nchange in supplier information security practices and service delivery.\n5.23\nInformation security for use of \ncloud services\nControl\nProcesses for acquisition, use, management and exit from cloud services \nshall be established in accordance with the organization’s information \nsecurity requirements.\n5.24\nInformation security incident \nmanagement planning and prepa-\nration\nControl\nThe organization shall plan and prepare for managing information secu-\nrity incidents by defining, establishing and communicating information \nsecurity incident management processes, roles and responsibilities.\nTable A.1 (continued)\nTable A.1 (continued)\n\t\n﻿\b\n© ISO/IEC 2022 – All rights reserved\n\b\n﻿\n12\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---",
          "char_count": 3045,
          "ocr_used": false
        },
        {
          "page": 19,
          "text": "ISO/IEC 27001:2022(E)\n5.25\nAssessment and decision on in-\nformation security events\nControl\nThe organization shall assess information security events and decide if \nthey are to be categorized as information security incidents.\n5.26\nResponse to information security \nincidents\nControl\nInformation security incidents shall be responded to in accordance with \nthe documented procedures.\n5.27\nLearning from information se-\ncurity incidents\nControl\nKnowledge gained from information security incidents shall be used to \nstrengthen and improve the information security controls.\n5.28\nCollection of evidence\nControl\nThe organization shall establish and implement procedures for the iden-\ntification, collection, acquisition and preservation of evidence related \nto information security events.\n5.29\nInformation security during \ndisruption\nControl\nThe organization shall plan how to maintain information security at an \nappropriate level during disruption.\n5.30\nICT readiness for business con-\ntinuity\nControl\nICT readiness shall be planned, implemented, maintained and tested \nbased on business continuity objectives and ICT continuity requirements.\n5.31\nLegal, statutory, regulatory and \ncontractual requirements\nControl\nLegal, statutory, regulatory and contractual requirements relevant to \ninformation security and the organization’s approach to meet these \nrequirements shall be identified, documented and kept up to date.\n5.32\nIntellectual property rights\nControl\nThe organization shall implement appropriate procedures to protect \nintellectual property rights.\n5.33\nProtection of records\nControl\nRecords shall be protected from loss, destruction, falsification, unau-\nthorized access and unauthorized release.\n5.34\nPrivacy and protection of person-\nal identifiable information (PII)\nControl\nThe organization shall identify and meet the requirements regarding \nthe preservation of privacy and protection of PII according to applicable \nlaws and regulations and contractual requirements.\n5.35\nIndependent review of informa-\ntion security\nControl\nThe organization’s approach to managing information security and \nits implementation including people, processes and technologies shall \nbe reviewed independently at planned intervals, or when significant \nchanges occur.\n5.36\nCompliance with policies, rules \nand standards for information \nsecurity\nControl\nCompliance with the organization’s information security policy, top-\nic-specific policies, rules and standards shall be regularly reviewed.\n5.37\nDocumented operating proce-\ndures\nControl\nOperating procedures for information processing facilities shall be \ndocumented and made available to personnel who need them.\nTable A.1 (continued)\nTable A.1 (continued)\n© ISO/IEC 2022 – All rights reserved\t\n﻿\n﻿\n13\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---",
          "char_count": 2803,
          "ocr_used": false
        },
        {
          "page": 20,
          "text": "ISO/IEC 27001:2022(E)\n6\nPeople controls\n6.1\nScreening\nControl\nBackground verification checks on all candidates to become personnel \nshall be carried out prior to joining the organization and on an ongoing \nbasis taking into consideration applicable laws, regulations and ethics \nand be proportional to the business requirements, the classification of \nthe information to be accessed and the perceived risks.\n6.2\nTerms and conditions of em-\nployment\nControl\nThe employment contractual agreements shall state the personnel’s and \nthe organization’s responsibilities for information security.\n6.3\nInformation security awareness, \neducation and training\nControl\nPersonnel of the organization and relevant interested parties shall receive \nappropriate information security awareness, education and training \nand regular updates of the organization's information security policy, \ntopic-specific policies and procedures, as relevant for their job function.\n6.4\nDisciplinary process\nControl\nA disciplinary process shall be formalized and communicated to take \nactions against personnel and other relevant interested parties who \nhave committed an information security policy violation.\n6.5\nResponsibilities after termination \nor change of employment\nControl\nInformation security responsibilities and duties that remain valid after \ntermination or change of employment shall be defined, enforced and \ncommunicated to relevant personnel and other interested parties.\n6.6\nConfidentiality or non-disclosure \nagreements\nControl\nConfidentiality or non-disclosure agreements reflecting the organ-\nization’s needs for the protection of information shall be identified, \ndocumented, regularly reviewed and signed by personnel and other \nrelevant interested parties.\n6.7\nRemote working\nControl\nSecurity measures shall be implemented when personnel are working \nremotely to protect information accessed, processed or stored outside \nthe organization’s premises.\n6.8\nInformation security event re-\nporting\nControl\nThe organization shall provide a mechanism for personnel to report \nobserved or suspected information security events through appropriate \nchannels in a timely manner.\n7\nPhysical controls\n7.1\nPhysical security perimeters\nControl\nSecurity perimeters shall be defined and used to protect areas that \ncontain information and other associated assets.\n7.2\nPhysical entry\nControl\nSecure areas shall be protected by appropriate entry controls and \naccess points.\n7.3\nSecuring offices, rooms and fa-\ncilities\nControl\nPhysical security for offices, rooms and facilities shall be designed and \nimplemented.\n7.4\nPhysical security monitoring\nControl\nPremises shall be continuously monitored for unauthorized physical \naccess.\nTable A.1 (continued)\nTable A.1 (continued)\n\t\n﻿\b\n© ISO/IEC 2022 – All rights reserved\n\b\n﻿\n14\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---",
          "char_count": 2852,
          "ocr_used": false
        },
        {
          "page": 21,
          "text": "ISO/IEC 27001:2022(E)\n7.5\nProtecting against physical and \nenvironmental threats\nControl\nProtection against physical and environmental threats, such as natural \ndisasters and other intentional or unintentional physical threats to \ninfrastructure shall be designed and implemented.\n7.6\nWorking in secure areas\nControl\nSecurity measures for working in secure areas shall be designed and \nimplemented.\n7.7\nClear desk and clear screen\nControl\nClear desk rules for papers and removable storage media and clear \nscreen rules for information processing facilities shall be defined and \nappropriately enforced.\n7.8\nEquipment siting and protection Control\nEquipment shall be sited securely and protected.\n7.9\nSecurity of assets off-premises Control\nOff-site assets shall be protected.\n7.10\nStorage media\nControl\nStorage media shall be managed through their life cycle of acquisition, \nuse, transportation and disposal in accordance with the organization’s \nclassification scheme and handling requirements.\n7.11\nSupporting utilities\nControl\nInformation processing facilities shall be protected from power failures \nand other disruptions caused by failures in supporting utilities.\n7.12\nCabling security\nControl\nCables carrying power, data or supporting information services shall \nbe protected from interception, interference or damage.\n7.13\nEquipment maintenance\nControl\nEquipment shall be maintained correctly to ensure availability, integrity \nand confidentiality of information.\n7.14\nSecure disposal or re-use of \nequipment\nControl\nItems of equipment containing storage media shall be verified to en-\nsure that any sensitive data and licensed software has been removed \nor securely overwritten prior to disposal or re-use.\n8\nTechnological controls\n8.1\nUser end point devices\nControl\nInformation stored on, processed by or accessible via user end point \ndevices shall be protected.\n8.2\nPrivileged access rights\nControl\nThe allocation and use of privileged access rights shall be restricted \nand managed.\n8.3\nInformation access restriction\nControl\nAccess to information and other associated assets shall be restricted in \naccordance with the established topic-specific policy on access control.\n8.4\nAccess to source code\nControl\nRead and write access to source code, development tools and software \nlibraries shall be appropriately managed.\nTable A.1 (continued)\nTable A.1 (continued)\n© ISO/IEC 2022 – All rights reserved\t\n﻿\n﻿\n15\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---",
          "char_count": 2472,
          "ocr_used": false
        },
        {
          "page": 22,
          "text": "ISO/IEC 27001:2022(E)\n8.5\nSecure authentication\nControl\nSecure authentication technologies and procedures shall be implemented \nbased on information access restrictions and the topic-specific policy \non access control.\n8.6\nCapacity management\nControl\nThe use of resources shall be monitored and adjusted in line with current \nand expected capacity requirements.\n8.7\nProtection against malware\nControl\nProtection against malware shall be implemented and supported by \nappropriate user awareness.\n8.8\nManagement of technical vul-\nnerabilities\nControl\nInformation about technical vulnerabilities of information systems in \nuse shall be obtained, the organization’s exposure to such vulnerabilities \nshall be evaluated and appropriate measures shall be taken.\n8.9\nConfiguration management\nControl\nConfigurations, including security configurations, of hardware, software, \nservices and networks shall be established, documented, implemented, \nmonitored and reviewed.\n8.10\nInformation deletion\nControl\nInformation stored in information systems, devices or in any other \nstorage media shall be deleted when no longer required.\n8.11\nData masking\nControl\nData masking shall be used in accordance with the organization’s \ntopic-specific policy on access control and other related topic-specific \npolicies, and business requirements, taking applicable legislation into \nconsideration.\n8.12\nData leakage prevention\nControl\nData leakage prevention measures shall be applied to systems, net-\nworks and any other devices that process, store or transmit sensitive \ninformation.\n8.13\nInformation backup\nControl\nBackup copies of information, software and systems shall be maintained \nand regularly tested in accordance with the agreed topic-specific policy \non backup.\n8.14\nRedundancy of information pro-\ncessing facilities\nControl\nInformation processing facilities shall be implemented with redundancy \nsufficient to meet availability requirements.\n8.15\nLogging\nControl\nLogs that record activities, exceptions, faults and other relevant events \nshall be produced, stored, protected and analysed.\n8.16\nMonitoring activities\nControl\nNetworks, systems and applications shall be monitored for anomalous \nbehaviour and appropriate actions taken to evaluate potential infor-\nmation security incidents.\n8.17\nClock synchronization\nControl\nThe clocks of information processing systems used by the organization \nshall be synchronized to approved time sources.\nTable A.1 (continued)\nTable A.1 (continued)\n\t\n﻿\b\n© ISO/IEC 2022 – All rights reserved\n\b\n﻿\n16\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---",
          "char_count": 2576,
          "ocr_used": false
        },
        {
          "page": 23,
          "text": "ISO/IEC 27001:2022(E)\n8.18\nUse of privileged utility programs Control\nThe use of utility programs that can be capable of overriding system \nand application controls shall be restricted and tightly controlled.\n8.19\nInstallation of software on op-\nerational systems\nControl\nProcedures and measures shall be implemented to securely manage \nsoftware installation on operational systems.\n8.20\nNetworks security\nControl\nNetworks and network devices shall be secured, managed and controlled \nto protect information in systems and applications.\n8.21\nSecurity of network services\nControl\nSecurity mechanisms, service levels and service requirements of network \nservices shall be identified, implemented and monitored.\n8.22\nSegregation of networks\nControl\nGroups of information services, users and information systems shall \nbe segregated in the organization’s networks.\n8.23\nWeb filtering\nControl\nAccess to external websites shall be managed to reduce exposure to \nmalicious content.\n8.24\nUse of cryptography\nControl\nRules for the effective use of cryptography, including cryptographic key \nmanagement, shall be defined and implemented.\n8.25\nSecure development life cycle\nControl\nRules for the secure development of software and systems shall be \nestablished and applied.\n8.26\nApplication security require-\nments\nControl\nInformation security requirements shall be identified, specified and \napproved when developing or acquiring applications.\n8.27\nSecure system architecture and \nengineering principles\nControl\nPrinciples for engineering secure systems shall be established, docu-\nmented, maintained and applied to any information system development \nactivities.\n8.28\nSecure coding\nControl\nSecure coding principles shall be applied to software development.\n8.29\nSecurity testing in development \nand acceptance\nControl\nSecurity testing processes shall be defined and implemented in the \ndevelopment life cycle.\n8.30\nOutsourced development\nControl\nThe organization shall direct, monitor and review the activities related \nto outsourced system development.\n8.31\nSeparation of development, test \nand production environments\nControl\nDevelopment, testing and production environments shall be separated \nand secured.\n8.32\nChange management\nControl\nChanges to information processing facilities and information systems \nshall be subject to change management procedures.\n8.33\nTest information\nControl\nTest information shall be appropriately selected, protected and managed.\nTable A.1 (continued)\nTable A.1 (continued)\n© ISO/IEC 2022 – All rights reserved\t\n﻿\n﻿\n17\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---",
          "char_count": 2594,
          "ocr_used": false
        },
        {
          "page": 24,
          "text": "ISO/IEC 27001:2022(E)\n8.34\nProtection of information sys-\ntems during audit testing\nControl\nAudit tests and other assurance activities involving assessment of op-\nerational systems shall be planned and agreed between the tester and \nappropriate management.\nTable A.1 (continued)\nTable A.1 (continued)\n\t\n﻿\b\n© ISO/IEC 2022 – All rights reserved\n\b\n﻿\n18\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---",
          "char_count": 400,
          "ocr_used": false
        },
        {
          "page": 25,
          "text": "ISO/IEC 27001:2022(E)\nBibliography\n[1]\t\nISO/IEC 27002:2022, Information security, cybersecurity and privacy protection — Information \nsecurity controls\n[2]\t\nISO/IEC 27003, Information technology — Security techniques — Information security management \nsystems — Guidance\n[3]\t\nISO/IEC 27004, Information technology — Security techniques — Information security management \n— Monitoring, measurement, analysis and evaluation\n[4]\t\nISO/IEC 27005, Information security, cybersecurity and privacy protection — Guidance on \nmanaging information security risks\n[5]\t\nISO 31000:2018, Risk management — Guidelines\n© ISO/IEC 2022 – All rights reserved\t\n﻿\n﻿\n19\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---",
          "char_count": 697,
          "ocr_used": false
        },
        {
          "page": 26,
          "text": "ISO/IEC 27001:2022(E)\nICS 03.100.70; 35.030\nPrice based on 19 pages\n© ISO/IEC 2022 – All rights reserved\t\n﻿\n--``,,,,,``````,,,,,`,`,`,`,,`,-`-`,,`,,`,`,,`---",
          "char_count": 158,
          "ocr_used": false
        }
      ],
      "metadata": {
        "format": "PDF 1.4",
        "title": "ISO/IEC 27001:2022",
        "author": "ISO",
        "subject": "",
        "keywords": "",
        "creator": "Adobe InDesign 16.4 (Windows)",
        "producer": "PDPreStamp v3.3",
        "creationDate": "D:20220929175400+02'00'",
        "modDate": "D:20221026192240+08'00'",
        "trapped": "",
        "encryption": null
      },
      "char_count": 57363,
      "word_count": 6701,
      "ocr_pages_count": 1,
      "error": null,
      "file_id": "1v_zYG2DgoJzGSJ9oiBqxH0-RmUCfLjiD",
      "filename": "Product Management.pdf",
      "filepath": "downloads/Product Management.pdf",
      "download_link": "https://drive.google.com/uc?export=download&id=1v_zYG2DgoJzGSJ9oiBqxH0-RmUCfLjiD"
    },
    {
      "success": true,
      "text": "7 WEBSITES THAT WILL\nSAVE 100S OF HOURS\n\n7 WEBSITES THAT WILL\nSAVE 100S OF HOURS\npy Svs\n. WN\n1.archive.org\nInternet Archive is a non-profit library of\nmillions of free books, movies, software,\nmusic, websites, and more.\n2. darebee.com\nAccess 1800+ free workouts in this\ndatabase. It's a non-profit (ad-free and\nproduct-placement-free). Most of the\nworkouts are body weight and require no\nequipment.\n3. remove.bg\nRemove image backgrounds\nautomatically in 5 seconds with just one\nclick.\n4.supercook.com\nSupercook is a recipe search engine that\nlets you search by ingredients you have\nat home.\n5.tinywow.com\nTinyWow provides free online\nconversion, PDF, and other handy tools\nto help you solve problems of all types.\nAll files both processed and\nunprocessed are deleted after 15\nminutes.\n6. edx.org\nThe most renowned online learning\nplatform for high-quality courses from\nworld-famous universities.\n7. carrd.co\nBuild simple, free, fully responsive one-\npage sites for pretty much anything.",
      "page_count": 8,
      "pages": [
        {
          "page": 1,
          "text": "7 WEBSITES THAT WILL\nSAVE 100S OF HOURS\n\n7 WEBSITES THAT WILL\nSAVE 100S OF HOURS\npy Svs\n. WN",
          "char_count": 93,
          "ocr_used": true,
          "original_char_count": 40
        },
        {
          "page": 2,
          "text": "1.archive.org\nInternet Archive is a non-profit library of\nmillions of free books, movies, software,\nmusic, websites, and more.",
          "char_count": 127,
          "ocr_used": false
        },
        {
          "page": 3,
          "text": "2. darebee.com\nAccess 1800+ free workouts in this\ndatabase. It's a non-profit (ad-free and\nproduct-placement-free). Most of the\nworkouts are body weight and require no\nequipment.",
          "char_count": 179,
          "ocr_used": false
        },
        {
          "page": 4,
          "text": "3. remove.bg\nRemove image backgrounds\nautomatically in 5 seconds with just one\nclick.",
          "char_count": 86,
          "ocr_used": false
        },
        {
          "page": 5,
          "text": "4.supercook.com\nSupercook is a recipe search engine that\nlets you search by ingredients you have\nat home.",
          "char_count": 106,
          "ocr_used": false
        },
        {
          "page": 6,
          "text": "5.tinywow.com\nTinyWow provides free online\nconversion, PDF, and other handy tools\nto help you solve problems of all types.\nAll files both processed and\nunprocessed are deleted after 15\nminutes.",
          "char_count": 194,
          "ocr_used": false
        },
        {
          "page": 7,
          "text": "6. edx.org\nThe most renowned online learning\nplatform for high-quality courses from\nworld-famous universities.",
          "char_count": 111,
          "ocr_used": false
        },
        {
          "page": 8,
          "text": "7. carrd.co\nBuild simple, free, fully responsive one-\npage sites for pretty much anything.",
          "char_count": 91,
          "ocr_used": false
        }
      ],
      "metadata": {
        "format": "PDF 1.4",
        "title": "",
        "author": "",
        "subject": "",
        "keywords": "",
        "creator": "",
        "producer": "",
        "creationDate": "",
        "modDate": "",
        "trapped": "",
        "encryption": null
      },
      "char_count": 987,
      "word_count": 152,
      "ocr_pages_count": 1,
      "error": null,
      "file_id": "1cV3lZTUqlApXqGH-AdZBT4SO8F_jOO3N",
      "filename": "Websites that will save 100s of hours!.pdf",
      "filepath": "downloads/Websites that will save 100s of hours!.pdf",
      "download_link": "https://drive.google.com/uc?export=download&id=1cV3lZTUqlApXqGH-AdZBT4SO8F_jOO3N"
    }
  ]
}